{
  "id": "database_cloud_optimization_cost_optimize",
  "name": "Cloud Cost Optimization",
  "source": "database-cloud-optimization",
  "originalPath": "plugins/database-cloud-optimization/commands/cost-optimize.md",
  "command": "/database-cloud-optimization:cost-optimize",
  "parameters": {},
  "outputs": {},
  "agentsUsing": [],
  "fullDefinition": "# Cloud Cost Optimization\n\nYou are a cloud cost optimization expert specializing in reducing infrastructure expenses while maintaining performance and reliability. Analyze cloud spending, identify savings opportunities, and implement cost-effective architectures across AWS, Azure, and GCP.\n\n## Context\nThe user needs to optimize cloud infrastructure costs without compromising performance or reliability. Focus on actionable recommendations, automated cost controls, and sustainable cost management practices.\n\n## Requirements\n$ARGUMENTS\n\n## Instructions\n\n### 1. Cost Analysis and Visibility\n\nImplement comprehensive cost analysis:\n\n**Cost Analysis Framework**\n```python\nimport boto3\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Any\nimport json\n\nclass CloudCostAnalyzer:\n    def __init__(self, cloud_provider: str):\n        self.provider = cloud_provider\n        self.client = self._initialize_client()\n        self.cost_data = None\n        \n    def analyze_costs(self, time_period: int = 30):\n        \"\"\"Comprehensive cost analysis\"\"\"\n        analysis = {\n            'total_cost': self._get_total_cost(time_period),\n            'cost_by_service': self._analyze_by_service(time_period),\n            'cost_by_resource': self._analyze_by_resource(time_period),\n            'cost_trends': self._analyze_trends(time_period),\n            'anomalies': self._detect_anomalies(time_period),\n            'waste_analysis': self._identify_waste(),\n            'optimization_opportunities': self._find_opportunities()\n        }\n        \n        return self._generate_report(analysis)\n    \n    def _analyze_by_service(self, days: int):\n        \"\"\"Analyze costs by service\"\"\"\n        if self.provider == 'aws':\n            ce = boto3.client('ce')\n            \n            response = ce.get_cost_and_usage(\n                TimePeriod={\n                    'Start': (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d'),\n                    'End': datetime.now().strftime('%Y-%m-%d')\n                },\n                Granularity='DAILY',\n                Metrics=['UnblendedCost'],\n                GroupBy=[\n                    {'Type': 'DIMENSION', 'Key': 'SERVICE'}\n                ]\n            )\n            \n            # Process response\n            service_costs = {}\n            for result in response['ResultsByTime']:\n                for group in result['Groups']:\n                    service = group['Keys'][0]\n                    cost = float(group['Metrics']['UnblendedCost']['Amount'])\n                    \n                    if service not in service_costs:\n                        service_costs[service] = []\n                    service_costs[service].append(cost)\n            \n            # Calculate totals and trends\n            analysis = {}\n            for service, costs in service_costs.items():\n                analysis[service] = {\n                    'total': sum(costs),\n                    'average_daily': sum(costs) / len(costs),\n                    'trend': self._calculate_trend(costs),\n                    'percentage': (sum(costs) / self._get_total_cost(days)) * 100\n                }\n            \n            return analysis\n    \n    def _identify_waste(self):\n        \"\"\"Identify wasted resources\"\"\"\n        waste_analysis = {\n            'unused_resources': self._find_unused_resources(),\n            'oversized_resources': self._find_oversized_resources(),\n            'unattached_storage': self._find_unattached_storage(),\n            'idle_load_balancers': self._find_idle_load_balancers(),\n            'old_snapshots': self._find_old_snapshots(),\n            'untagged_resources': self._find_untagged_resources()\n        }\n        \n        total_waste = sum(item['estimated_savings'] \n                         for category in waste_analysis.values() \n                         for item in category)\n        \n        waste_analysis['total_potential_savings'] = total_waste\n        \n        return waste_analysis\n    \n    def _find_unused_resources(self):\n        \"\"\"Find resources with no usage\"\"\"\n        unused = []\n        \n        if self.provider == 'aws':\n            # Check EC2 instances\n            ec2 = boto3.client('ec2')\n            cloudwatch = boto3.client('cloudwatch')\n            \n            instances = ec2.describe_instances(\n                Filters=[{'Name': 'instance-state-name', 'Values': ['running']}]\n            )\n            \n            for reservation in instances['Reservations']:\n                for instance in reservation['Instances']:\n                    # Check CPU utilization\n                    metrics = cloudwatch.get_metric_statistics(\n                        Namespace='AWS/EC2',\n                        MetricName='CPUUtilization',\n                        Dimensions=[\n                            {'Name': 'InstanceId', 'Value': instance['InstanceId']}\n                        ],\n                        StartTime=datetime.now() - timedelta(days=7),\n                        EndTime=datetime.now(),\n                        Period=3600,\n                        Statistics=['Average']\n                    )\n                    \n                    if metrics['Datapoints']:\n                        avg_cpu = sum(d['Average'] for d in metrics['Datapoints']) / len(metrics['Datapoints'])\n                        \n                        if avg_cpu < 5:  # Less than 5% CPU usage\n                            unused.append({\n                                'resource_type': 'EC2 Instance',\n                                'resource_id': instance['InstanceId'],\n                                'reason': f'Average CPU: {avg_cpu:.2f}%',\n                                'estimated_savings': self._calculate_instance_cost(instance)\n                            })\n        \n        return unused\n```\n\n### 2. Resource Rightsizing\n\nImplement intelligent rightsizing:\n\n**Rightsizing Engine**\n```python\nclass ResourceRightsizer:\n    def __init__(self):\n        self.utilization_thresholds = {\n            'cpu_low': 20,\n            'cpu_high': 80,\n            'memory_low': 30,\n            'memory_high': 85,\n            'network_low': 10,\n            'network_high': 70\n        }\n    \n    def analyze_rightsizing_opportunities(self):\n        \"\"\"Find rightsizing opportunities\"\"\"\n        opportunities = {\n            'ec2_instances': self._rightsize_ec2(),\n            'rds_instances': self._rightsize_rds(),\n            'containers': self._rightsize_containers(),\n            'lambda_functions': self._rightsize_lambda(),\n            'storage_volumes': self._rightsize_storage()\n        }\n        \n        return self._prioritize_opportunities(opportunities)\n    \n    def _rightsize_ec2(self):\n        \"\"\"Rightsize EC2 instances\"\"\"\n        recommendations = []\n        \n        instances = self._get_running_instances()\n        \n        for instance in instances:\n            # Get utilization metrics\n            utilization = self._get_instance_utilization(instance['InstanceId'])\n            \n            # Determine if oversized or undersized\n            current_type = instance['InstanceType']\n            recommended_type = self._recommend_instance_type(\n                current_type, \n                utilization\n            )\n            \n            if recommended_type != current_type:\n                current_cost = self._get_instance_cost(current_type)\n                new_cost = self._get_instance_cost(recommended_type)\n                \n                recommendations.append({\n                    'resource_id': instance['InstanceId'],\n                    'current_type': current_type,\n                    'recommended_type': recommended_type,\n                    'reason': self._generate_reason(utilization),\n                    'current_cost': current_cost,\n                    'new_cost': new_cost,\n                    'monthly_savings': (current_cost - new_cost) * 730,\n                    'effort': 'medium',\n                    'risk': 'low' if 'downsize' in self._generate_reason(utilization) else 'medium'\n                })\n        \n        return recommendations\n    \n    def _recommend_instance_type(self, current_type: str, utilization: Dict):\n        \"\"\"Recommend optimal instance type\"\"\"\n        # Parse current instance family and size\n        family, size = self._parse_instance_type(current_type)\n        \n        # Calculate required resources\n        required_cpu = self._calculate_required_cpu(utilization['cpu'])\n        required_memory = self._calculate_required_memory(utilization['memory'])\n        \n        # Find best matching instance\n        instance_catalog = self._get_instance_catalog()\n        \n        candidates = []\n        for instance_type, specs in instance_catalog.items():\n            if (specs['vcpu'] >= required_cpu and \n                specs['memory'] >= required_memory):\n                candidates.append({\n                    'type': instance_type,\n                    'cost': specs['cost'],\n                    'vcpu': specs['vcpu'],\n                    'memory': specs['memory'],\n                    'efficiency_score': self._calculate_efficiency_score(\n                        specs, required_cpu, required_memory\n                    )\n                })\n        \n        # Select best candidate\n        if candidates:\n            best = sorted(candidates, \n                         key=lambda x: (x['efficiency_score'], x['cost']))[0]\n            return best['type']\n        \n        return current_type\n    \n    def create_rightsizing_automation(self):\n        \"\"\"Automated rightsizing implementation\"\"\"\n        return '''\nimport boto3\nfrom datetime import datetime\nimport logging\n\nclass AutomatedRightsizer:\n    def __init__(self):\n        self.ec2 = boto3.client('ec2')\n        self.cloudwatch = boto3.client('cloudwatch')\n        self.logger = logging.getLogger(__name__)\n        \n    def execute_rightsizing(self, recommendations: List[Dict], dry_run: bool = True):\n        \"\"\"Execute rightsizing recommendations\"\"\"\n        results = []\n        \n        for recommendation in recommendations:\n            try:\n                if recommendation['risk'] == 'low' or self._get_approval(recommendation):\n                    result = self._resize_instance(\n                        recommendation['resource_id'],\n                        recommendation['recommended_type'],\n                        dry_run=dry_run\n                    )\n                    results.append(result)\n            except Exception as e:\n                self.logger.error(f\"Failed to resize {recommendation['resource_id']}: {e}\")\n                \n        return results\n    \n    def _resize_instance(self, instance_id: str, new_type: str, dry_run: bool):\n        \"\"\"Resize an EC2 instance\"\"\"\n        # Create snapshot for rollback\n        snapshot_id = self._create_snapshot(instance_id)\n        \n        try:\n            # Stop instance\n            if not dry_run:\n                self.ec2.stop_instances(InstanceIds=[instance_id])\n                self._wait_for_state(instance_id, 'stopped')\n            \n            # Change instance type\n            self.ec2.modify_instance_attribute(\n                InstanceId=instance_id,\n                InstanceType={'Value': new_type},\n                DryRun=dry_run\n            )\n            \n            # Start instance\n            if not dry_run:\n                self.ec2.start_instances(InstanceIds=[instance_id])\n                self._wait_for_state(instance_id, 'running')\n            \n            return {\n                'instance_id': instance_id,\n                'status': 'success',\n                'new_type': new_type,\n                'snapshot_id': snapshot_id\n            }\n            \n        except Exception as e:\n            # Rollback on failure\n            if not dry_run:\n                self._rollback_instance(instance_id, snapshot_id)\n            raise\n'''\n```\n\n### 3. Reserved Instances and Savings Plans\n\nOptimize commitment-based discounts:\n\n**Reservation Optimizer**\n```python\nclass ReservationOptimizer:\n    def __init__(self):\n        self.usage_history = None\n        self.existing_reservations = None\n        \n    def analyze_reservation_opportunities(self):\n        \"\"\"Analyze opportunities for reservations\"\"\"\n        analysis = {\n            'current_coverage': self._analyze_current_coverage(),\n            'usage_patterns': self._analyze_usage_patterns(),\n            'recommendations': self._generate_recommendations(),\n            'roi_analysis': self._calculate_roi(),\n            'risk_assessment': self._assess_commitment_risk()\n        }\n        \n        return analysis\n    \n    def _analyze_usage_patterns(self):\n        \"\"\"Analyze historical usage patterns\"\"\"\n        # Get 12 months of usage data\n        usage_data = self._get_historical_usage(months=12)\n        \n        patterns = {\n            'stable_workloads': [],\n            'variable_workloads': [],\n            'seasonal_patterns': [],\n            'growth_trends': []\n        }\n        \n        # Analyze each instance family\n        for family in self._get_instance_families(usage_data):\n            family_usage = self._filter_by_family(usage_data, family)\n            \n            # Calculate stability metrics\n            stability = self._calculate_stability(family_usage)\n            \n            if stability['coefficient_of_variation'] < 0.1:\n                patterns['stable_workloads'].append({\n                    'family': family,\n                    'average_usage': stability['mean'],\n                    'min_usage': stability['min'],\n                    'recommendation': 'reserved_instance',\n                    'term': '3_year',\n                    'payment': 'all_upfront'\n                })\n            elif stability['coefficient_of_variation'] < 0.3:\n                patterns['variable_workloads'].append({\n                    'family': family,\n                    'average_usage': stability['mean'],\n                    'baseline': stability['percentile_25'],\n                    'recommendation': 'savings_plan',\n                    'commitment': stability['percentile_25']\n                })\n            \n            # Check for seasonal patterns\n            if self._has_seasonal_pattern(family_usage):\n                patterns['seasonal_patterns'].append({\n                    'family': family,\n                    'pattern': self._identify_seasonal_pattern(family_usage),\n                    'recommendation': 'spot_with_savings_plan_baseline'\n                })\n        \n        return patterns\n    \n    def _generate_recommendations(self):\n        \"\"\"Generate reservation recommendations\"\"\"\n        recommendations = []\n        \n        patterns = self._analyze_usage_patterns()\n        current_costs = self._calculate_current_costs()\n        \n        # Reserved Instance recommendations\n        for workload in patterns['stable_workloads']:\n            ri_options = self._calculate_ri_options(workload)\n            \n            for option in ri_options:\n                savings = current_costs[workload['family']] - option['total_cost']\n                \n                if savings > 0:\n                    recommendations.append({\n                        'type': 'reserved_instance',\n                        'family': workload['family'],\n                        'quantity': option['quantity'],\n                        'term': option['term'],\n                        'payment': option['payment_option'],\n                        'upfront_cost': option['upfront_cost'],\n                        'monthly_cost': option['monthly_cost'],\n                        'total_savings': savings,\n                        'break_even_months': option['upfront_cost'] / (savings / 36),\n                        'confidence': 'high'\n                    })\n        \n        # Savings Plan recommendations\n        for workload in patterns['variable_workloads']:\n            sp_options = self._calculate_savings_plan_options(workload)\n            \n            for option in sp_options:\n                recommendations.append({\n                    'type': 'savings_plan',\n                    'commitment_type': option['type'],\n                    'hourly_commitment': option['commitment'],\n                    'term': option['term'],\n                    'estimated_savings': option['savings'],\n                    'flexibility': option['flexibility_score'],\n                    'confidence': 'medium'\n                })\n        \n        return sorted(recommendations, key=lambda x: x.get('total_savings', 0), reverse=True)\n    \n    def create_reservation_dashboard(self):\n        \"\"\"Create reservation tracking dashboard\"\"\"\n        return '''\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Reservation & Savings Dashboard</title>\n    <script src=\"https://cdn.jsdelivr.net/npm/chart.js\"></script>\n</head>\n<body>\n    <div class=\"dashboard\">\n        <div class=\"summary-cards\">\n            <div class=\"card\">\n                <h3>Current Coverage</h3>\n                <div class=\"metric\">{coverage_percentage}%</div>\n                <div class=\"sub-metric\">On-Demand: ${on_demand_cost}</div>\n                <div class=\"sub-metric\">Reserved: ${reserved_cost}</div>\n            </div>\n            \n            <div class=\"card\">\n                <h3>Potential Savings</h3>\n                <div class=\"metric\">${potential_savings}/month</div>\n                <div class=\"sub-metric\">{recommendations_count} opportunities</div>\n            </div>\n            \n            <div class=\"card\">\n                <h3>Expiring Soon</h3>\n                <div class=\"metric\">{expiring_count} RIs</div>\n                <div class=\"sub-metric\">Next 30 days</div>\n            </div>\n        </div>\n        \n        <div class=\"charts\">\n            <canvas id=\"coverageChart\"></canvas>\n            <canvas id=\"savingsChart\"></canvas>\n        </div>\n        \n        <div class=\"recommendations-table\">\n            <h3>Top Recommendations</h3>\n            <table>\n                <tr>\n                    <th>Type</th>\n                    <th>Resource</th>\n                    <th>Term</th>\n                    <th>Upfront</th>\n                    <th>Monthly Savings</th>\n                    <th>ROI</th>\n                    <th>Action</th>\n                </tr>\n                {recommendation_rows}\n            </table>\n        </div>\n    </div>\n</body>\n</html>\n'''\n```\n\n### 4. Spot Instance Optimization\n\nLeverage spot instances effectively:\n\n**Spot Instance Manager**\n```python\nclass SpotInstanceOptimizer:\n    def __init__(self):\n        self.spot_advisor = self._init_spot_advisor()\n        self.interruption_handler = None\n        \n    def identify_spot_opportunities(self):\n        \"\"\"Identify workloads suitable for spot\"\"\"\n        workloads = self._analyze_workloads()\n        \n        spot_candidates = {\n            'batch_processing': [],\n            'dev_test': [],\n            'stateless_apps': [],\n            'ci_cd': [],\n            'data_processing': []\n        }\n        \n        for workload in workloads:\n            suitability = self._assess_spot_suitability(workload)\n            \n            if suitability['score'] > 0.7:\n                spot_candidates[workload['type']].append({\n                    'workload': workload['name'],\n                    'current_cost': workload['cost'],\n                    'spot_savings': workload['cost'] * 0.7,  # ~70% savings\n                    'interruption_tolerance': suitability['interruption_tolerance'],\n                    'recommended_strategy': self._recommend_spot_strategy(workload)\n                })\n        \n        return spot_candidates\n    \n    def _recommend_spot_strategy(self, workload):\n        \"\"\"Recommend spot instance strategy\"\"\"\n        if workload['interruption_tolerance'] == 'high':\n            return {\n                'strategy': 'spot_fleet_diverse',\n                'instance_pools': 10,\n                'allocation_strategy': 'capacity-optimized',\n                'on_demand_base': 0,\n                'spot_percentage': 100\n            }\n        elif workload['interruption_tolerance'] == 'medium':\n            return {\n                'strategy': 'mixed_instances',\n                'on_demand_base': 25,\n                'spot_percentage': 75,\n                'spot_allocation': 'lowest-price'\n            }\n        else:\n            return {\n                'strategy': 'spot_with_fallback',\n                'primary': 'spot',\n                'fallback': 'on-demand',\n                'checkpointing': True\n            }\n    \n    def create_spot_configuration(self):\n        \"\"\"Create spot instance configuration\"\"\"\n        return '''\n# Terraform configuration for Spot instances\nresource \"aws_spot_fleet_request\" \"processing_fleet\" {\n  iam_fleet_role = aws_iam_role.spot_fleet.arn\n  \n  allocation_strategy = \"diversified\"\n  target_capacity     = 100\n  valid_until        = timeadd(timestamp(), \"168h\")\n  \n  # Define multiple launch specifications for diversity\n  dynamic \"launch_specification\" {\n    for_each = var.spot_instance_types\n    \n    content {\n      instance_type     = launch_specification.value\n      ami              = var.ami_id\n      key_name         = var.key_name\n      subnet_id        = var.subnet_ids[launch_specification.key % length(var.subnet_ids)]\n      \n      weighted_capacity = var.instance_weights[launch_specification.value]\n      spot_price       = var.max_spot_prices[launch_specification.value]\n      \n      user_data = base64encode(templatefile(\"${path.module}/spot-init.sh\", {\n        interruption_handler = true\n        checkpoint_s3_bucket = var.checkpoint_bucket\n      }))\n      \n      tags = {\n        Name = \"spot-processing-${launch_specification.key}\"\n        Type = \"spot\"\n      }\n    }\n  }\n  \n  # Interruption handling\n  lifecycle {\n    create_before_destroy = true\n  }\n}\n\n# Spot interruption handler\nresource \"aws_lambda_function\" \"spot_interruption_handler\" {\n  filename         = \"spot-handler.zip\"\n  function_name    = \"spot-interruption-handler\"\n  role            = aws_iam_role.lambda_role.arn\n  handler         = \"handler.main\"\n  runtime         = \"python3.9\"\n  \n  environment {\n    variables = {\n      CHECKPOINT_BUCKET = var.checkpoint_bucket\n      SNS_TOPIC_ARN    = aws_sns_topic.spot_interruptions.arn\n    }\n  }\n}\n'''\n```\n\n### 5. Storage Optimization\n\nOptimize storage costs:\n\n**Storage Optimizer**\n```python\nclass StorageOptimizer:\n    def analyze_storage_costs(self):\n        \"\"\"Comprehensive storage analysis\"\"\"\n        analysis = {\n            'ebs_volumes': self._analyze_ebs_volumes(),\n            's3_buckets': self._analyze_s3_buckets(),\n            'snapshots': self._analyze_snapshots(),\n            'lifecycle_opportunities': self._find_lifecycle_opportunities(),\n            'compression_opportunities': self._find_compression_opportunities()\n        }\n        \n        return analysis\n    \n    def _analyze_s3_buckets(self):\n        \"\"\"Analyze S3 bucket costs and optimization\"\"\"\n        s3 = boto3.client('s3')\n        cloudwatch = boto3.client('cloudwatch')\n        \n        buckets = s3.list_buckets()['Buckets']\n        bucket_analysis = []\n        \n        for bucket in buckets:\n            bucket_name = bucket['Name']\n            \n            # Get storage metrics\n            metrics = self._get_s3_metrics(bucket_name)\n            \n            # Analyze storage classes\n            storage_class_distribution = self._get_storage_class_distribution(bucket_name)\n            \n            # Calculate optimization potential\n            optimization = self._calculate_s3_optimization(\n                bucket_name,\n                metrics,\n                storage_class_distribution\n            )\n            \n            bucket_analysis.append({\n                'bucket_name': bucket_name,\n                'total_size_gb': metrics['size_gb'],\n                'total_objects': metrics['object_count'],\n                'current_cost': metrics['monthly_cost'],\n                'storage_classes': storage_class_distribution,\n                'optimization_recommendations': optimization['recommendations'],\n                'potential_savings': optimization['savings']\n            })\n        \n        return bucket_analysis\n    \n    def create_lifecycle_policies(self):\n        \"\"\"Create S3 lifecycle policies\"\"\"\n        return '''\nimport boto3\nfrom datetime import datetime\n\nclass S3LifecycleManager:\n    def __init__(self):\n        self.s3 = boto3.client('s3')\n        \n    def create_intelligent_lifecycle(self, bucket_name: str, access_patterns: Dict):\n        \"\"\"Create lifecycle policy based on access patterns\"\"\"\n        \n        rules = []\n        \n        # Intelligent tiering for unknown access patterns\n        if access_patterns.get('unpredictable'):\n            rules.append({\n                'ID': 'intelligent-tiering',\n                'Status': 'Enabled',\n                'Transitions': [{\n                    'Days': 1,\n                    'StorageClass': 'INTELLIGENT_TIERING'\n                }]\n            })\n        \n        # Standard lifecycle for predictable patterns\n        if access_patterns.get('predictable'):\n            rules.append({\n                'ID': 'standard-lifecycle',\n                'Status': 'Enabled',\n                'Transitions': [\n                    {\n                        'Days': 30,\n                        'StorageClass': 'STANDARD_IA'\n                    },\n                    {\n                        'Days': 90,\n                        'StorageClass': 'GLACIER'\n                    },\n                    {\n                        'Days': 180,\n                        'StorageClass': 'DEEP_ARCHIVE'\n                    }\n                ]\n            })\n        \n        # Delete old versions\n        rules.append({\n            'ID': 'delete-old-versions',\n            'Status': 'Enabled',\n            'NoncurrentVersionTransitions': [\n                {\n                    'NoncurrentDays': 30,\n                    'StorageClass': 'GLACIER'\n                }\n            ],\n            'NoncurrentVersionExpiration': {\n                'NoncurrentDays': 90\n            }\n        })\n        \n        # Apply lifecycle configuration\n        self.s3.put_bucket_lifecycle_configuration(\n            Bucket=bucket_name,\n            LifecycleConfiguration={'Rules': rules}\n        )\n        \n        return rules\n    \n    def optimize_ebs_volumes(self):\n        \"\"\"Optimize EBS volume types and sizes\"\"\"\n        ec2 = boto3.client('ec2')\n        \n        volumes = ec2.describe_volumes()['Volumes']\n        optimizations = []\n        \n        for volume in volumes:\n            # Analyze volume metrics\n            iops_usage = self._get_volume_iops_usage(volume['VolumeId'])\n            throughput_usage = self._get_volume_throughput_usage(volume['VolumeId'])\n            \n            current_type = volume['VolumeType']\n            recommended_type = self._recommend_volume_type(\n                iops_usage,\n                throughput_usage,\n                volume['Size']\n            )\n            \n            if recommended_type != current_type:\n                optimizations.append({\n                    'volume_id': volume['VolumeId'],\n                    'current_type': current_type,\n                    'recommended_type': recommended_type,\n                    'reason': self._get_optimization_reason(\n                        current_type,\n                        recommended_type,\n                        iops_usage,\n                        throughput_usage\n                    ),\n                    'monthly_savings': self._calculate_volume_savings(\n                        volume,\n                        recommended_type\n                    )\n                })\n        \n        return optimizations\n'''\n```\n\n### 6. Network Cost Optimization\n\nReduce network transfer costs:\n\n**Network Cost Optimizer**\n```python\nclass NetworkCostOptimizer:\n    def analyze_network_costs(self):\n        \"\"\"Analyze network transfer costs\"\"\"\n        analysis = {\n            'data_transfer_costs': self._analyze_data_transfer(),\n            'nat_gateway_costs': self._analyze_nat_gateways(),\n            'load_balancer_costs': self._analyze_load_balancers(),\n            'vpc_endpoint_opportunities': self._find_vpc_endpoint_opportunities(),\n            'cdn_optimization': self._analyze_cdn_usage()\n        }\n        \n        return analysis\n    \n    def _analyze_data_transfer(self):\n        \"\"\"Analyze data transfer patterns and costs\"\"\"\n        transfers = {\n            'inter_region': self._get_inter_region_transfers(),\n            'internet_egress': self._get_internet_egress(),\n            'inter_az': self._get_inter_az_transfers(),\n            'vpc_peering': self._get_vpc_peering_transfers()\n        }\n        \n        recommendations = []\n        \n        # Analyze inter-region transfers\n        if transfers['inter_region']['monthly_gb'] > 1000:\n            recommendations.append({\n                'type': 'region_consolidation',\n                'description': 'Consider consolidating resources in fewer regions',\n                'current_cost': transfers['inter_region']['monthly_cost'],\n                'potential_savings': transfers['inter_region']['monthly_cost'] * 0.8\n            })\n        \n        # Analyze internet egress\n        if transfers['internet_egress']['monthly_gb'] > 10000:\n            recommendations.append({\n                'type': 'cdn_implementation',\n                'description': 'Implement CDN to reduce origin egress',\n                'current_cost': transfers['internet_egress']['monthly_cost'],\n                'potential_savings': transfers['internet_egress']['monthly_cost'] * 0.6\n            })\n        \n        return {\n            'current_costs': transfers,\n            'recommendations': recommendations\n        }\n    \n    def create_network_optimization_script(self):\n        \"\"\"Script to implement network optimizations\"\"\"\n        return '''\n#!/usr/bin/env python3\nimport boto3\nfrom collections import defaultdict\n\nclass NetworkOptimizer:\n    def __init__(self):\n        self.ec2 = boto3.client('ec2')\n        self.cloudwatch = boto3.client('cloudwatch')\n        \n    def optimize_nat_gateways(self):\n        \"\"\"Consolidate and optimize NAT gateways\"\"\"\n        # Get all NAT gateways\n        nat_gateways = self.ec2.describe_nat_gateways()['NatGateways']\n        \n        # Group by VPC\n        vpc_nat_gateways = defaultdict(list)\n        for nat in nat_gateways:\n            if nat['State'] == 'available':\n                vpc_nat_gateways[nat['VpcId']].append(nat)\n        \n        optimizations = []\n        \n        for vpc_id, nats in vpc_nat_gateways.items():\n            if len(nats) > 1:\n                # Check if consolidation is possible\n                traffic_analysis = self._analyze_nat_traffic(nats)\n                \n                if traffic_analysis['can_consolidate']:\n                    optimizations.append({\n                        'vpc_id': vpc_id,\n                        'action': 'consolidate_nat',\n                        'current_count': len(nats),\n                        'recommended_count': traffic_analysis['recommended_count'],\n                        'monthly_savings': (len(nats) - traffic_analysis['recommended_count']) * 45\n                    })\n        \n        return optimizations\n    \n    def implement_vpc_endpoints(self):\n        \"\"\"Implement VPC endpoints for AWS services\"\"\"\n        services_to_check = ['s3', 'dynamodb', 'ec2', 'sns', 'sqs']\n        vpc_list = self.ec2.describe_vpcs()['Vpcs']\n        \n        implementations = []\n        \n        for vpc in vpc_list:\n            vpc_id = vpc['VpcId']\n            \n            # Check existing endpoints\n            existing = self._get_existing_endpoints(vpc_id)\n            \n            for service in services_to_check:\n                if service not in existing:\n                    # Check if service is being used\n                    if self._is_service_used(vpc_id, service):\n                        # Create VPC endpoint\n                        endpoint = self._create_vpc_endpoint(vpc_id, service)\n                        \n                        implementations.append({\n                            'vpc_id': vpc_id,\n                            'service': service,\n                            'endpoint_id': endpoint['VpcEndpointId'],\n                            'estimated_savings': self._estimate_endpoint_savings(vpc_id, service)\n                        })\n        \n        return implementations\n    \n    def optimize_cloudfront_distribution(self):\n        \"\"\"Optimize CloudFront for cost reduction\"\"\"\n        cloudfront = boto3.client('cloudfront')\n        \n        distributions = cloudfront.list_distributions()\n        optimizations = []\n        \n        for dist in distributions.get('DistributionList', {}).get('Items', []):\n            # Analyze distribution patterns\n            analysis = self._analyze_distribution(dist['Id'])\n            \n            if analysis['optimization_potential']:\n                optimizations.append({\n                    'distribution_id': dist['Id'],\n                    'recommendations': [\n                        {\n                            'action': 'adjust_price_class',\n                            'current': dist['PriceClass'],\n                            'recommended': analysis['recommended_price_class'],\n                            'savings': analysis['price_class_savings']\n                        },\n                        {\n                            'action': 'optimize_cache_behaviors',\n                            'cache_improvements': analysis['cache_improvements'],\n                            'savings': analysis['cache_savings']\n                        }\n                    ]\n                })\n        \n        return optimizations\n'''\n```\n\n### 7. Container Cost Optimization\n\nOptimize container workloads:\n\n**Container Cost Optimizer**\n```python\nclass ContainerCostOptimizer:\n    def optimize_ecs_costs(self):\n        \"\"\"Optimize ECS/Fargate costs\"\"\"\n        return {\n            'cluster_optimization': self._optimize_clusters(),\n            'task_rightsizing': self._rightsize_tasks(),\n            'scheduling_optimization': self._optimize_scheduling(),\n            'fargate_spot': self._implement_fargate_spot()\n        }\n    \n    def _rightsize_tasks(self):\n        \"\"\"Rightsize ECS tasks\"\"\"\n        ecs = boto3.client('ecs')\n        cloudwatch = boto3.client('cloudwatch')\n        \n        clusters = ecs.list_clusters()['clusterArns']\n        recommendations = []\n        \n        for cluster in clusters:\n            # Get services\n            services = ecs.list_services(cluster=cluster)['serviceArns']\n            \n            for service in services:\n                # Get task definition\n                service_detail = ecs.describe_services(\n                    cluster=cluster,\n                    services=[service]\n                )['services'][0]\n                \n                task_def = service_detail['taskDefinition']\n                \n                # Analyze resource utilization\n                utilization = self._analyze_task_utilization(cluster, service)\n                \n                # Generate recommendations\n                if utilization['cpu']['average'] < 30 or utilization['memory']['average'] < 40:\n                    recommendations.append({\n                        'cluster': cluster,\n                        'service': service,\n                        'current_cpu': service_detail['cpu'],\n                        'current_memory': service_detail['memory'],\n                        'recommended_cpu': int(service_detail['cpu'] * 0.7),\n                        'recommended_memory': int(service_detail['memory'] * 0.8),\n                        'monthly_savings': self._calculate_task_savings(\n                            service_detail,\n                            utilization\n                        )\n                    })\n        \n        return recommendations\n    \n    def create_k8s_cost_optimization(self):\n        \"\"\"Kubernetes cost optimization\"\"\"\n        return '''\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cost-optimization-config\ndata:\n  vertical-pod-autoscaler.yaml: |\n    apiVersion: autoscaling.k8s.io/v1\n    kind: VerticalPodAutoscaler\n    metadata:\n      name: app-vpa\n    spec:\n      targetRef:\n        apiVersion: apps/v1\n        kind: Deployment\n        name: app-deployment\n      updatePolicy:\n        updateMode: \"Auto\"\n      resourcePolicy:\n        containerPolicies:\n        - containerName: app\n          minAllowed:\n            cpu: 100m\n            memory: 128Mi\n          maxAllowed:\n            cpu: 2\n            memory: 2Gi\n  \n  cluster-autoscaler-config.yaml: |\n    apiVersion: apps/v1\n    kind: Deployment\n    metadata:\n      name: cluster-autoscaler\n    spec:\n      template:\n        spec:\n          containers:\n          - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0\n            name: cluster-autoscaler\n            command:\n            - ./cluster-autoscaler\n            - --v=4\n            - --stderrthreshold=info\n            - --cloud-provider=aws\n            - --skip-nodes-with-local-storage=false\n            - --expander=priority\n            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/cluster-name\n            - --scale-down-enabled=true\n            - --scale-down-unneeded-time=10m\n            - --scale-down-utilization-threshold=0.5\n  \n  spot-instance-handler.yaml: |\n    apiVersion: apps/v1\n    kind: DaemonSet\n    metadata:\n      name: aws-node-termination-handler\n    spec:\n      selector:\n        matchLabels:\n          app: aws-node-termination-handler\n      template:\n        spec:\n          containers:\n          - name: aws-node-termination-handler\n            image: amazon/aws-node-termination-handler:v1.13.0\n            env:\n            - name: NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: ENABLE_SPOT_INTERRUPTION_DRAINING\n              value: \"true\"\n            - name: ENABLE_SCHEDULED_EVENT_DRAINING\n              value: \"true\"\n'''\n```\n\n### 8. Serverless Cost Optimization\n\nOptimize serverless workloads:\n\n**Serverless Optimizer**\n```python\nclass ServerlessOptimizer:\n    def optimize_lambda_costs(self):\n        \"\"\"Optimize Lambda function costs\"\"\"\n        lambda_client = boto3.client('lambda')\n        cloudwatch = boto3.client('cloudwatch')\n        \n        functions = lambda_client.list_functions()['Functions']\n        optimizations = []\n        \n        for function in functions:\n            # Analyze function performance\n            analysis = self._analyze_lambda_function(function)\n            \n            # Memory optimization\n            if analysis['memory_optimization_possible']:\n                optimizations.append({\n                    'function_name': function['FunctionName'],\n                    'type': 'memory_optimization',\n                    'current_memory': function['MemorySize'],\n                    'recommended_memory': analysis['optimal_memory'],\n                    'estimated_savings': analysis['memory_savings']\n                })\n            \n            # Timeout optimization\n            if analysis['timeout_optimization_possible']:\n                optimizations.append({\n                    'function_name': function['FunctionName'],\n                    'type': 'timeout_optimization',\n                    'current_timeout': function['Timeout'],\n                    'recommended_timeout': analysis['optimal_timeout'],\n                    'risk_reduction': 'prevents unnecessary charges from hanging functions'\n                })\n        \n        return optimizations\n    \n    def implement_lambda_cost_controls(self):\n        \"\"\"Implement Lambda cost controls\"\"\"\n        return '''\nimport json\nimport boto3\nfrom datetime import datetime\n\ndef lambda_cost_controller(event, context):\n    \"\"\"Lambda function to monitor and control Lambda costs\"\"\"\n    \n    cloudwatch = boto3.client('cloudwatch')\n    lambda_client = boto3.client('lambda')\n    \n    # Get current month costs\n    costs = get_current_month_lambda_costs()\n    \n    # Check against budget\n    budget_limit = float(os.environ.get('MONTHLY_BUDGET', '1000'))\n    \n    if costs > budget_limit * 0.8:  # 80% of budget\n        # Implement cost controls\n        high_cost_functions = identify_high_cost_functions()\n        \n        for func in high_cost_functions:\n            # Reduce concurrency\n            lambda_client.put_function_concurrency(\n                FunctionName=func['FunctionName'],\n                ReservedConcurrentExecutions=max(\n                    1, \n                    int(func['CurrentConcurrency'] * 0.5)\n                )\n            )\n            \n            # Alert\n            send_cost_alert(func, costs, budget_limit)\n    \n    # Implement provisioned concurrency optimization\n    optimize_provisioned_concurrency()\n    \n    return {\n        'statusCode': 200,\n        'body': json.dumps({\n            'current_costs': costs,\n            'budget_limit': budget_limit,\n            'actions_taken': len(high_cost_functions)\n        })\n    }\n\ndef optimize_provisioned_concurrency():\n    \"\"\"Optimize provisioned concurrency based on usage patterns\"\"\"\n    functions = get_functions_with_provisioned_concurrency()\n    \n    for func in functions:\n        # Analyze invocation patterns\n        patterns = analyze_invocation_patterns(func['FunctionName'])\n        \n        if patterns['predictable']:\n            # Schedule provisioned concurrency\n            create_scheduled_scaling(\n                func['FunctionName'],\n                patterns['peak_hours'],\n                patterns['peak_concurrency']\n            )\n        else:\n            # Consider removing provisioned concurrency\n            if patterns['avg_cold_starts'] < 10:  # per minute\n                remove_provisioned_concurrency(func['FunctionName'])\n'''\n```\n\n### 9. Cost Allocation and Tagging\n\nImplement cost allocation strategies:\n\n**Cost Allocation Manager**\n```python\nclass CostAllocationManager:\n    def implement_tagging_strategy(self):\n        \"\"\"Implement comprehensive tagging strategy\"\"\"\n        return {\n            'required_tags': [\n                {'key': 'Environment', 'values': ['prod', 'staging', 'dev', 'test']},\n                {'key': 'CostCenter', 'values': 'dynamic'},\n                {'key': 'Project', 'values': 'dynamic'},\n                {'key': 'Owner', 'values': 'dynamic'},\n                {'key': 'Department', 'values': 'dynamic'}\n            ],\n            'automation': self._create_tagging_automation(),\n            'enforcement': self._create_tag_enforcement(),\n            'reporting': self._create_cost_allocation_reports()\n        }\n    \n    def _create_tagging_automation(self):\n        \"\"\"Automate resource tagging\"\"\"\n        return '''\nimport boto3\nfrom datetime import datetime\n\nclass AutoTagger:\n    def __init__(self):\n        self.tag_policies = self.load_tag_policies()\n        \n    def auto_tag_resources(self, event, context):\n        \"\"\"Auto-tag resources on creation\"\"\"\n        \n        # Parse CloudTrail event\n        detail = event['detail']\n        event_name = detail['eventName']\n        \n        # Map events to resource types\n        if event_name.startswith('Create'):\n            resource_arn = self.extract_resource_arn(detail)\n            \n            if resource_arn:\n                # Determine tags\n                tags = self.determine_tags(detail)\n                \n                # Apply tags\n                self.apply_tags(resource_arn, tags)\n                \n                # Log tagging action\n                self.log_tagging(resource_arn, tags)\n    \n    def determine_tags(self, event_detail):\n        \"\"\"Determine tags based on context\"\"\"\n        tags = []\n        \n        # User-based tags\n        user_identity = event_detail.get('userIdentity', {})\n        if 'userName' in user_identity:\n            tags.append({\n                'Key': 'Creator',\n                'Value': user_identity['userName']\n            })\n        \n        # Time-based tags\n        tags.append({\n            'Key': 'CreatedDate',\n            'Value': datetime.now().strftime('%Y-%m-%d')\n        })\n        \n        # Environment inference\n        if 'prod' in event_detail.get('sourceIPAddress', ''):\n            env = 'prod'\n        elif 'dev' in event_detail.get('sourceIPAddress', ''):\n            env = 'dev'\n        else:\n            env = 'unknown'\n            \n        tags.append({\n            'Key': 'Environment',\n            'Value': env\n        })\n        \n        return tags\n    \n    def create_cost_allocation_dashboard(self):\n        \"\"\"Create cost allocation dashboard\"\"\"\n        return \"\"\"\n        SELECT \n            tags.environment,\n            tags.department,\n            tags.project,\n            SUM(costs.amount) as total_cost,\n            SUM(costs.amount) / SUM(SUM(costs.amount)) OVER () * 100 as percentage\n        FROM \n            aws_costs costs\n        JOIN \n            resource_tags tags ON costs.resource_id = tags.resource_id\n        WHERE \n            costs.date >= DATE_TRUNC('month', CURRENT_DATE)\n        GROUP BY \n            tags.environment,\n            tags.department,\n            tags.project\n        ORDER BY \n            total_cost DESC\n        \"\"\"\n'''\n```\n\n### 10. Cost Monitoring and Alerts\n\nImplement proactive cost monitoring:\n\n**Cost Monitoring System**\n```python\nclass CostMonitoringSystem:\n    def setup_cost_alerts(self):\n        \"\"\"Setup comprehensive cost alerting\"\"\"\n        alerts = []\n        \n        # Budget alerts\n        alerts.extend(self._create_budget_alerts())\n        \n        # Anomaly detection\n        alerts.extend(self._create_anomaly_alerts())\n        \n        # Threshold alerts\n        alerts.extend(self._create_threshold_alerts())\n        \n        # Forecast alerts\n        alerts.extend(self._create_forecast_alerts())\n        \n        return alerts\n    \n    def _create_anomaly_alerts(self):\n        \"\"\"Create anomaly detection alerts\"\"\"\n        ce = boto3.client('ce')\n        \n        # Create anomaly monitor\n        monitor = ce.create_anomaly_monitor(\n            AnomalyMonitor={\n                'MonitorName': 'ServiceCostMonitor',\n                'MonitorType': 'DIMENSIONAL',\n                'MonitorDimension': 'SERVICE'\n            }\n        )\n        \n        # Create anomaly subscription\n        subscription = ce.create_anomaly_subscription(\n            AnomalySubscription={\n                'SubscriptionName': 'CostAnomalyAlerts',\n                'Threshold': 100.0,  # Alert on anomalies > $100\n                'Frequency': 'DAILY',\n                'MonitorArnList': [monitor['MonitorArn']],\n                'Subscribers': [\n                    {\n                        'Type': 'EMAIL',\n                        'Address': 'team@company.com'\n                    },\n                    {\n                        'Type': 'SNS',\n                        'Address': 'arn:aws:sns:us-east-1:123456789012:cost-alerts'\n                    }\n                ]\n            }\n        )\n        \n        return [monitor, subscription]\n    \n    def create_cost_dashboard(self):\n        \"\"\"Create executive cost dashboard\"\"\"\n        return '''\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cloud Cost Dashboard</title>\n    <script src=\"https://d3js.org/d3.v7.min.js\"></script>\n    <style>\n        .metric-card {\n            background: #f5f5f5;\n            padding: 20px;\n            margin: 10px;\n            border-radius: 8px;\n            box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n        }\n        .alert { color: #d32f2f; }\n        .warning { color: #f57c00; }\n        .success { color: #388e3c; }\n    </style>\n</head>\n<body>\n    <div id=\"dashboard\">\n        <h1>Cloud Cost Optimization Dashboard</h1>\n        \n        <div class=\"summary-row\">\n            <div class=\"metric-card\">\n                <h3>Current Month Spend</h3>\n                <div class=\"metric\">${current_spend}</div>\n                <div class=\"trend ${spend_trend_class}\">${spend_trend}% vs last month</div>\n            </div>\n            \n            <div class=\"metric-card\">\n                <h3>Projected Month End</h3>\n                <div class=\"metric\">${projected_spend}</div>\n                <div class=\"budget-status\">Budget: ${budget}</div>\n            </div>\n            \n            <div class=\"metric-card\">\n                <h3>Optimization Opportunities</h3>\n                <div class=\"metric\">${total_savings_identified}</div>\n                <div class=\"count\">{opportunity_count} recommendations</div>\n            </div>\n            \n            <div class=\"metric-card\">\n                <h3>Realized Savings</h3>\n                <div class=\"metric\">${realized_savings_mtd}</div>\n                <div class=\"count\">YTD: ${realized_savings_ytd}</div>\n            </div>\n        </div>\n        \n        <div class=\"charts-row\">\n            <div id=\"spend-trend-chart\"></div>\n            <div id=\"service-breakdown-chart\"></div>\n            <div id=\"optimization-progress-chart\"></div>\n        </div>\n        \n        <div class=\"recommendations-section\">\n            <h2>Top Optimization Recommendations</h2>\n            <table id=\"recommendations-table\">\n                <thead>\n                    <tr>\n                        <th>Priority</th>\n                        <th>Service</th>\n                        <th>Recommendation</th>\n                        <th>Monthly Savings</th>\n                        <th>Effort</th>\n                        <th>Action</th>\n                    </tr>\n                </thead>\n                <tbody>\n                    ${recommendation_rows}\n                </tbody>\n            </table>\n        </div>\n    </div>\n    \n    <script>\n        // Real-time updates\n        setInterval(updateDashboard, 60000);\n        \n        // Initialize charts\n        initializeCharts();\n    </script>\n</body>\n</html>\n'''\n```\n\n## Output Format\n\n1. **Cost Analysis Report**: Comprehensive breakdown of current cloud costs\n2. **Optimization Recommendations**: Prioritized list of cost-saving opportunities\n3. **Implementation Scripts**: Automated scripts for implementing optimizations\n4. **Monitoring Dashboards**: Real-time cost tracking and alerting\n5. **ROI Calculations**: Detailed savings projections and payback periods\n6. **Risk Assessment**: Analysis of risks associated with each optimization\n7. **Implementation Roadmap**: Phased approach to cost optimization\n8. **Best Practices Guide**: Long-term cost management strategies\n\nFocus on delivering immediate cost savings while establishing sustainable cost optimization practices that maintain performance and reliability standards."
}