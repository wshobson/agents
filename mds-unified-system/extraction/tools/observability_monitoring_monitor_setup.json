{
  "id": "observability_monitoring_monitor_setup",
  "name": "Monitoring and Observability Setup",
  "source": "observability-monitoring",
  "originalPath": "plugins/observability-monitoring/commands/monitor-setup.md",
  "command": "/observability-monitoring:monitor-setup",
  "parameters": {},
  "outputs": {},
  "agentsUsing": [],
  "fullDefinition": "# Monitoring and Observability Setup\n\nYou are a monitoring and observability expert specializing in implementing comprehensive monitoring solutions. Set up metrics collection, distributed tracing, log aggregation, and create insightful dashboards that provide full visibility into system health and performance.\n\n## Context\nThe user needs to implement or improve monitoring and observability. Focus on the three pillars of observability (metrics, logs, traces), setting up monitoring infrastructure, creating actionable dashboards, and establishing effective alerting strategies.\n\n## Requirements\n$ARGUMENTS\n\n## Instructions\n\n### 1. Prometheus & Metrics Setup\n\n**Prometheus Configuration**\n```yaml\n# prometheus.yml\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n  external_labels:\n    cluster: 'production'\n    region: 'us-east-1'\n\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets: ['alertmanager:9093']\n\nrule_files:\n  - \"alerts/*.yml\"\n  - \"recording_rules/*.yml\"\n\nscrape_configs:\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: ['localhost:9090']\n\n  - job_name: 'node'\n    static_configs:\n      - targets: ['node-exporter:9100']\n\n  - job_name: 'application'\n    kubernetes_sd_configs:\n      - role: pod\n    relabel_configs:\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n```\n\n**Custom Metrics Implementation**\n```typescript\n// metrics.ts\nimport { Counter, Histogram, Gauge, Registry } from 'prom-client';\n\nexport class MetricsCollector {\n    private registry: Registry;\n    private httpRequestDuration: Histogram<string>;\n    private httpRequestTotal: Counter<string>;\n\n    constructor() {\n        this.registry = new Registry();\n        this.initializeMetrics();\n    }\n\n    private initializeMetrics() {\n        this.httpRequestDuration = new Histogram({\n            name: 'http_request_duration_seconds',\n            help: 'Duration of HTTP requests in seconds',\n            labelNames: ['method', 'route', 'status_code'],\n            buckets: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2, 5]\n        });\n\n        this.httpRequestTotal = new Counter({\n            name: 'http_requests_total',\n            help: 'Total number of HTTP requests',\n            labelNames: ['method', 'route', 'status_code']\n        });\n\n        this.registry.registerMetric(this.httpRequestDuration);\n        this.registry.registerMetric(this.httpRequestTotal);\n    }\n\n    httpMetricsMiddleware() {\n        return (req: Request, res: Response, next: NextFunction) => {\n            const start = Date.now();\n            const route = req.route?.path || req.path;\n\n            res.on('finish', () => {\n                const duration = (Date.now() - start) / 1000;\n                const labels = {\n                    method: req.method,\n                    route,\n                    status_code: res.statusCode.toString()\n                };\n\n                this.httpRequestDuration.observe(labels, duration);\n                this.httpRequestTotal.inc(labels);\n            });\n\n            next();\n        };\n    }\n\n    async getMetrics(): Promise<string> {\n        return this.registry.metrics();\n    }\n}\n```\n\n### 2. Grafana Dashboard Setup\n\n**Dashboard Configuration**\n```typescript\n// dashboards/service-dashboard.ts\nexport const createServiceDashboard = (serviceName: string) => {\n    return {\n        title: `${serviceName} Service Dashboard`,\n        uid: `${serviceName}-overview`,\n        tags: ['service', serviceName],\n        time: { from: 'now-6h', to: 'now' },\n        refresh: '30s',\n\n        panels: [\n            // Golden Signals\n            {\n                title: 'Request Rate',\n                type: 'graph',\n                gridPos: { x: 0, y: 0, w: 6, h: 8 },\n                targets: [{\n                    expr: `sum(rate(http_requests_total{service=\"${serviceName}\"}[5m])) by (method)`,\n                    legendFormat: '{{method}}'\n                }]\n            },\n            {\n                title: 'Error Rate',\n                type: 'graph',\n                gridPos: { x: 6, y: 0, w: 6, h: 8 },\n                targets: [{\n                    expr: `sum(rate(http_requests_total{service=\"${serviceName}\",status_code=~\"5..\"}[5m])) / sum(rate(http_requests_total{service=\"${serviceName}\"}[5m]))`,\n                    legendFormat: 'Error %'\n                }]\n            },\n            {\n                title: 'Latency Percentiles',\n                type: 'graph',\n                gridPos: { x: 12, y: 0, w: 12, h: 8 },\n                targets: [\n                    {\n                        expr: `histogram_quantile(0.50, sum(rate(http_request_duration_seconds_bucket{service=\"${serviceName}\"}[5m])) by (le))`,\n                        legendFormat: 'p50'\n                    },\n                    {\n                        expr: `histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{service=\"${serviceName}\"}[5m])) by (le))`,\n                        legendFormat: 'p95'\n                    },\n                    {\n                        expr: `histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket{service=\"${serviceName}\"}[5m])) by (le))`,\n                        legendFormat: 'p99'\n                    }\n                ]\n            }\n        ]\n    };\n};\n```\n\n### 3. Distributed Tracing\n\n**OpenTelemetry Configuration**\n```typescript\n// tracing.ts\nimport { NodeSDK } from '@opentelemetry/sdk-node';\nimport { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';\nimport { Resource } from '@opentelemetry/resources';\nimport { SemanticResourceAttributes } from '@opentelemetry/semantic-conventions';\nimport { JaegerExporter } from '@opentelemetry/exporter-jaeger';\nimport { BatchSpanProcessor } from '@opentelemetry/sdk-trace-base';\n\nexport class TracingSetup {\n    private sdk: NodeSDK;\n\n    constructor(serviceName: string, environment: string) {\n        const jaegerExporter = new JaegerExporter({\n            endpoint: process.env.JAEGER_ENDPOINT || 'http://localhost:14268/api/traces',\n        });\n\n        this.sdk = new NodeSDK({\n            resource: new Resource({\n                [SemanticResourceAttributes.SERVICE_NAME]: serviceName,\n                [SemanticResourceAttributes.SERVICE_VERSION]: process.env.SERVICE_VERSION || '1.0.0',\n                [SemanticResourceAttributes.DEPLOYMENT_ENVIRONMENT]: environment,\n            }),\n\n            traceExporter: jaegerExporter,\n            spanProcessor: new BatchSpanProcessor(jaegerExporter),\n\n            instrumentations: [\n                getNodeAutoInstrumentations({\n                    '@opentelemetry/instrumentation-fs': { enabled: false },\n                }),\n            ],\n        });\n    }\n\n    start() {\n        this.sdk.start()\n            .then(() => console.log('Tracing initialized'))\n            .catch((error) => console.error('Error initializing tracing', error));\n    }\n\n    shutdown() {\n        return this.sdk.shutdown();\n    }\n}\n```\n\n### 4. Log Aggregation\n\n**Fluentd Configuration**\n```yaml\n# fluent.conf\n<source>\n  @type tail\n  path /var/log/containers/*.log\n  pos_file /var/log/fluentd-containers.log.pos\n  tag kubernetes.*\n  <parse>\n    @type json\n    time_format %Y-%m-%dT%H:%M:%S.%NZ\n  </parse>\n</source>\n\n<filter kubernetes.**>\n  @type kubernetes_metadata\n  kubernetes_url \"#{ENV['KUBERNETES_SERVICE_HOST']}\"\n</filter>\n\n<filter kubernetes.**>\n  @type record_transformer\n  <record>\n    cluster_name ${ENV['CLUSTER_NAME']}\n    environment ${ENV['ENVIRONMENT']}\n    @timestamp ${time.strftime('%Y-%m-%dT%H:%M:%S.%LZ')}\n  </record>\n</filter>\n\n<match kubernetes.**>\n  @type elasticsearch\n  host \"#{ENV['FLUENT_ELASTICSEARCH_HOST']}\"\n  port \"#{ENV['FLUENT_ELASTICSEARCH_PORT']}\"\n  index_name logstash\n  logstash_format true\n  <buffer>\n    @type file\n    path /var/log/fluentd-buffers/kubernetes.buffer\n    flush_interval 5s\n    chunk_limit_size 2M\n  </buffer>\n</match>\n```\n\n**Structured Logging Library**\n```python\n# structured_logging.py\nimport json\nimport logging\nfrom datetime import datetime\nfrom typing import Any, Dict, Optional\n\nclass StructuredLogger:\n    def __init__(self, name: str, service: str, version: str):\n        self.logger = logging.getLogger(name)\n        self.service = service\n        self.version = version\n        self.default_context = {\n            'service': service,\n            'version': version,\n            'environment': os.getenv('ENVIRONMENT', 'development')\n        }\n\n    def _format_log(self, level: str, message: str, context: Dict[str, Any]) -> str:\n        log_entry = {\n            '@timestamp': datetime.utcnow().isoformat() + 'Z',\n            'level': level,\n            'message': message,\n            **self.default_context,\n            **context\n        }\n\n        trace_context = self._get_trace_context()\n        if trace_context:\n            log_entry['trace'] = trace_context\n\n        return json.dumps(log_entry)\n\n    def info(self, message: str, **context):\n        log_msg = self._format_log('INFO', message, context)\n        self.logger.info(log_msg)\n\n    def error(self, message: str, error: Optional[Exception] = None, **context):\n        if error:\n            context['error'] = {\n                'type': type(error).__name__,\n                'message': str(error),\n                'stacktrace': traceback.format_exc()\n            }\n\n        log_msg = self._format_log('ERROR', message, context)\n        self.logger.error(log_msg)\n```\n\n### 5. Alert Configuration\n\n**Alert Rules**\n```yaml\n# alerts/application.yml\ngroups:\n  - name: application\n    interval: 30s\n    rules:\n      - alert: HighErrorRate\n        expr: |\n          sum(rate(http_requests_total{status_code=~\"5..\"}[5m])) by (service)\n          / sum(rate(http_requests_total[5m])) by (service) > 0.05\n        for: 5m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"High error rate on {{ $labels.service }}\"\n          description: \"Error rate is {{ $value | humanizePercentage }}\"\n\n      - alert: SlowResponseTime\n        expr: |\n          histogram_quantile(0.95,\n            sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le)\n          ) > 1\n        for: 10m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Slow response time on {{ $labels.service }}\"\n\n  - name: infrastructure\n    rules:\n      - alert: HighCPUUsage\n        expr: avg(rate(container_cpu_usage_seconds_total[5m])) by (pod) > 0.8\n        for: 15m\n        labels:\n          severity: warning\n\n      - alert: HighMemoryUsage\n        expr: |\n          container_memory_working_set_bytes / container_spec_memory_limit_bytes > 0.9\n        for: 10m\n        labels:\n          severity: critical\n```\n\n**Alertmanager Configuration**\n```yaml\n# alertmanager.yml\nglobal:\n  resolve_timeout: 5m\n  slack_api_url: '$SLACK_API_URL'\n\nroute:\n  group_by: ['alertname', 'cluster', 'service']\n  group_wait: 10s\n  group_interval: 10s\n  repeat_interval: 12h\n  receiver: 'default'\n\n  routes:\n    - match:\n        severity: critical\n      receiver: pagerduty\n      continue: true\n\n    - match_re:\n        severity: critical|warning\n      receiver: slack\n\nreceivers:\n  - name: 'slack'\n    slack_configs:\n      - channel: '#alerts'\n        title: '{{ .GroupLabels.alertname }}'\n        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'\n        send_resolved: true\n\n  - name: 'pagerduty'\n    pagerduty_configs:\n      - service_key: '$PAGERDUTY_SERVICE_KEY'\n        description: '{{ .GroupLabels.alertname }}: {{ .Annotations.summary }}'\n```\n\n### 6. SLO Implementation\n\n**SLO Configuration**\n```typescript\n// slo-manager.ts\ninterface SLO {\n    name: string;\n    target: number; // e.g., 99.9\n    window: string; // e.g., '30d'\n    burnRates: BurnRate[];\n}\n\nexport class SLOManager {\n    private slos: SLO[] = [\n        {\n            name: 'API Availability',\n            target: 99.9,\n            window: '30d',\n            burnRates: [\n                { window: '1h', threshold: 14.4, severity: 'critical' },\n                { window: '6h', threshold: 6, severity: 'critical' },\n                { window: '1d', threshold: 3, severity: 'warning' }\n            ]\n        }\n    ];\n\n    generateSLOQueries(): string {\n        return this.slos.map(slo => this.generateSLOQuery(slo)).join('\\n\\n');\n    }\n\n    private generateSLOQuery(slo: SLO): string {\n        const errorBudget = 1 - (slo.target / 100);\n\n        return `\n# ${slo.name} SLO\n- record: slo:${this.sanitizeName(slo.name)}:error_budget\n  expr: ${errorBudget}\n\n- record: slo:${this.sanitizeName(slo.name)}:consumed_error_budget\n  expr: |\n    1 - (sum(rate(successful_requests[${slo.window}])) / sum(rate(total_requests[${slo.window}])))\n        `;\n    }\n}\n```\n\n### 7. Infrastructure as Code\n\n**Terraform Configuration**\n```hcl\n# monitoring.tf\nmodule \"prometheus\" {\n  source = \"./modules/prometheus\"\n\n  namespace = \"monitoring\"\n  storage_size = \"100Gi\"\n  retention_days = 30\n\n  external_labels = {\n    cluster = var.cluster_name\n    region  = var.region\n  }\n}\n\nmodule \"grafana\" {\n  source = \"./modules/grafana\"\n\n  namespace = \"monitoring\"\n  admin_password = var.grafana_admin_password\n\n  datasources = [\n    {\n      name = \"Prometheus\"\n      type = \"prometheus\"\n      url  = \"http://prometheus:9090\"\n    }\n  ]\n}\n\nmodule \"alertmanager\" {\n  source = \"./modules/alertmanager\"\n\n  namespace = \"monitoring\"\n\n  config = templatefile(\"${path.module}/alertmanager.yml\", {\n    slack_webhook = var.slack_webhook\n    pagerduty_key = var.pagerduty_service_key\n  })\n}\n```\n\n## Output Format\n\n1. **Infrastructure Assessment**: Current monitoring capabilities analysis\n2. **Monitoring Architecture**: Complete monitoring stack design\n3. **Implementation Plan**: Step-by-step deployment guide\n4. **Metric Definitions**: Comprehensive metrics catalog\n5. **Dashboard Templates**: Ready-to-use Grafana dashboards\n6. **Alert Runbooks**: Detailed alert response procedures\n7. **SLO Definitions**: Service level objectives and error budgets\n8. **Integration Guide**: Service instrumentation instructions\n\nFocus on creating a monitoring system that provides actionable insights, reduces MTTR, and enables proactive issue detection.\n"
}