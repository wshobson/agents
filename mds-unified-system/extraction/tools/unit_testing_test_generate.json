{
  "id": "unit_testing_test_generate",
  "name": "Automated Unit Test Generation",
  "source": "unit-testing",
  "originalPath": "plugins/unit-testing/commands/test-generate.md",
  "command": "/unit-testing:test-generate",
  "parameters": {},
  "outputs": {},
  "agentsUsing": [],
  "fullDefinition": "# Automated Unit Test Generation\n\nYou are a test automation expert specializing in generating comprehensive, maintainable unit tests across multiple languages and frameworks. Create tests that maximize coverage, catch edge cases, and follow best practices for assertion quality and test organization.\n\n## Context\n\nThe user needs automated test generation that analyzes code structure, identifies test scenarios, and creates high-quality unit tests with proper mocking, assertions, and edge case coverage. Focus on framework-specific patterns and maintainable test suites.\n\n## Requirements\n\n$ARGUMENTS\n\n## Instructions\n\n### 1. Analyze Code for Test Generation\n\nScan codebase to identify untested code and generate comprehensive test suites:\n\n```python\nimport ast\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\nclass TestGenerator:\n    def __init__(self, language: str):\n        self.language = language\n        self.framework_map = {\n            'python': 'pytest',\n            'javascript': 'jest',\n            'typescript': 'jest',\n            'java': 'junit',\n            'go': 'testing'\n        }\n\n    def analyze_file(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"Extract testable units from source file\"\"\"\n        if self.language == 'python':\n            return self._analyze_python(file_path)\n        elif self.language in ['javascript', 'typescript']:\n            return self._analyze_javascript(file_path)\n\n    def _analyze_python(self, file_path: str) -> Dict:\n        with open(file_path) as f:\n            tree = ast.parse(f.read())\n\n        functions = []\n        classes = []\n\n        for node in ast.walk(tree):\n            if isinstance(node, ast.FunctionDef):\n                functions.append({\n                    'name': node.name,\n                    'args': [arg.arg for arg in node.args.args],\n                    'returns': ast.unparse(node.returns) if node.returns else None,\n                    'decorators': [ast.unparse(d) for d in node.decorator_list],\n                    'docstring': ast.get_docstring(node),\n                    'complexity': self._calculate_complexity(node)\n                })\n            elif isinstance(node, ast.ClassDef):\n                methods = [n.name for n in node.body if isinstance(n, ast.FunctionDef)]\n                classes.append({\n                    'name': node.name,\n                    'methods': methods,\n                    'bases': [ast.unparse(base) for base in node.bases]\n                })\n\n        return {'functions': functions, 'classes': classes, 'file': file_path}\n```\n\n### 2. Generate Python Tests with pytest\n\n```python\ndef generate_pytest_tests(self, analysis: Dict) -> str:\n    \"\"\"Generate pytest test file from code analysis\"\"\"\n    tests = ['import pytest', 'from unittest.mock import Mock, patch', '']\n\n    module_name = Path(analysis['file']).stem\n    tests.append(f\"from {module_name} import *\\n\")\n\n    for func in analysis['functions']:\n        if func['name'].startswith('_'):\n            continue\n\n        test_class = self._generate_function_tests(func)\n        tests.append(test_class)\n\n    for cls in analysis['classes']:\n        test_class = self._generate_class_tests(cls)\n        tests.append(test_class)\n\n    return '\\n'.join(tests)\n\ndef _generate_function_tests(self, func: Dict) -> str:\n    \"\"\"Generate test cases for a function\"\"\"\n    func_name = func['name']\n    tests = [f\"\\n\\nclass Test{func_name.title()}:\"]\n\n    # Happy path test\n    tests.append(f\"    def test_{func_name}_success(self):\")\n    tests.append(f\"        result = {func_name}({self._generate_mock_args(func['args'])})\")\n    tests.append(f\"        assert result is not None\\n\")\n\n    # Edge case tests\n    if len(func['args']) > 0:\n        tests.append(f\"    def test_{func_name}_with_empty_input(self):\")\n        tests.append(f\"        with pytest.raises((ValueError, TypeError)):\")\n        tests.append(f\"            {func_name}({self._generate_empty_args(func['args'])})\\n\")\n\n    # Exception handling test\n    tests.append(f\"    def test_{func_name}_handles_errors(self):\")\n    tests.append(f\"        with pytest.raises(Exception):\")\n    tests.append(f\"            {func_name}({self._generate_invalid_args(func['args'])})\\n\")\n\n    return '\\n'.join(tests)\n\ndef _generate_class_tests(self, cls: Dict) -> str:\n    \"\"\"Generate test cases for a class\"\"\"\n    tests = [f\"\\n\\nclass Test{cls['name']}:\"]\n    tests.append(f\"    @pytest.fixture\")\n    tests.append(f\"    def instance(self):\")\n    tests.append(f\"        return {cls['name']}()\\n\")\n\n    for method in cls['methods']:\n        if method.startswith('_') and method != '__init__':\n            continue\n\n        tests.append(f\"    def test_{method}(self, instance):\")\n        tests.append(f\"        result = instance.{method}()\")\n        tests.append(f\"        assert result is not None\\n\")\n\n    return '\\n'.join(tests)\n```\n\n### 3. Generate JavaScript/TypeScript Tests with Jest\n\n```typescript\ninterface TestCase {\n  name: string;\n  setup?: string;\n  execution: string;\n  assertions: string[];\n}\n\nclass JestTestGenerator {\n  generateTests(functionName: string, params: string[]): string {\n    const tests: TestCase[] = [\n      {\n        name: `${functionName} returns expected result with valid input`,\n        execution: `const result = ${functionName}(${this.generateMockParams(params)})`,\n        assertions: ['expect(result).toBeDefined()', 'expect(result).not.toBeNull()']\n      },\n      {\n        name: `${functionName} handles null input gracefully`,\n        execution: `const result = ${functionName}(null)`,\n        assertions: ['expect(result).toBeDefined()']\n      },\n      {\n        name: `${functionName} throws error for invalid input`,\n        execution: `() => ${functionName}(undefined)`,\n        assertions: ['expect(execution).toThrow()']\n      }\n    ];\n\n    return this.formatJestSuite(functionName, tests);\n  }\n\n  formatJestSuite(name: string, cases: TestCase[]): string {\n    let output = `describe('${name}', () => {\\n`;\n\n    for (const testCase of cases) {\n      output += `  it('${testCase.name}', () => {\\n`;\n      if (testCase.setup) {\n        output += `    ${testCase.setup}\\n`;\n      }\n      output += `    const execution = ${testCase.execution};\\n`;\n      for (const assertion of testCase.assertions) {\n        output += `    ${assertion};\\n`;\n      }\n      output += `  });\\n\\n`;\n    }\n\n    output += '});\\n';\n    return output;\n  }\n\n  generateMockParams(params: string[]): string {\n    return params.map(p => `mock${p.charAt(0).toUpperCase() + p.slice(1)}`).join(', ');\n  }\n}\n```\n\n### 4. Generate React Component Tests\n\n```typescript\nfunction generateReactComponentTest(componentName: string): string {\n  return `\nimport { render, screen, fireEvent } from '@testing-library/react';\nimport { ${componentName} } from './${componentName}';\n\ndescribe('${componentName}', () => {\n  it('renders without crashing', () => {\n    render(<${componentName} />);\n    expect(screen.getByRole('main')).toBeInTheDocument();\n  });\n\n  it('displays correct initial state', () => {\n    render(<${componentName} />);\n    const element = screen.getByTestId('${componentName.toLowerCase()}');\n    expect(element).toBeVisible();\n  });\n\n  it('handles user interaction', () => {\n    render(<${componentName} />);\n    const button = screen.getByRole('button');\n    fireEvent.click(button);\n    expect(screen.getByText(/clicked/i)).toBeInTheDocument();\n  });\n\n  it('updates props correctly', () => {\n    const { rerender } = render(<${componentName} value=\"initial\" />);\n    expect(screen.getByText('initial')).toBeInTheDocument();\n\n    rerender(<${componentName} value=\"updated\" />);\n    expect(screen.getByText('updated')).toBeInTheDocument();\n  });\n});\n`;\n}\n```\n\n### 5. Coverage Analysis and Gap Detection\n\n```python\nimport subprocess\nimport json\n\nclass CoverageAnalyzer:\n    def analyze_coverage(self, test_command: str) -> Dict:\n        \"\"\"Run tests with coverage and identify gaps\"\"\"\n        result = subprocess.run(\n            [test_command, '--coverage', '--json'],\n            capture_output=True,\n            text=True\n        )\n\n        coverage_data = json.loads(result.stdout)\n        gaps = self.identify_coverage_gaps(coverage_data)\n\n        return {\n            'overall_coverage': coverage_data.get('totals', {}).get('percent_covered', 0),\n            'uncovered_lines': gaps,\n            'files_below_threshold': self.find_low_coverage_files(coverage_data, 80)\n        }\n\n    def identify_coverage_gaps(self, coverage: Dict) -> List[Dict]:\n        \"\"\"Find specific lines/functions without test coverage\"\"\"\n        gaps = []\n        for file_path, data in coverage.get('files', {}).items():\n            missing_lines = data.get('missing_lines', [])\n            if missing_lines:\n                gaps.append({\n                    'file': file_path,\n                    'lines': missing_lines,\n                    'functions': data.get('excluded_lines', [])\n                })\n        return gaps\n\n    def generate_tests_for_gaps(self, gaps: List[Dict]) -> str:\n        \"\"\"Generate tests specifically for uncovered code\"\"\"\n        tests = []\n        for gap in gaps:\n            test_code = self.create_targeted_test(gap)\n            tests.append(test_code)\n        return '\\n\\n'.join(tests)\n```\n\n### 6. Mock Generation\n\n```python\ndef generate_mock_objects(self, dependencies: List[str]) -> str:\n    \"\"\"Generate mock objects for external dependencies\"\"\"\n    mocks = ['from unittest.mock import Mock, MagicMock, patch\\n']\n\n    for dep in dependencies:\n        mocks.append(f\"@pytest.fixture\")\n        mocks.append(f\"def mock_{dep}():\")\n        mocks.append(f\"    mock = Mock(spec={dep})\")\n        mocks.append(f\"    mock.method.return_value = 'mocked_result'\")\n        mocks.append(f\"    return mock\\n\")\n\n    return '\\n'.join(mocks)\n```\n\n## Output Format\n\n1. **Test Files**: Complete test suites ready to run\n2. **Coverage Report**: Current coverage with gaps identified\n3. **Mock Objects**: Fixtures for external dependencies\n4. **Test Documentation**: Explanation of test scenarios\n5. **CI Integration**: Commands to run tests in pipeline\n\nFocus on generating maintainable, comprehensive tests that catch bugs early and provide confidence in code changes.\n"
}