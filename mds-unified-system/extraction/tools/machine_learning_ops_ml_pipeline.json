{
  "id": "machine_learning_ops_ml_pipeline",
  "name": "Machine Learning Pipeline - Multi-Agent MLOps Orchestration",
  "source": "machine-learning-ops",
  "originalPath": "plugins/machine-learning-ops/commands/ml-pipeline.md",
  "command": "/machine-learning-ops:ml-pipeline",
  "parameters": {},
  "outputs": {},
  "agentsUsing": [],
  "fullDefinition": "# Machine Learning Pipeline - Multi-Agent MLOps Orchestration\n\nDesign and implement a complete ML pipeline for: $ARGUMENTS\n\n## Thinking\n\nThis workflow orchestrates multiple specialized agents to build a production-ready ML pipeline following modern MLOps best practices. The approach emphasizes:\n\n- **Phase-based coordination**: Each phase builds upon previous outputs, with clear handoffs between agents\n- **Modern tooling integration**: MLflow/W&B for experiments, Feast/Tecton for features, KServe/Seldon for serving\n- **Production-first mindset**: Every component designed for scale, monitoring, and reliability\n- **Reproducibility**: Version control for data, models, and infrastructure\n- **Continuous improvement**: Automated retraining, A/B testing, and drift detection\n\nThe multi-agent approach ensures each aspect is handled by domain experts:\n- Data engineers handle ingestion and quality\n- Data scientists design features and experiments\n- ML engineers implement training pipelines\n- MLOps engineers handle production deployment\n- Observability engineers ensure monitoring\n\n## Phase 1: Data & Requirements Analysis\n\n<Task>\nsubagent_type: data-engineer\nprompt: |\n  Analyze and design data pipeline for ML system with requirements: $ARGUMENTS\n\n  Deliverables:\n  1. Data source audit and ingestion strategy:\n     - Source systems and connection patterns\n     - Schema validation using Pydantic/Great Expectations\n     - Data versioning with DVC or lakeFS\n     - Incremental loading and CDC strategies\n\n  2. Data quality framework:\n     - Profiling and statistics generation\n     - Anomaly detection rules\n     - Data lineage tracking\n     - Quality gates and SLAs\n\n  3. Storage architecture:\n     - Raw/processed/feature layers\n     - Partitioning strategy\n     - Retention policies\n     - Cost optimization\n\n  Provide implementation code for critical components and integration patterns.\n</Task>\n\n<Task>\nsubagent_type: data-scientist\nprompt: |\n  Design feature engineering and model requirements for: $ARGUMENTS\n  Using data architecture from: {phase1.data-engineer.output}\n\n  Deliverables:\n  1. Feature engineering pipeline:\n     - Transformation specifications\n     - Feature store schema (Feast/Tecton)\n     - Statistical validation rules\n     - Handling strategies for missing data/outliers\n\n  2. Model requirements:\n     - Algorithm selection rationale\n     - Performance metrics and baselines\n     - Training data requirements\n     - Evaluation criteria and thresholds\n\n  3. Experiment design:\n     - Hypothesis and success metrics\n     - A/B testing methodology\n     - Sample size calculations\n     - Bias detection approach\n\n  Include feature transformation code and statistical validation logic.\n</Task>\n\n## Phase 2: Model Development & Training\n\n<Task>\nsubagent_type: ml-engineer\nprompt: |\n  Implement training pipeline based on requirements: {phase1.data-scientist.output}\n  Using data pipeline: {phase1.data-engineer.output}\n\n  Build comprehensive training system:\n  1. Training pipeline implementation:\n     - Modular training code with clear interfaces\n     - Hyperparameter optimization (Optuna/Ray Tune)\n     - Distributed training support (Horovod/PyTorch DDP)\n     - Cross-validation and ensemble strategies\n\n  2. Experiment tracking setup:\n     - MLflow/Weights & Biases integration\n     - Metric logging and visualization\n     - Artifact management (models, plots, data samples)\n     - Experiment comparison and analysis tools\n\n  3. Model registry integration:\n     - Version control and tagging strategy\n     - Model metadata and lineage\n     - Promotion workflows (dev -> staging -> prod)\n     - Rollback procedures\n\n  Provide complete training code with configuration management.\n</Task>\n\n<Task>\nsubagent_type: python-pro\nprompt: |\n  Optimize and productionize ML code from: {phase2.ml-engineer.output}\n\n  Focus areas:\n  1. Code quality and structure:\n     - Refactor for production standards\n     - Add comprehensive error handling\n     - Implement proper logging with structured formats\n     - Create reusable components and utilities\n\n  2. Performance optimization:\n     - Profile and optimize bottlenecks\n     - Implement caching strategies\n     - Optimize data loading and preprocessing\n     - Memory management for large-scale training\n\n  3. Testing framework:\n     - Unit tests for data transformations\n     - Integration tests for pipeline components\n     - Model quality tests (invariance, directional)\n     - Performance regression tests\n\n  Deliver production-ready, maintainable code with full test coverage.\n</Task>\n\n## Phase 3: Production Deployment & Serving\n\n<Task>\nsubagent_type: mlops-engineer\nprompt: |\n  Design production deployment for models from: {phase2.ml-engineer.output}\n  With optimized code from: {phase2.python-pro.output}\n\n  Implementation requirements:\n  1. Model serving infrastructure:\n     - REST/gRPC APIs with FastAPI/TorchServe\n     - Batch prediction pipelines (Airflow/Kubeflow)\n     - Stream processing (Kafka/Kinesis integration)\n     - Model serving platforms (KServe/Seldon Core)\n\n  2. Deployment strategies:\n     - Blue-green deployments for zero downtime\n     - Canary releases with traffic splitting\n     - Shadow deployments for validation\n     - A/B testing infrastructure\n\n  3. CI/CD pipeline:\n     - GitHub Actions/GitLab CI workflows\n     - Automated testing gates\n     - Model validation before deployment\n     - ArgoCD for GitOps deployment\n\n  4. Infrastructure as Code:\n     - Terraform modules for cloud resources\n     - Helm charts for Kubernetes deployments\n     - Docker multi-stage builds for optimization\n     - Secret management with Vault/Secrets Manager\n\n  Provide complete deployment configuration and automation scripts.\n</Task>\n\n<Task>\nsubagent_type: kubernetes-architect\nprompt: |\n  Design Kubernetes infrastructure for ML workloads from: {phase3.mlops-engineer.output}\n\n  Kubernetes-specific requirements:\n  1. Workload orchestration:\n     - Training job scheduling with Kubeflow\n     - GPU resource allocation and sharing\n     - Spot/preemptible instance integration\n     - Priority classes and resource quotas\n\n  2. Serving infrastructure:\n     - HPA/VPA for autoscaling\n     - KEDA for event-driven scaling\n     - Istio service mesh for traffic management\n     - Model caching and warm-up strategies\n\n  3. Storage and data access:\n     - PVC strategies for training data\n     - Model artifact storage with CSI drivers\n     - Distributed storage for feature stores\n     - Cache layers for inference optimization\n\n  Provide Kubernetes manifests and Helm charts for entire ML platform.\n</Task>\n\n## Phase 4: Monitoring & Continuous Improvement\n\n<Task>\nsubagent_type: observability-engineer\nprompt: |\n  Implement comprehensive monitoring for ML system deployed in: {phase3.mlops-engineer.output}\n  Using Kubernetes infrastructure: {phase3.kubernetes-architect.output}\n\n  Monitoring framework:\n  1. Model performance monitoring:\n     - Prediction accuracy tracking\n     - Latency and throughput metrics\n     - Feature importance shifts\n     - Business KPI correlation\n\n  2. Data and model drift detection:\n     - Statistical drift detection (KS test, PSI)\n     - Concept drift monitoring\n     - Feature distribution tracking\n     - Automated drift alerts and reports\n\n  3. System observability:\n     - Prometheus metrics for all components\n     - Grafana dashboards for visualization\n     - Distributed tracing with Jaeger/Zipkin\n     - Log aggregation with ELK/Loki\n\n  4. Alerting and automation:\n     - PagerDuty/Opsgenie integration\n     - Automated retraining triggers\n     - Performance degradation workflows\n     - Incident response runbooks\n\n  5. Cost tracking:\n     - Resource utilization metrics\n     - Cost allocation by model/experiment\n     - Optimization recommendations\n     - Budget alerts and controls\n\n  Deliver monitoring configuration, dashboards, and alert rules.\n</Task>\n\n## Configuration Options\n\n- **experiment_tracking**: mlflow | wandb | neptune | clearml\n- **feature_store**: feast | tecton | databricks | custom\n- **serving_platform**: kserve | seldon | torchserve | triton\n- **orchestration**: kubeflow | airflow | prefect | dagster\n- **cloud_provider**: aws | azure | gcp | multi-cloud\n- **deployment_mode**: realtime | batch | streaming | hybrid\n- **monitoring_stack**: prometheus | datadog | newrelic | custom\n\n## Success Criteria\n\n1. **Data Pipeline Success**:\n   - < 0.1% data quality issues in production\n   - Automated data validation passing 99.9% of time\n   - Complete data lineage tracking\n   - Sub-second feature serving latency\n\n2. **Model Performance**:\n   - Meeting or exceeding baseline metrics\n   - < 5% performance degradation before retraining\n   - Successful A/B tests with statistical significance\n   - No undetected model drift > 24 hours\n\n3. **Operational Excellence**:\n   - 99.9% uptime for model serving\n   - < 200ms p99 inference latency\n   - Automated rollback within 5 minutes\n   - Complete observability with < 1 minute alert time\n\n4. **Development Velocity**:\n   - < 1 hour from commit to production\n   - Parallel experiment execution\n   - Reproducible training runs\n   - Self-service model deployment\n\n5. **Cost Efficiency**:\n   - < 20% infrastructure waste\n   - Optimized resource allocation\n   - Automatic scaling based on load\n   - Spot instance utilization > 60%\n\n## Final Deliverables\n\nUpon completion, the orchestrated pipeline will provide:\n- End-to-end ML pipeline with full automation\n- Comprehensive documentation and runbooks\n- Production-ready infrastructure as code\n- Complete monitoring and alerting system\n- CI/CD pipelines for continuous improvement\n- Cost optimization and scaling strategies\n- Disaster recovery and rollback procedures"
}