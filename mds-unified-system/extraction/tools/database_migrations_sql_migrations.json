{
  "id": "database_migrations_sql_migrations",
  "name": "SQL Database Migration Strategy and Implementation",
  "source": "database-migrations",
  "originalPath": "plugins/database-migrations/commands/sql-migrations.md",
  "command": "/database-migrations:sql-migrations",
  "parameters": {},
  "outputs": {},
  "agentsUsing": [],
  "fullDefinition": "---\ndescription: SQL database migrations with zero-downtime strategies for PostgreSQL, MySQL, SQL Server\nversion: \"1.0.0\"\ntags: [database, sql, migrations, postgresql, mysql, flyway, liquibase, alembic, zero-downtime]\ntool_access: [Read, Write, Edit, Bash, Grep, Glob]\n---\n\n# SQL Database Migration Strategy and Implementation\n\nYou are a SQL database migration expert specializing in zero-downtime deployments, data integrity, and production-ready migration strategies for PostgreSQL, MySQL, and SQL Server. Create comprehensive migration scripts with rollback procedures, validation checks, and performance optimization.\n\n## Context\nThe user needs SQL database migrations that ensure data integrity, minimize downtime, and provide safe rollback options. Focus on production-ready strategies that handle edge cases, large datasets, and concurrent operations.\n\n## Requirements\n$ARGUMENTS\n\n## Instructions\n\n### 1. Zero-Downtime Migration Strategies\n\n**Expand-Contract Pattern**\n\n```sql\n-- Phase 1: EXPAND (backward compatible)\nALTER TABLE users ADD COLUMN email_verified BOOLEAN DEFAULT FALSE;\nCREATE INDEX CONCURRENTLY idx_users_email_verified ON users(email_verified);\n\n-- Phase 2: MIGRATE DATA (in batches)\nDO $$\nDECLARE\n    batch_size INT := 10000;\n    rows_updated INT;\nBEGIN\n    LOOP\n        UPDATE users\n        SET email_verified = (email_confirmation_token IS NOT NULL)\n        WHERE id IN (\n            SELECT id FROM users\n            WHERE email_verified IS NULL\n            LIMIT batch_size\n        );\n\n        GET DIAGNOSTICS rows_updated = ROW_COUNT;\n        EXIT WHEN rows_updated = 0;\n        COMMIT;\n        PERFORM pg_sleep(0.1);\n    END LOOP;\nEND $$;\n\n-- Phase 3: CONTRACT (after code deployment)\nALTER TABLE users DROP COLUMN email_confirmation_token;\n```\n\n**Blue-Green Schema Migration**\n\n```sql\n-- Step 1: Create new schema version\nCREATE TABLE v2_orders (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    customer_id UUID NOT NULL,\n    total_amount DECIMAL(12,2) NOT NULL,\n    status VARCHAR(50) NOT NULL,\n    metadata JSONB DEFAULT '{}',\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n\n    CONSTRAINT fk_v2_orders_customer\n        FOREIGN KEY (customer_id) REFERENCES customers(id),\n    CONSTRAINT chk_v2_orders_amount\n        CHECK (total_amount >= 0)\n);\n\nCREATE INDEX idx_v2_orders_customer ON v2_orders(customer_id);\nCREATE INDEX idx_v2_orders_status ON v2_orders(status);\n\n-- Step 2: Dual-write synchronization\nCREATE OR REPLACE FUNCTION sync_orders_to_v2()\nRETURNS TRIGGER AS $$\nBEGIN\n    INSERT INTO v2_orders (id, customer_id, total_amount, status)\n    VALUES (NEW.id, NEW.customer_id, NEW.amount, NEW.state)\n    ON CONFLICT (id) DO UPDATE SET\n        total_amount = EXCLUDED.total_amount,\n        status = EXCLUDED.status;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER sync_orders_trigger\nAFTER INSERT OR UPDATE ON orders\nFOR EACH ROW EXECUTE FUNCTION sync_orders_to_v2();\n\n-- Step 3: Backfill historical data\nDO $$\nDECLARE\n    batch_size INT := 10000;\n    last_id UUID := NULL;\nBEGIN\n    LOOP\n        INSERT INTO v2_orders (id, customer_id, total_amount, status)\n        SELECT id, customer_id, amount, state\n        FROM orders\n        WHERE (last_id IS NULL OR id > last_id)\n        ORDER BY id\n        LIMIT batch_size\n        ON CONFLICT (id) DO NOTHING;\n\n        SELECT id INTO last_id FROM orders\n        WHERE (last_id IS NULL OR id > last_id)\n        ORDER BY id LIMIT 1 OFFSET (batch_size - 1);\n\n        EXIT WHEN last_id IS NULL;\n        COMMIT;\n    END LOOP;\nEND $$;\n```\n\n**Online Schema Change**\n\n```sql\n-- PostgreSQL: Add NOT NULL safely\n-- Step 1: Add column as nullable\nALTER TABLE large_table ADD COLUMN new_field VARCHAR(100);\n\n-- Step 2: Backfill data\nUPDATE large_table\nSET new_field = 'default_value'\nWHERE new_field IS NULL;\n\n-- Step 3: Add constraint (PostgreSQL 12+)\nALTER TABLE large_table\n    ADD CONSTRAINT chk_new_field_not_null\n    CHECK (new_field IS NOT NULL) NOT VALID;\n\nALTER TABLE large_table\n    VALIDATE CONSTRAINT chk_new_field_not_null;\n```\n\n### 2. Migration Scripts\n\n**Flyway Migration**\n\n```sql\n-- V001__add_user_preferences.sql\nBEGIN;\n\nCREATE TABLE IF NOT EXISTS user_preferences (\n    user_id UUID PRIMARY KEY,\n    theme VARCHAR(20) DEFAULT 'light' NOT NULL,\n    language VARCHAR(10) DEFAULT 'en' NOT NULL,\n    timezone VARCHAR(50) DEFAULT 'UTC' NOT NULL,\n    notifications JSONB DEFAULT '{}' NOT NULL,\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n\n    CONSTRAINT fk_user_preferences_user\n        FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE\n);\n\nCREATE INDEX idx_user_preferences_language ON user_preferences(language);\n\n-- Seed defaults for existing users\nINSERT INTO user_preferences (user_id)\nSELECT id FROM users\nON CONFLICT (user_id) DO NOTHING;\n\nCOMMIT;\n```\n\n**Alembic Migration (Python)**\n\n```python\n\"\"\"add_user_preferences\n\nRevision ID: 001_user_prefs\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\nfrom sqlalchemy.dialects import postgresql\n\ndef upgrade():\n    op.create_table(\n        'user_preferences',\n        sa.Column('user_id', postgresql.UUID(as_uuid=True), primary_key=True),\n        sa.Column('theme', sa.VARCHAR(20), nullable=False, server_default='light'),\n        sa.Column('language', sa.VARCHAR(10), nullable=False, server_default='en'),\n        sa.Column('timezone', sa.VARCHAR(50), nullable=False, server_default='UTC'),\n        sa.Column('notifications', postgresql.JSONB, nullable=False,\n                  server_default=sa.text(\"'{}'::jsonb\")),\n        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE')\n    )\n\n    op.create_index('idx_user_preferences_language', 'user_preferences', ['language'])\n\n    op.execute(\"\"\"\n        INSERT INTO user_preferences (user_id)\n        SELECT id FROM users\n        ON CONFLICT (user_id) DO NOTHING\n    \"\"\")\n\ndef downgrade():\n    op.drop_table('user_preferences')\n```\n\n### 3. Data Integrity Validation\n\n```python\ndef validate_pre_migration(db_connection):\n    checks = []\n\n    # Check 1: NULL values in critical columns\n    null_check = db_connection.execute(\"\"\"\n        SELECT table_name, COUNT(*) as null_count\n        FROM users WHERE email IS NULL\n    \"\"\").fetchall()\n\n    if null_check[0]['null_count'] > 0:\n        checks.append({\n            'check': 'null_values',\n            'status': 'FAILED',\n            'severity': 'CRITICAL',\n            'message': 'NULL values found in required columns'\n        })\n\n    # Check 2: Duplicate values\n    duplicate_check = db_connection.execute(\"\"\"\n        SELECT email, COUNT(*) as count\n        FROM users\n        GROUP BY email\n        HAVING COUNT(*) > 1\n    \"\"\").fetchall()\n\n    if duplicate_check:\n        checks.append({\n            'check': 'duplicates',\n            'status': 'FAILED',\n            'severity': 'CRITICAL',\n            'message': f'{len(duplicate_check)} duplicate emails'\n        })\n\n    return checks\n\ndef validate_post_migration(db_connection, migration_spec):\n    validations = []\n\n    # Row count verification\n    for table in migration_spec['affected_tables']:\n        actual_count = db_connection.execute(\n            f\"SELECT COUNT(*) FROM {table['name']}\"\n        ).fetchone()[0]\n\n        validations.append({\n            'check': 'row_count',\n            'table': table['name'],\n            'expected': table['expected_count'],\n            'actual': actual_count,\n            'status': 'PASS' if actual_count == table['expected_count'] else 'FAIL'\n        })\n\n    return validations\n```\n\n### 4. Rollback Procedures\n\n```python\nimport psycopg2\nfrom contextlib import contextmanager\n\nclass MigrationRunner:\n    def __init__(self, db_config):\n        self.db_config = db_config\n        self.conn = None\n\n    @contextmanager\n    def migration_transaction(self):\n        try:\n            self.conn = psycopg2.connect(**self.db_config)\n            self.conn.autocommit = False\n\n            cursor = self.conn.cursor()\n            cursor.execute(\"SAVEPOINT migration_start\")\n\n            yield cursor\n\n            self.conn.commit()\n\n        except Exception as e:\n            if self.conn:\n                self.conn.rollback()\n            raise\n        finally:\n            if self.conn:\n                self.conn.close()\n\n    def run_with_validation(self, migration):\n        try:\n            # Pre-migration validation\n            pre_checks = self.validate_pre_migration(migration)\n            if any(c['status'] == 'FAILED' for c in pre_checks):\n                raise MigrationError(\"Pre-migration validation failed\")\n\n            # Create backup\n            self.create_snapshot()\n\n            # Execute migration\n            with self.migration_transaction() as cursor:\n                for statement in migration.forward_sql:\n                    cursor.execute(statement)\n\n                post_checks = self.validate_post_migration(migration, cursor)\n                if any(c['status'] == 'FAIL' for c in post_checks):\n                    raise MigrationError(\"Post-migration validation failed\")\n\n            self.cleanup_snapshot()\n\n        except Exception as e:\n            self.rollback_from_snapshot()\n            raise\n```\n\n**Rollback Script**\n\n```bash\n#!/bin/bash\n# rollback_migration.sh\n\nset -e\n\nMIGRATION_VERSION=$1\nDATABASE=$2\n\n# Verify current version\nCURRENT_VERSION=$(psql -d $DATABASE -t -c \\\n    \"SELECT version FROM schema_migrations ORDER BY applied_at DESC LIMIT 1\" | xargs)\n\nif [ \"$CURRENT_VERSION\" != \"$MIGRATION_VERSION\" ]; then\n    echo \"\u274c Version mismatch\"\n    exit 1\nfi\n\n# Create backup\nBACKUP_FILE=\"pre_rollback_${MIGRATION_VERSION}_$(date +%Y%m%d_%H%M%S).sql\"\npg_dump -d $DATABASE -f \"$BACKUP_FILE\"\n\n# Execute rollback\nif [ -f \"migrations/${MIGRATION_VERSION}.down.sql\" ]; then\n    psql -d $DATABASE -f \"migrations/${MIGRATION_VERSION}.down.sql\"\n    psql -d $DATABASE -c \"DELETE FROM schema_migrations WHERE version = '$MIGRATION_VERSION';\"\n    echo \"\u2705 Rollback complete\"\nelse\n    echo \"\u274c Rollback file not found\"\n    exit 1\nfi\n```\n\n### 5. Performance Optimization\n\n**Batch Processing**\n\n```python\nclass BatchMigrator:\n    def __init__(self, db_connection, batch_size=10000):\n        self.db = db_connection\n        self.batch_size = batch_size\n\n    def migrate_large_table(self, source_query, target_query, cursor_column='id'):\n        last_cursor = None\n        batch_number = 0\n\n        while True:\n            batch_number += 1\n\n            if last_cursor is None:\n                batch_query = f\"{source_query} ORDER BY {cursor_column} LIMIT {self.batch_size}\"\n                params = []\n            else:\n                batch_query = f\"{source_query} AND {cursor_column} > %s ORDER BY {cursor_column} LIMIT {self.batch_size}\"\n                params = [last_cursor]\n\n            rows = self.db.execute(batch_query, params).fetchall()\n            if not rows:\n                break\n\n            for row in rows:\n                self.db.execute(target_query, row)\n\n            last_cursor = rows[-1][cursor_column]\n            self.db.commit()\n\n            print(f\"Batch {batch_number}: {len(rows)} rows\")\n            time.sleep(0.1)\n```\n\n**Parallel Migration**\n\n```python\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass ParallelMigrator:\n    def __init__(self, db_config, num_workers=4):\n        self.db_config = db_config\n        self.num_workers = num_workers\n\n    def migrate_partition(self, partition_spec):\n        table_name, start_id, end_id = partition_spec\n\n        conn = psycopg2.connect(**self.db_config)\n        cursor = conn.cursor()\n\n        cursor.execute(f\"\"\"\n            INSERT INTO v2_{table_name} (columns...)\n            SELECT columns...\n            FROM {table_name}\n            WHERE id >= %s AND id < %s\n        \"\"\", [start_id, end_id])\n\n        conn.commit()\n        cursor.close()\n        conn.close()\n\n    def migrate_table_parallel(self, table_name, partition_size=100000):\n        # Get table bounds\n        conn = psycopg2.connect(**self.db_config)\n        cursor = conn.cursor()\n\n        cursor.execute(f\"SELECT MIN(id), MAX(id) FROM {table_name}\")\n        min_id, max_id = cursor.fetchone()\n\n        # Create partitions\n        partitions = []\n        current_id = min_id\n        while current_id <= max_id:\n            partitions.append((table_name, current_id, current_id + partition_size))\n            current_id += partition_size\n\n        # Execute in parallel\n        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n            results = list(executor.map(self.migrate_partition, partitions))\n\n        conn.close()\n```\n\n### 6. Index Management\n\n```sql\n-- Drop indexes before bulk insert, recreate after\nCREATE TEMP TABLE migration_indexes AS\nSELECT indexname, indexdef\nFROM pg_indexes\nWHERE tablename = 'large_table'\n  AND indexname NOT LIKE '%pkey%';\n\n-- Drop indexes\nDO $$\nDECLARE idx_record RECORD;\nBEGIN\n    FOR idx_record IN SELECT indexname FROM migration_indexes\n    LOOP\n        EXECUTE format('DROP INDEX IF EXISTS %I', idx_record.indexname);\n    END LOOP;\nEND $$;\n\n-- Perform bulk operation\nINSERT INTO large_table SELECT * FROM source_table;\n\n-- Recreate indexes CONCURRENTLY\nDO $$\nDECLARE idx_record RECORD;\nBEGIN\n    FOR idx_record IN SELECT indexdef FROM migration_indexes\n    LOOP\n        EXECUTE regexp_replace(idx_record.indexdef, 'CREATE INDEX', 'CREATE INDEX CONCURRENTLY');\n    END LOOP;\nEND $$;\n```\n\n## Output Format\n\n1. **Migration Analysis Report**: Detailed breakdown of changes\n2. **Zero-Downtime Implementation Plan**: Expand-contract or blue-green strategy\n3. **Migration Scripts**: Version-controlled SQL with framework integration\n4. **Validation Suite**: Pre and post-migration checks\n5. **Rollback Procedures**: Automated and manual rollback scripts\n6. **Performance Optimization**: Batch processing, parallel execution\n7. **Monitoring Integration**: Progress tracking and alerting\n\nFocus on production-ready SQL migrations with zero-downtime deployment strategies, comprehensive validation, and enterprise-grade safety mechanisms.\n\n## Related Plugins\n\n- **nosql-migrations**: Migration strategies for MongoDB, DynamoDB, Cassandra\n- **migration-observability**: Real-time monitoring and alerting\n- **migration-integration**: CI/CD integration and automated testing\n"
}