{
  "id": "llm_application_dev_vector_index_tuning",
  "name": "vector-index-tuning",
  "source": "llm-application-dev",
  "originalPath": "plugins/llm-application-dev/skills/vector-index-tuning/SKILL.md",
  "activationCriteria": "Optimize vector index performance for latency, recall, and memory. Use when tuning HNSW parameters, selecting quantization strategies, or scaling vector search infrastructure.",
  "tier1_metadata": "vector-index-tuning: Optimize vector index performance for latency, recall, and memory. Use when tuning HNSW parameters, ",
  "tier2_instructions": "# Vector Index Tuning\n\nGuide to optimizing vector indexes for production performance.\n\n## When to Use This Skill\n\n- Tuning HNSW parameters\n- Implementing quantization\n- Optimizing memory usage\n- Reducing search latency\n- Balancing recall vs speed\n- Scaling to billions of vectors\n\n## Core Concepts\n\n### 1. Index Type Selection\n\n```\nData Size           Recommended Index\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n< 10K vectors  \u2192    Flat (exact search)\n10K - 1M       \u2192    HNSW\n1M - 100M      \u2192    HNSW + Quantization\n> 100M         \u2192    IVF + PQ or DiskANN\n```\n\n### 2. HNSW Parameters\n\n| Parameter | Default | Effect |\n|-----------|---------|--------|\n| **M** | 16 | Connections per node, \u2191 = better recall, more memory |\n| **efConstruction** | 100 | Build quality, \u2191 = better index, slower build |\n| **efSearch** | 50 | Search quality, \u2191 = better recall, slower search |\n\n### 3. Quantization Types\n\n```\nFull Precision (FP32): 4 bytes \u00d7 dimensions\nHalf Precision (FP16): 2 bytes \u00d7 dimensions\nINT8 Scalar:           1 byte \u00d7 dimensions\nProduct Quantization:  ~32-64 bytes total\nBinary:                dimensions/8 bytes\n```\n\n## Templates\n\n### Template 1: HNSW Parameter Tuning\n\n```python\nimport numpy as np\nfrom typing import List, Tuple\nimport time\n\ndef benchmark_hnsw_parameters(\n    vectors: np.ndarray,\n    queries: np.ndarray,\n    ground_truth: np.ndarray,\n    m_values: List[int] = [8, 16, 32, 64],\n    ef_construction_values: List[int] = [64, 128, 256],\n    ef_search_values: List[int] = [32, 64, 128, 256]\n) -> List[dict]:\n    \"\"\"Benchmark different HNSW configurations.\"\"\"\n    import hnswlib\n\n    results = []\n    dim = vectors.shape[1]\n    n = vectors.shape[0]\n\n    for m in m_values:\n        for ef_construction in ef_construction_values:\n            # Build index\n            index = hnswlib.Index(space='cosine', dim=dim)\n            index.init_index(max_elements=n, M=m, ef_construction=ef_construction)\n\n            build_start = time.time()\n            index.add_items(vectors)\n        ",
  "tier3_resources": "    build_time = time.time() - build_start\n\n            # Get memory usage\n            memory_bytes = index.element_count * (\n                dim * 4 +  # Vector storage\n                m * 2 * 4  # Graph edges (approximate)\n            )\n\n            for ef_search in ef_search_values:\n                index.set_ef(ef_search)\n\n                # Measure search\n                search_start = time.time()\n                labels, distances = index.knn_query(queries, k=10)\n                search_time = time.time() - search_start\n\n                # Calculate recall\n                recall = calculate_recall(labels, ground_truth, k=10)\n\n                results.append({\n                    \"M\": m,\n                    \"ef_construction\": ef_construction,\n                    \"ef_search\": ef_search,\n                    \"build_time_s\": build_time,\n                    \"search_time_ms\": search_time * 1000 / len(queries),\n                    \"recall@10\": recall,\n                    \"memory_mb\": memory_bytes / 1024 / 1024\n                })\n\n    return results\n\n\ndef calculate_recall(predictions: np.ndarray, ground_truth: np.ndarray, k: int) -> float:\n    \"\"\"Calculate recall@k.\"\"\"\n    correct = 0\n    for pred, truth in zip(predictions, ground_truth):\n        correct += len(set(pred[:k]) & set(truth[:k]))\n    return correct / (len(predictions) * k)\n\n\ndef recommend_hnsw_params(\n    num_vectors: int,\n    target_recall: float = 0.95,\n    max_latency_ms: float = 10,\n    available_memory_gb: float = 8\n) -> dict:\n    \"\"\"Recommend HNSW parameters based on requirements.\"\"\"\n\n    # Base recommendations\n    if num_vectors < 100_000:\n        m = 16\n        ef_construction = 100\n    elif num_vectors < 1_000_000:\n        m = 32\n        ef_construction = 200\n    else:\n        m = 48\n        ef_construction = 256\n\n    # Adjust ef_search based on recall target\n    if target_recall >= 0.99:\n        ef_search = 256\n    elif target_recall >= 0.95:\n        ef_search = 128\n    else:\n        ef_search = 64\n\n    return {\n        \"M\": m,\n        \"ef_construction\": ef_construction,\n        \"ef_search\": ef_search,\n        \"notes\": f\"Estimated for {num_vectors:,} vectors, {target_recall:.0%} recall\"\n    }\n```\n\n### Template 2: Quantization Strategies\n\n```python\nimport numpy as np\nfrom typing import Optional\n\nclass VectorQuantizer:\n    \"\"\"Quantization strategies for vector compression.\"\"\"\n\n    @staticmethod\n    def scalar_quantize_int8(\n        vectors: np.ndarray,\n        min_val: Optional[float] = None,\n        max_val: Optional[float] = None\n    ) -> Tuple[np.ndarray, dict]:\n        \"\"\"Scalar quantization to INT8.\"\"\"\n        if min_val is None:\n            min_val = vectors.min()\n        if max_val is None:\n            max_val = vectors.max()\n\n        # Scale to 0-255 range\n        scale = 255.0 / (max_val - min_val)\n        quantized = np.clip(\n            np.round((vectors - min_val) * scale),\n            0, 255\n        ).astype(np.uint8)\n\n        params = {\"min_val\": min_val, \"max_val\": max_",
  "tokenEstimate": {
    "tier1": 19.5,
    "tier2": 341.90000000000003,
    "tier3": 1528.8
  },
  "fullDefinition": "---\nname: vector-index-tuning\ndescription: Optimize vector index performance for latency, recall, and memory. Use when tuning HNSW parameters, selecting quantization strategies, or scaling vector search infrastructure.\n---\n\n# Vector Index Tuning\n\nGuide to optimizing vector indexes for production performance.\n\n## When to Use This Skill\n\n- Tuning HNSW parameters\n- Implementing quantization\n- Optimizing memory usage\n- Reducing search latency\n- Balancing recall vs speed\n- Scaling to billions of vectors\n\n## Core Concepts\n\n### 1. Index Type Selection\n\n```\nData Size           Recommended Index\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n< 10K vectors  \u2192    Flat (exact search)\n10K - 1M       \u2192    HNSW\n1M - 100M      \u2192    HNSW + Quantization\n> 100M         \u2192    IVF + PQ or DiskANN\n```\n\n### 2. HNSW Parameters\n\n| Parameter | Default | Effect |\n|-----------|---------|--------|\n| **M** | 16 | Connections per node, \u2191 = better recall, more memory |\n| **efConstruction** | 100 | Build quality, \u2191 = better index, slower build |\n| **efSearch** | 50 | Search quality, \u2191 = better recall, slower search |\n\n### 3. Quantization Types\n\n```\nFull Precision (FP32): 4 bytes \u00d7 dimensions\nHalf Precision (FP16): 2 bytes \u00d7 dimensions\nINT8 Scalar:           1 byte \u00d7 dimensions\nProduct Quantization:  ~32-64 bytes total\nBinary:                dimensions/8 bytes\n```\n\n## Templates\n\n### Template 1: HNSW Parameter Tuning\n\n```python\nimport numpy as np\nfrom typing import List, Tuple\nimport time\n\ndef benchmark_hnsw_parameters(\n    vectors: np.ndarray,\n    queries: np.ndarray,\n    ground_truth: np.ndarray,\n    m_values: List[int] = [8, 16, 32, 64],\n    ef_construction_values: List[int] = [64, 128, 256],\n    ef_search_values: List[int] = [32, 64, 128, 256]\n) -> List[dict]:\n    \"\"\"Benchmark different HNSW configurations.\"\"\"\n    import hnswlib\n\n    results = []\n    dim = vectors.shape[1]\n    n = vectors.shape[0]\n\n    for m in m_values:\n        for ef_construction in ef_construction_values:\n            # Build index\n            index = hnswlib.Index(space='cosine', dim=dim)\n            index.init_index(max_elements=n, M=m, ef_construction=ef_construction)\n\n            build_start = time.time()\n            index.add_items(vectors)\n            build_time = time.time() - build_start\n\n            # Get memory usage\n            memory_bytes = index.element_count * (\n                dim * 4 +  # Vector storage\n                m * 2 * 4  # Graph edges (approximate)\n            )\n\n            for ef_search in ef_search_values:\n                index.set_ef(ef_search)\n\n                # Measure search\n                search_start = time.time()\n                labels, distances = index.knn_query(queries, k=10)\n                search_time = time.time() - search_start\n\n                # Calculate recall\n                recall = calculate_recall(labels, ground_truth, k=10)\n\n                results.append({\n                    \"M\": m,\n                    \"ef_construction\": ef_construction,\n                    \"ef_search\": ef_search,\n                    \"build_time_s\": build_time,\n                    \"search_time_ms\": search_time * 1000 / len(queries),\n                    \"recall@10\": recall,\n                    \"memory_mb\": memory_bytes / 1024 / 1024\n                })\n\n    return results\n\n\ndef calculate_recall(predictions: np.ndarray, ground_truth: np.ndarray, k: int) -> float:\n    \"\"\"Calculate recall@k.\"\"\"\n    correct = 0\n    for pred, truth in zip(predictions, ground_truth):\n        correct += len(set(pred[:k]) & set(truth[:k]))\n    return correct / (len(predictions) * k)\n\n\ndef recommend_hnsw_params(\n    num_vectors: int,\n    target_recall: float = 0.95,\n    max_latency_ms: float = 10,\n    available_memory_gb: float = 8\n) -> dict:\n    \"\"\"Recommend HNSW parameters based on requirements.\"\"\"\n\n    # Base recommendations\n    if num_vectors < 100_000:\n        m = 16\n        ef_construction = 100\n    elif num_vectors < 1_000_000:\n        m = 32\n        ef_construction = 200\n    else:\n        m = 48\n        ef_construction = 256\n\n    # Adjust ef_search based on recall target\n    if target_recall >= 0.99:\n        ef_search = 256\n    elif target_recall >= 0.95:\n        ef_search = 128\n    else:\n        ef_search = 64\n\n    return {\n        \"M\": m,\n        \"ef_construction\": ef_construction,\n        \"ef_search\": ef_search,\n        \"notes\": f\"Estimated for {num_vectors:,} vectors, {target_recall:.0%} recall\"\n    }\n```\n\n### Template 2: Quantization Strategies\n\n```python\nimport numpy as np\nfrom typing import Optional\n\nclass VectorQuantizer:\n    \"\"\"Quantization strategies for vector compression.\"\"\"\n\n    @staticmethod\n    def scalar_quantize_int8(\n        vectors: np.ndarray,\n        min_val: Optional[float] = None,\n        max_val: Optional[float] = None\n    ) -> Tuple[np.ndarray, dict]:\n        \"\"\"Scalar quantization to INT8.\"\"\"\n        if min_val is None:\n            min_val = vectors.min()\n        if max_val is None:\n            max_val = vectors.max()\n\n        # Scale to 0-255 range\n        scale = 255.0 / (max_val - min_val)\n        quantized = np.clip(\n            np.round((vectors - min_val) * scale),\n            0, 255\n        ).astype(np.uint8)\n\n        params = {\"min_val\": min_val, \"max_val\": max_val, \"scale\": scale}\n        return quantized, params\n\n    @staticmethod\n    def dequantize_int8(\n        quantized: np.ndarray,\n        params: dict\n    ) -> np.ndarray:\n        \"\"\"Dequantize INT8 vectors.\"\"\"\n        return quantized.astype(np.float32) / params[\"scale\"] + params[\"min_val\"]\n\n    @staticmethod\n    def product_quantize(\n        vectors: np.ndarray,\n        n_subvectors: int = 8,\n        n_centroids: int = 256\n    ) -> Tuple[np.ndarray, dict]:\n        \"\"\"Product quantization for aggressive compression.\"\"\"\n        from sklearn.cluster import KMeans\n\n        n, dim = vectors.shape\n        assert dim % n_subvectors == 0\n        subvector_dim = dim // n_subvectors\n\n        codebooks = []\n        codes = np.zeros((n, n_subvectors), dtype=np.uint8)\n\n        for i in range(n_subvectors):\n            start = i * subvector_dim\n            end = (i + 1) * subvector_dim\n            subvectors = vectors[:, start:end]\n\n            kmeans = KMeans(n_clusters=n_centroids, random_state=42)\n            codes[:, i] = kmeans.fit_predict(subvectors)\n            codebooks.append(kmeans.cluster_centers_)\n\n        params = {\n            \"codebooks\": codebooks,\n            \"n_subvectors\": n_subvectors,\n            \"subvector_dim\": subvector_dim\n        }\n        return codes, params\n\n    @staticmethod\n    def binary_quantize(vectors: np.ndarray) -> np.ndarray:\n        \"\"\"Binary quantization (sign of each dimension).\"\"\"\n        # Convert to binary: positive = 1, negative = 0\n        binary = (vectors > 0).astype(np.uint8)\n\n        # Pack bits into bytes\n        n, dim = vectors.shape\n        packed_dim = (dim + 7) // 8\n\n        packed = np.zeros((n, packed_dim), dtype=np.uint8)\n        for i in range(dim):\n            byte_idx = i // 8\n            bit_idx = i % 8\n            packed[:, byte_idx] |= (binary[:, i] << bit_idx)\n\n        return packed\n\n\ndef estimate_memory_usage(\n    num_vectors: int,\n    dimensions: int,\n    quantization: str = \"fp32\",\n    index_type: str = \"hnsw\",\n    hnsw_m: int = 16\n) -> dict:\n    \"\"\"Estimate memory usage for different configurations.\"\"\"\n\n    # Vector storage\n    bytes_per_dimension = {\n        \"fp32\": 4,\n        \"fp16\": 2,\n        \"int8\": 1,\n        \"pq\": 0.05,  # Approximate\n        \"binary\": 0.125\n    }\n\n    vector_bytes = num_vectors * dimensions * bytes_per_dimension[quantization]\n\n    # Index overhead\n    if index_type == \"hnsw\":\n        # Each node has ~M*2 edges, each edge is 4 bytes (int32)\n        index_bytes = num_vectors * hnsw_m * 2 * 4\n    elif index_type == \"ivf\":\n        # Inverted lists + centroids\n        index_bytes = num_vectors * 8 + 65536 * dimensions * 4\n    else:\n        index_bytes = 0\n\n    total_bytes = vector_bytes + index_bytes\n\n    return {\n        \"vector_storage_mb\": vector_bytes / 1024 / 1024,\n        \"index_overhead_mb\": index_bytes / 1024 / 1024,\n        \"total_mb\": total_bytes / 1024 / 1024,\n        \"total_gb\": total_bytes / 1024 / 1024 / 1024\n    }\n```\n\n### Template 3: Qdrant Index Configuration\n\n```python\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http import models\n\ndef create_optimized_collection(\n    client: QdrantClient,\n    collection_name: str,\n    vector_size: int,\n    num_vectors: int,\n    optimize_for: str = \"balanced\"  # \"recall\", \"speed\", \"memory\"\n) -> None:\n    \"\"\"Create collection with optimized settings.\"\"\"\n\n    # HNSW configuration based on optimization target\n    hnsw_configs = {\n        \"recall\": models.HnswConfigDiff(m=32, ef_construct=256),\n        \"speed\": models.HnswConfigDiff(m=16, ef_construct=64),\n        \"balanced\": models.HnswConfigDiff(m=16, ef_construct=128),\n        \"memory\": models.HnswConfigDiff(m=8, ef_construct=64)\n    }\n\n    # Quantization configuration\n    quantization_configs = {\n        \"recall\": None,  # No quantization for max recall\n        \"speed\": models.ScalarQuantization(\n            scalar=models.ScalarQuantizationConfig(\n                type=models.ScalarType.INT8,\n                quantile=0.99,\n                always_ram=True\n            )\n        ),\n        \"balanced\": models.ScalarQuantization(\n            scalar=models.ScalarQuantizationConfig(\n                type=models.ScalarType.INT8,\n                quantile=0.99,\n                always_ram=False\n            )\n        ),\n        \"memory\": models.ProductQuantization(\n            product=models.ProductQuantizationConfig(\n                compression=models.CompressionRatio.X16,\n                always_ram=False\n            )\n        )\n    }\n\n    # Optimizer configuration\n    optimizer_configs = {\n        \"recall\": models.OptimizersConfigDiff(\n            indexing_threshold=10000,\n            memmap_threshold=50000\n        ),\n        \"speed\": models.OptimizersConfigDiff(\n            indexing_threshold=5000,\n            memmap_threshold=20000\n        ),\n        \"balanced\": models.OptimizersConfigDiff(\n            indexing_threshold=20000,\n            memmap_threshold=50000\n        ),\n        \"memory\": models.OptimizersConfigDiff(\n            indexing_threshold=50000,\n            memmap_threshold=10000  # Use disk sooner\n        )\n    }\n\n    client.create_collection(\n        collection_name=collection_name,\n        vectors_config=models.VectorParams(\n            size=vector_size,\n            distance=models.Distance.COSINE\n        ),\n        hnsw_config=hnsw_configs[optimize_for],\n        quantization_config=quantization_configs[optimize_for],\n        optimizers_config=optimizer_configs[optimize_for]\n    )\n\n\ndef tune_search_parameters(\n    client: QdrantClient,\n    collection_name: str,\n    target_recall: float = 0.95\n) -> dict:\n    \"\"\"Tune search parameters for target recall.\"\"\"\n\n    # Search parameter recommendations\n    if target_recall >= 0.99:\n        search_params = models.SearchParams(\n            hnsw_ef=256,\n            exact=False,\n            quantization=models.QuantizationSearchParams(\n                ignore=True,  # Don't use quantization for search\n                rescore=True\n            )\n        )\n    elif target_recall >= 0.95:\n        search_params = models.SearchParams(\n            hnsw_ef=128,\n            exact=False,\n            quantization=models.QuantizationSearchParams(\n                ignore=False,\n                rescore=True,\n                oversampling=2.0\n            )\n        )\n    else:\n        search_params = models.SearchParams(\n            hnsw_ef=64,\n            exact=False,\n            quantization=models.QuantizationSearchParams(\n                ignore=False,\n                rescore=False\n            )\n        )\n\n    return search_params\n```\n\n### Template 4: Performance Monitoring\n\n```python\nimport time\nfrom dataclasses import dataclass\nfrom typing import List\nimport numpy as np\n\n@dataclass\nclass SearchMetrics:\n    latency_p50_ms: float\n    latency_p95_ms: float\n    latency_p99_ms: float\n    recall: float\n    qps: float\n\n\nclass VectorSearchMonitor:\n    \"\"\"Monitor vector search performance.\"\"\"\n\n    def __init__(self, ground_truth_fn=None):\n        self.latencies = []\n        self.recalls = []\n        self.ground_truth_fn = ground_truth_fn\n\n    def measure_search(\n        self,\n        search_fn,\n        query_vectors: np.ndarray,\n        k: int = 10,\n        num_iterations: int = 100\n    ) -> SearchMetrics:\n        \"\"\"Benchmark search performance.\"\"\"\n        latencies = []\n\n        for _ in range(num_iterations):\n            for query in query_vectors:\n                start = time.perf_counter()\n                results = search_fn(query, k=k)\n                latency = (time.perf_counter() - start) * 1000\n                latencies.append(latency)\n\n        latencies = np.array(latencies)\n        total_queries = num_iterations * len(query_vectors)\n        total_time = sum(latencies) / 1000  # seconds\n\n        return SearchMetrics(\n            latency_p50_ms=np.percentile(latencies, 50),\n            latency_p95_ms=np.percentile(latencies, 95),\n            latency_p99_ms=np.percentile(latencies, 99),\n            recall=self._calculate_recall(search_fn, query_vectors, k) if self.ground_truth_fn else 0,\n            qps=total_queries / total_time\n        )\n\n    def _calculate_recall(self, search_fn, queries: np.ndarray, k: int) -> float:\n        \"\"\"Calculate recall against ground truth.\"\"\"\n        if not self.ground_truth_fn:\n            return 0\n\n        correct = 0\n        total = 0\n\n        for query in queries:\n            predicted = set(search_fn(query, k=k))\n            actual = set(self.ground_truth_fn(query, k=k))\n            correct += len(predicted & actual)\n            total += k\n\n        return correct / total\n\n\ndef profile_index_build(\n    build_fn,\n    vectors: np.ndarray,\n    batch_sizes: List[int] = [1000, 10000, 50000]\n) -> dict:\n    \"\"\"Profile index build performance.\"\"\"\n    results = {}\n\n    for batch_size in batch_sizes:\n        times = []\n        for i in range(0, len(vectors), batch_size):\n            batch = vectors[i:i + batch_size]\n            start = time.perf_counter()\n            build_fn(batch)\n            times.append(time.perf_counter() - start)\n\n        results[batch_size] = {\n            \"avg_batch_time_s\": np.mean(times),\n            \"vectors_per_second\": batch_size / np.mean(times)\n        }\n\n    return results\n```\n\n## Best Practices\n\n### Do's\n- **Benchmark with real queries** - Synthetic may not represent production\n- **Monitor recall continuously** - Can degrade with data drift\n- **Start with defaults** - Tune only when needed\n- **Use quantization** - Significant memory savings\n- **Consider tiered storage** - Hot/cold data separation\n\n### Don'ts\n- **Don't over-optimize early** - Profile first\n- **Don't ignore build time** - Index updates have cost\n- **Don't forget reindexing** - Plan for maintenance\n- **Don't skip warming** - Cold indexes are slow\n\n## Resources\n\n- [HNSW Paper](https://arxiv.org/abs/1603.09320)\n- [Faiss Wiki](https://github.com/facebookresearch/faiss/wiki)\n- [ANN Benchmarks](https://ann-benchmarks.com/)\n"
}