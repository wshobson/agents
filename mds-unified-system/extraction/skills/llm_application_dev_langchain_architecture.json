{
  "id": "llm_application_dev_langchain_architecture",
  "name": "langchain-architecture",
  "source": "llm-application-dev",
  "originalPath": "plugins/llm-application-dev/skills/langchain-architecture/SKILL.md",
  "activationCriteria": "Design LLM applications using the LangChain framework with agents, memory, and tool integration patterns. Use when building LangChain applications, implementing AI agents, or creating complex LLM work",
  "tier1_metadata": "langchain-architecture: Design LLM applications using the LangChain framework with agents, memory, and tool integration patt",
  "tier2_instructions": "# LangChain Architecture\n\nMaster the LangChain framework for building sophisticated LLM applications with agents, chains, memory, and tool integration.\n\n## When to Use This Skill\n\n- Building autonomous AI agents with tool access\n- Implementing complex multi-step LLM workflows\n- Managing conversation memory and state\n- Integrating LLMs with external data sources and APIs\n- Creating modular, reusable LLM application components\n- Implementing document processing pipelines\n- Building production-grade LLM applications\n\n## Core Concepts\n\n### 1. Agents\nAutonomous systems that use LLMs to decide which actions to take.\n\n**Agent Types:**\n- **ReAct**: Reasoning + Acting in interleaved manner\n- **OpenAI Functions**: Leverages function calling API\n- **Structured Chat**: Handles multi-input tools\n- **Conversational**: Optimized for chat interfaces\n- **Self-Ask with Search**: Decomposes complex queries\n\n### 2. Chains\nSequences of calls to LLMs or other utilities.\n\n**Chain Types:**\n- **LLMChain**: Basic prompt + LLM combination\n- **SequentialChain**: Multiple chains in sequence\n- **RouterChain**: Routes inputs to specialized chains\n- **TransformChain**: Data transformations between steps\n- **MapReduceChain**: Parallel processing with aggregation\n\n### 3. Memory\nSystems for maintaining context across interactions.\n\n**Memory Types:**\n- **ConversationBufferMemory**: Stores all messages\n- **ConversationSummaryMemory**: Summarizes older messages\n- **ConversationBufferWindowMemory**: Keeps last N messages\n- **EntityMemory**: Tracks information about entities\n- **VectorStoreMemory**: Semantic similarity retrieval\n\n### 4. Document Processing\nLoading, transforming, and storing documents for retrieval.\n\n**Components:**\n- **Document Loaders**: Load from various sources\n- **Text Splitters**: Chunk documents intelligently\n- **Vector Stores**: Store and retrieve embeddings\n- **Retrievers**: Fetch relevant documents\n- **Indexes**: Organize documents for efficient access\n\n### 5. Callbacks\nHooks for",
  "tier3_resources": " logging, monitoring, and debugging.\n\n**Use Cases:**\n- Request/response logging\n- Token usage tracking\n- Latency monitoring\n- Error handling\n- Custom metrics collection\n\n## Quick Start\n\n```python\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\nfrom langchain.memory import ConversationBufferMemory\n\n# Initialize LLM\nllm = OpenAI(temperature=0)\n\n# Load tools\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n\n# Add memory\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\n\n# Create agent\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n    memory=memory,\n    verbose=True\n)\n\n# Run agent\nresult = agent.run(\"What's the weather in SF? Then calculate 25 * 4\")\n```\n\n## Architecture Patterns\n\n### Pattern 1: RAG with LangChain\n```python\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# Load and process documents\nloader = TextLoader('documents.txt')\ndocuments = loader.load()\n\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\ntexts = text_splitter.split_documents(documents)\n\n# Create vector store\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(texts, embeddings)\n\n# Create retrieval chain\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=vectorstore.as_retriever(),\n    return_source_documents=True\n)\n\n# Query\nresult = qa_chain({\"query\": \"What is the main topic?\"})\n```\n\n### Pattern 2: Custom Agent with Tools\n```python\nfrom langchain.agents import Tool, AgentExecutor\nfrom langchain.agents.react.base import ReActDocstoreAgent\nfrom langchain.tools import tool\n\n@tool\ndef search_database(query: str) -> str:\n    \"\"\"Search internal database for information.\"\"\"\n    # Your database search logic\n    return f\"Results for: {query}\"\n\n@tool\ndef send_email(recipient: str, content: str) -> str:\n    \"\"\"Send an email to specified recipient.\"\"\"\n    # Email sending logic\n    return f\"Email sent to {recipient}\"\n\ntools = [search_database, send_email]\n\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=True\n)\n```\n\n### Pattern 3: Multi-Step Chain\n```python\nfrom langchain.chains import LLMChain, SequentialChain\nfrom langchain.prompts import PromptTemplate\n\n# Step 1: Extract key information\nextract_prompt = PromptTemplate(\n    input_variables=[\"text\"],\n    template=\"Extract key entities from: {text}\\n\\nEntities:\"\n)\nextract_chain = LLMChain(llm=llm, prompt=extract_prompt, output_key=\"entities\")\n\n# Step 2: Analyze entities\nanalyze_prompt = PromptTemplate(\n    input_variables=[\"entities\"],\n    template=\"Analyze these entities: {entities}\\n\\nAnalysis:\"\n)\nanalyze_chain = LLMChain(llm=llm, prompt=analyze_prompt, output_key=\"analysis\")\n\n# Step 3: Gene",
  "tokenEstimate": {
    "tier1": 19.5,
    "tier2": 332.8,
    "tier3": 1019.2
  },
  "fullDefinition": "---\nname: langchain-architecture\ndescription: Design LLM applications using the LangChain framework with agents, memory, and tool integration patterns. Use when building LangChain applications, implementing AI agents, or creating complex LLM workflows.\n---\n\n# LangChain Architecture\n\nMaster the LangChain framework for building sophisticated LLM applications with agents, chains, memory, and tool integration.\n\n## When to Use This Skill\n\n- Building autonomous AI agents with tool access\n- Implementing complex multi-step LLM workflows\n- Managing conversation memory and state\n- Integrating LLMs with external data sources and APIs\n- Creating modular, reusable LLM application components\n- Implementing document processing pipelines\n- Building production-grade LLM applications\n\n## Core Concepts\n\n### 1. Agents\nAutonomous systems that use LLMs to decide which actions to take.\n\n**Agent Types:**\n- **ReAct**: Reasoning + Acting in interleaved manner\n- **OpenAI Functions**: Leverages function calling API\n- **Structured Chat**: Handles multi-input tools\n- **Conversational**: Optimized for chat interfaces\n- **Self-Ask with Search**: Decomposes complex queries\n\n### 2. Chains\nSequences of calls to LLMs or other utilities.\n\n**Chain Types:**\n- **LLMChain**: Basic prompt + LLM combination\n- **SequentialChain**: Multiple chains in sequence\n- **RouterChain**: Routes inputs to specialized chains\n- **TransformChain**: Data transformations between steps\n- **MapReduceChain**: Parallel processing with aggregation\n\n### 3. Memory\nSystems for maintaining context across interactions.\n\n**Memory Types:**\n- **ConversationBufferMemory**: Stores all messages\n- **ConversationSummaryMemory**: Summarizes older messages\n- **ConversationBufferWindowMemory**: Keeps last N messages\n- **EntityMemory**: Tracks information about entities\n- **VectorStoreMemory**: Semantic similarity retrieval\n\n### 4. Document Processing\nLoading, transforming, and storing documents for retrieval.\n\n**Components:**\n- **Document Loaders**: Load from various sources\n- **Text Splitters**: Chunk documents intelligently\n- **Vector Stores**: Store and retrieve embeddings\n- **Retrievers**: Fetch relevant documents\n- **Indexes**: Organize documents for efficient access\n\n### 5. Callbacks\nHooks for logging, monitoring, and debugging.\n\n**Use Cases:**\n- Request/response logging\n- Token usage tracking\n- Latency monitoring\n- Error handling\n- Custom metrics collection\n\n## Quick Start\n\n```python\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\nfrom langchain.memory import ConversationBufferMemory\n\n# Initialize LLM\nllm = OpenAI(temperature=0)\n\n# Load tools\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n\n# Add memory\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\n\n# Create agent\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n    memory=memory,\n    verbose=True\n)\n\n# Run agent\nresult = agent.run(\"What's the weather in SF? Then calculate 25 * 4\")\n```\n\n## Architecture Patterns\n\n### Pattern 1: RAG with LangChain\n```python\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# Load and process documents\nloader = TextLoader('documents.txt')\ndocuments = loader.load()\n\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\ntexts = text_splitter.split_documents(documents)\n\n# Create vector store\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(texts, embeddings)\n\n# Create retrieval chain\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=vectorstore.as_retriever(),\n    return_source_documents=True\n)\n\n# Query\nresult = qa_chain({\"query\": \"What is the main topic?\"})\n```\n\n### Pattern 2: Custom Agent with Tools\n```python\nfrom langchain.agents import Tool, AgentExecutor\nfrom langchain.agents.react.base import ReActDocstoreAgent\nfrom langchain.tools import tool\n\n@tool\ndef search_database(query: str) -> str:\n    \"\"\"Search internal database for information.\"\"\"\n    # Your database search logic\n    return f\"Results for: {query}\"\n\n@tool\ndef send_email(recipient: str, content: str) -> str:\n    \"\"\"Send an email to specified recipient.\"\"\"\n    # Email sending logic\n    return f\"Email sent to {recipient}\"\n\ntools = [search_database, send_email]\n\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=True\n)\n```\n\n### Pattern 3: Multi-Step Chain\n```python\nfrom langchain.chains import LLMChain, SequentialChain\nfrom langchain.prompts import PromptTemplate\n\n# Step 1: Extract key information\nextract_prompt = PromptTemplate(\n    input_variables=[\"text\"],\n    template=\"Extract key entities from: {text}\\n\\nEntities:\"\n)\nextract_chain = LLMChain(llm=llm, prompt=extract_prompt, output_key=\"entities\")\n\n# Step 2: Analyze entities\nanalyze_prompt = PromptTemplate(\n    input_variables=[\"entities\"],\n    template=\"Analyze these entities: {entities}\\n\\nAnalysis:\"\n)\nanalyze_chain = LLMChain(llm=llm, prompt=analyze_prompt, output_key=\"analysis\")\n\n# Step 3: Generate summary\nsummary_prompt = PromptTemplate(\n    input_variables=[\"entities\", \"analysis\"],\n    template=\"Summarize:\\nEntities: {entities}\\nAnalysis: {analysis}\\n\\nSummary:\"\n)\nsummary_chain = LLMChain(llm=llm, prompt=summary_prompt, output_key=\"summary\")\n\n# Combine into sequential chain\noverall_chain = SequentialChain(\n    chains=[extract_chain, analyze_chain, summary_chain],\n    input_variables=[\"text\"],\n    output_variables=[\"entities\", \"analysis\", \"summary\"],\n    verbose=True\n)\n```\n\n## Memory Management Best Practices\n\n### Choosing the Right Memory Type\n```python\n# For short conversations (< 10 messages)\nfrom langchain.memory import ConversationBufferMemory\nmemory = ConversationBufferMemory()\n\n# For long conversations (summarize old messages)\nfrom langchain.memory import ConversationSummaryMemory\nmemory = ConversationSummaryMemory(llm=llm)\n\n# For sliding window (last N messages)\nfrom langchain.memory import ConversationBufferWindowMemory\nmemory = ConversationBufferWindowMemory(k=5)\n\n# For entity tracking\nfrom langchain.memory import ConversationEntityMemory\nmemory = ConversationEntityMemory(llm=llm)\n\n# For semantic retrieval of relevant history\nfrom langchain.memory import VectorStoreRetrieverMemory\nmemory = VectorStoreRetrieverMemory(retriever=retriever)\n```\n\n## Callback System\n\n### Custom Callback Handler\n```python\nfrom langchain.callbacks.base import BaseCallbackHandler\n\nclass CustomCallbackHandler(BaseCallbackHandler):\n    def on_llm_start(self, serialized, prompts, **kwargs):\n        print(f\"LLM started with prompts: {prompts}\")\n\n    def on_llm_end(self, response, **kwargs):\n        print(f\"LLM ended with response: {response}\")\n\n    def on_llm_error(self, error, **kwargs):\n        print(f\"LLM error: {error}\")\n\n    def on_chain_start(self, serialized, inputs, **kwargs):\n        print(f\"Chain started with inputs: {inputs}\")\n\n    def on_agent_action(self, action, **kwargs):\n        print(f\"Agent taking action: {action}\")\n\n# Use callback\nagent.run(\"query\", callbacks=[CustomCallbackHandler()])\n```\n\n## Testing Strategies\n\n```python\nimport pytest\nfrom unittest.mock import Mock\n\ndef test_agent_tool_selection():\n    # Mock LLM to return specific tool selection\n    mock_llm = Mock()\n    mock_llm.predict.return_value = \"Action: search_database\\nAction Input: test query\"\n\n    agent = initialize_agent(tools, mock_llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)\n\n    result = agent.run(\"test query\")\n\n    # Verify correct tool was selected\n    assert \"search_database\" in str(mock_llm.predict.call_args)\n\ndef test_memory_persistence():\n    memory = ConversationBufferMemory()\n\n    memory.save_context({\"input\": \"Hi\"}, {\"output\": \"Hello!\"})\n\n    assert \"Hi\" in memory.load_memory_variables({})['history']\n    assert \"Hello!\" in memory.load_memory_variables({})['history']\n```\n\n## Performance Optimization\n\n### 1. Caching\n```python\nfrom langchain.cache import InMemoryCache\nimport langchain\n\nlangchain.llm_cache = InMemoryCache()\n```\n\n### 2. Batch Processing\n```python\n# Process multiple documents in parallel\nfrom langchain.document_loaders import DirectoryLoader\nfrom concurrent.futures import ThreadPoolExecutor\n\nloader = DirectoryLoader('./docs')\ndocs = loader.load()\n\ndef process_doc(doc):\n    return text_splitter.split_documents([doc])\n\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    split_docs = list(executor.map(process_doc, docs))\n```\n\n### 3. Streaming Responses\n```python\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\nllm = OpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n```\n\n## Resources\n\n- **references/agents.md**: Deep dive on agent architectures\n- **references/memory.md**: Memory system patterns\n- **references/chains.md**: Chain composition strategies\n- **references/document-processing.md**: Document loading and indexing\n- **references/callbacks.md**: Monitoring and observability\n- **assets/agent-template.py**: Production-ready agent template\n- **assets/memory-config.yaml**: Memory configuration examples\n- **assets/chain-example.py**: Complex chain examples\n\n## Common Pitfalls\n\n1. **Memory Overflow**: Not managing conversation history length\n2. **Tool Selection Errors**: Poor tool descriptions confuse agents\n3. **Context Window Exceeded**: Exceeding LLM token limits\n4. **No Error Handling**: Not catching and handling agent failures\n5. **Inefficient Retrieval**: Not optimizing vector store queries\n\n## Production Checklist\n\n- [ ] Implement proper error handling\n- [ ] Add request/response logging\n- [ ] Monitor token usage and costs\n- [ ] Set timeout limits for agent execution\n- [ ] Implement rate limiting\n- [ ] Add input validation\n- [ ] Test with edge cases\n- [ ] Set up observability (callbacks)\n- [ ] Implement fallback strategies\n- [ ] Version control prompts and configurations\n"
}