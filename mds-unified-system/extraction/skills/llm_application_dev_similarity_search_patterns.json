{
  "id": "llm_application_dev_similarity_search_patterns",
  "name": "similarity-search-patterns",
  "source": "llm-application-dev",
  "originalPath": "plugins/llm-application-dev/skills/similarity-search-patterns/SKILL.md",
  "activationCriteria": "Implement efficient similarity search with vector databases. Use when building semantic search, implementing nearest neighbor queries, or optimizing retrieval performance.",
  "tier1_metadata": "similarity-search-patterns: Implement efficient similarity search with vector databases. Use when building semantic search, impl",
  "tier2_instructions": "# Similarity Search Patterns\n\nPatterns for implementing efficient similarity search in production systems.\n\n## When to Use This Skill\n\n- Building semantic search systems\n- Implementing RAG retrieval\n- Creating recommendation engines\n- Optimizing search latency\n- Scaling to millions of vectors\n- Combining semantic and keyword search\n\n## Core Concepts\n\n### 1. Distance Metrics\n\n| Metric | Formula | Best For |\n|--------|---------|----------|\n| **Cosine** | 1 - (A\u00b7B)/(\u2016A\u2016\u2016B\u2016) | Normalized embeddings |\n| **Euclidean (L2)** | \u221a\u03a3(a-b)\u00b2 | Raw embeddings |\n| **Dot Product** | A\u00b7B | Magnitude matters |\n| **Manhattan (L1)** | \u03a3|a-b| | Sparse vectors |\n\n### 2. Index Types\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 Index Types                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502    Flat     \u2502     HNSW      \u2502    IVF+PQ         \u2502\n\u2502 (Exact)     \u2502 (Graph-based) \u2502 (Quantized)       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 O(n) search \u2502 O(log n)      \u2502 O(\u221an)             \u2502\n\u2502 100% recall \u2502 ~95-99%       \u2502 ~90-95%           \u2502\n\u2502 Small data  \u2502 Medium-Large  \u2502 Very Large        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Templates\n\n### Template 1: Pinecone Implementation\n\n```python\nfrom pinecone import Pinecone, ServerlessSpec\nfrom typing import List, Dict, Optional\nimport hashlib\n\nclass PineconeVectorStore:\n    def __init__(\n        self,\n        api_key: str,\n        index_name: str,\n        dimension: int = 1536,\n        metric: str = \"cosine\"\n    ):\n        self.pc = Pinecone(api_key=api_key)\n\n        # Create index if not exists\n        if index_name not in self.pc.list_indexes().names():\n            self.pc.create_index(\n                name=index_name,\n                dimension=dimension,\n                metric=metric,\n                spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n            )\n\n        self.index = self.pc.Index(index_name)\n\n    def upsert(\n        self,\n        vectors: Li",
  "tier3_resources": "st[Dict],\n        namespace: str = \"\"\n    ) -> int:\n        \"\"\"\n        Upsert vectors.\n        vectors: [{\"id\": str, \"values\": List[float], \"metadata\": dict}]\n        \"\"\"\n        # Batch upsert\n        batch_size = 100\n        total = 0\n\n        for i in range(0, len(vectors), batch_size):\n            batch = vectors[i:i + batch_size]\n            self.index.upsert(vectors=batch, namespace=namespace)\n            total += len(batch)\n\n        return total\n\n    def search(\n        self,\n        query_vector: List[float],\n        top_k: int = 10,\n        namespace: str = \"\",\n        filter: Optional[Dict] = None,\n        include_metadata: bool = True\n    ) -> List[Dict]:\n        \"\"\"Search for similar vectors.\"\"\"\n        results = self.index.query(\n            vector=query_vector,\n            top_k=top_k,\n            namespace=namespace,\n            filter=filter,\n            include_metadata=include_metadata\n        )\n\n        return [\n            {\n                \"id\": match.id,\n                \"score\": match.score,\n                \"metadata\": match.metadata\n            }\n            for match in results.matches\n        ]\n\n    def search_with_rerank(\n        self,\n        query: str,\n        query_vector: List[float],\n        top_k: int = 10,\n        rerank_top_n: int = 50,\n        namespace: str = \"\"\n    ) -> List[Dict]:\n        \"\"\"Search and rerank results.\"\"\"\n        # Over-fetch for reranking\n        initial_results = self.search(\n            query_vector,\n            top_k=rerank_top_n,\n            namespace=namespace\n        )\n\n        # Rerank with cross-encoder or LLM\n        reranked = self._rerank(query, initial_results)\n\n        return reranked[:top_k]\n\n    def _rerank(self, query: str, results: List[Dict]) -> List[Dict]:\n        \"\"\"Rerank results using cross-encoder.\"\"\"\n        from sentence_transformers import CrossEncoder\n\n        model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\n        pairs = [(query, r[\"metadata\"][\"text\"]) for r in results]\n        scores = model.predict(pairs)\n\n        for result, score in zip(results, scores):\n            result[\"rerank_score\"] = float(score)\n\n        return sorted(results, key=lambda x: x[\"rerank_score\"], reverse=True)\n\n    def delete(self, ids: List[str], namespace: str = \"\"):\n        \"\"\"Delete vectors by ID.\"\"\"\n        self.index.delete(ids=ids, namespace=namespace)\n\n    def delete_by_filter(self, filter: Dict, namespace: str = \"\"):\n        \"\"\"Delete vectors matching filter.\"\"\"\n        self.index.delete(filter=filter, namespace=namespace)\n```\n\n### Template 2: Qdrant Implementation\n\n```python\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http import models\nfrom typing import List, Dict, Optional\n\nclass QdrantVectorStore:\n    def __init__(\n        self,\n        url: str = \"localhost\",\n        port: int = 6333,\n        collection_name: str = \"documents\",\n        vector_size: int = 1536\n    ):\n        self.client = QdrantClient(url=url, port=port)\n        self.collection_n",
  "tokenEstimate": {
    "tier1": 18.2,
    "tier2": 289.90000000000003,
    "tier3": 1617.2
  },
  "fullDefinition": "---\nname: similarity-search-patterns\ndescription: Implement efficient similarity search with vector databases. Use when building semantic search, implementing nearest neighbor queries, or optimizing retrieval performance.\n---\n\n# Similarity Search Patterns\n\nPatterns for implementing efficient similarity search in production systems.\n\n## When to Use This Skill\n\n- Building semantic search systems\n- Implementing RAG retrieval\n- Creating recommendation engines\n- Optimizing search latency\n- Scaling to millions of vectors\n- Combining semantic and keyword search\n\n## Core Concepts\n\n### 1. Distance Metrics\n\n| Metric | Formula | Best For |\n|--------|---------|----------|\n| **Cosine** | 1 - (A\u00b7B)/(\u2016A\u2016\u2016B\u2016) | Normalized embeddings |\n| **Euclidean (L2)** | \u221a\u03a3(a-b)\u00b2 | Raw embeddings |\n| **Dot Product** | A\u00b7B | Magnitude matters |\n| **Manhattan (L1)** | \u03a3|a-b| | Sparse vectors |\n\n### 2. Index Types\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 Index Types                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502    Flat     \u2502     HNSW      \u2502    IVF+PQ         \u2502\n\u2502 (Exact)     \u2502 (Graph-based) \u2502 (Quantized)       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 O(n) search \u2502 O(log n)      \u2502 O(\u221an)             \u2502\n\u2502 100% recall \u2502 ~95-99%       \u2502 ~90-95%           \u2502\n\u2502 Small data  \u2502 Medium-Large  \u2502 Very Large        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Templates\n\n### Template 1: Pinecone Implementation\n\n```python\nfrom pinecone import Pinecone, ServerlessSpec\nfrom typing import List, Dict, Optional\nimport hashlib\n\nclass PineconeVectorStore:\n    def __init__(\n        self,\n        api_key: str,\n        index_name: str,\n        dimension: int = 1536,\n        metric: str = \"cosine\"\n    ):\n        self.pc = Pinecone(api_key=api_key)\n\n        # Create index if not exists\n        if index_name not in self.pc.list_indexes().names():\n            self.pc.create_index(\n                name=index_name,\n                dimension=dimension,\n                metric=metric,\n                spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n            )\n\n        self.index = self.pc.Index(index_name)\n\n    def upsert(\n        self,\n        vectors: List[Dict],\n        namespace: str = \"\"\n    ) -> int:\n        \"\"\"\n        Upsert vectors.\n        vectors: [{\"id\": str, \"values\": List[float], \"metadata\": dict}]\n        \"\"\"\n        # Batch upsert\n        batch_size = 100\n        total = 0\n\n        for i in range(0, len(vectors), batch_size):\n            batch = vectors[i:i + batch_size]\n            self.index.upsert(vectors=batch, namespace=namespace)\n            total += len(batch)\n\n        return total\n\n    def search(\n        self,\n        query_vector: List[float],\n        top_k: int = 10,\n        namespace: str = \"\",\n        filter: Optional[Dict] = None,\n        include_metadata: bool = True\n    ) -> List[Dict]:\n        \"\"\"Search for similar vectors.\"\"\"\n        results = self.index.query(\n            vector=query_vector,\n            top_k=top_k,\n            namespace=namespace,\n            filter=filter,\n            include_metadata=include_metadata\n        )\n\n        return [\n            {\n                \"id\": match.id,\n                \"score\": match.score,\n                \"metadata\": match.metadata\n            }\n            for match in results.matches\n        ]\n\n    def search_with_rerank(\n        self,\n        query: str,\n        query_vector: List[float],\n        top_k: int = 10,\n        rerank_top_n: int = 50,\n        namespace: str = \"\"\n    ) -> List[Dict]:\n        \"\"\"Search and rerank results.\"\"\"\n        # Over-fetch for reranking\n        initial_results = self.search(\n            query_vector,\n            top_k=rerank_top_n,\n            namespace=namespace\n        )\n\n        # Rerank with cross-encoder or LLM\n        reranked = self._rerank(query, initial_results)\n\n        return reranked[:top_k]\n\n    def _rerank(self, query: str, results: List[Dict]) -> List[Dict]:\n        \"\"\"Rerank results using cross-encoder.\"\"\"\n        from sentence_transformers import CrossEncoder\n\n        model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\n        pairs = [(query, r[\"metadata\"][\"text\"]) for r in results]\n        scores = model.predict(pairs)\n\n        for result, score in zip(results, scores):\n            result[\"rerank_score\"] = float(score)\n\n        return sorted(results, key=lambda x: x[\"rerank_score\"], reverse=True)\n\n    def delete(self, ids: List[str], namespace: str = \"\"):\n        \"\"\"Delete vectors by ID.\"\"\"\n        self.index.delete(ids=ids, namespace=namespace)\n\n    def delete_by_filter(self, filter: Dict, namespace: str = \"\"):\n        \"\"\"Delete vectors matching filter.\"\"\"\n        self.index.delete(filter=filter, namespace=namespace)\n```\n\n### Template 2: Qdrant Implementation\n\n```python\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http import models\nfrom typing import List, Dict, Optional\n\nclass QdrantVectorStore:\n    def __init__(\n        self,\n        url: str = \"localhost\",\n        port: int = 6333,\n        collection_name: str = \"documents\",\n        vector_size: int = 1536\n    ):\n        self.client = QdrantClient(url=url, port=port)\n        self.collection_name = collection_name\n\n        # Create collection if not exists\n        collections = self.client.get_collections().collections\n        if collection_name not in [c.name for c in collections]:\n            self.client.create_collection(\n                collection_name=collection_name,\n                vectors_config=models.VectorParams(\n                    size=vector_size,\n                    distance=models.Distance.COSINE\n                ),\n                # Optional: enable quantization for memory efficiency\n                quantization_config=models.ScalarQuantization(\n                    scalar=models.ScalarQuantizationConfig(\n                        type=models.ScalarType.INT8,\n                        quantile=0.99,\n                        always_ram=True\n                    )\n                )\n            )\n\n    def upsert(self, points: List[Dict]) -> int:\n        \"\"\"\n        Upsert points.\n        points: [{\"id\": str/int, \"vector\": List[float], \"payload\": dict}]\n        \"\"\"\n        qdrant_points = [\n            models.PointStruct(\n                id=p[\"id\"],\n                vector=p[\"vector\"],\n                payload=p.get(\"payload\", {})\n            )\n            for p in points\n        ]\n\n        self.client.upsert(\n            collection_name=self.collection_name,\n            points=qdrant_points\n        )\n        return len(points)\n\n    def search(\n        self,\n        query_vector: List[float],\n        limit: int = 10,\n        filter: Optional[models.Filter] = None,\n        score_threshold: Optional[float] = None\n    ) -> List[Dict]:\n        \"\"\"Search for similar vectors.\"\"\"\n        results = self.client.search(\n            collection_name=self.collection_name,\n            query_vector=query_vector,\n            limit=limit,\n            query_filter=filter,\n            score_threshold=score_threshold\n        )\n\n        return [\n            {\n                \"id\": r.id,\n                \"score\": r.score,\n                \"payload\": r.payload\n            }\n            for r in results\n        ]\n\n    def search_with_filter(\n        self,\n        query_vector: List[float],\n        must_conditions: List[Dict] = None,\n        should_conditions: List[Dict] = None,\n        must_not_conditions: List[Dict] = None,\n        limit: int = 10\n    ) -> List[Dict]:\n        \"\"\"Search with complex filters.\"\"\"\n        conditions = []\n\n        if must_conditions:\n            conditions.extend([\n                models.FieldCondition(\n                    key=c[\"key\"],\n                    match=models.MatchValue(value=c[\"value\"])\n                )\n                for c in must_conditions\n            ])\n\n        filter = models.Filter(must=conditions) if conditions else None\n\n        return self.search(query_vector, limit=limit, filter=filter)\n\n    def search_with_sparse(\n        self,\n        dense_vector: List[float],\n        sparse_vector: Dict[int, float],\n        limit: int = 10,\n        dense_weight: float = 0.7\n    ) -> List[Dict]:\n        \"\"\"Hybrid search with dense and sparse vectors.\"\"\"\n        # Requires collection with named vectors\n        results = self.client.search(\n            collection_name=self.collection_name,\n            query_vector=models.NamedVector(\n                name=\"dense\",\n                vector=dense_vector\n            ),\n            limit=limit\n        )\n        return [{\"id\": r.id, \"score\": r.score, \"payload\": r.payload} for r in results]\n```\n\n### Template 3: pgvector with PostgreSQL\n\n```python\nimport asyncpg\nfrom typing import List, Dict, Optional\nimport numpy as np\n\nclass PgVectorStore:\n    def __init__(self, connection_string: str):\n        self.connection_string = connection_string\n\n    async def init(self):\n        \"\"\"Initialize connection pool and extension.\"\"\"\n        self.pool = await asyncpg.create_pool(self.connection_string)\n\n        async with self.pool.acquire() as conn:\n            # Enable extension\n            await conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n\n            # Create table\n            await conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS documents (\n                    id TEXT PRIMARY KEY,\n                    content TEXT,\n                    metadata JSONB,\n                    embedding vector(1536)\n                )\n            \"\"\")\n\n            # Create index (HNSW for better performance)\n            await conn.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS documents_embedding_idx\n                ON documents\n                USING hnsw (embedding vector_cosine_ops)\n                WITH (m = 16, ef_construction = 64)\n            \"\"\")\n\n    async def upsert(self, documents: List[Dict]):\n        \"\"\"Upsert documents with embeddings.\"\"\"\n        async with self.pool.acquire() as conn:\n            await conn.executemany(\n                \"\"\"\n                INSERT INTO documents (id, content, metadata, embedding)\n                VALUES ($1, $2, $3, $4)\n                ON CONFLICT (id) DO UPDATE SET\n                    content = EXCLUDED.content,\n                    metadata = EXCLUDED.metadata,\n                    embedding = EXCLUDED.embedding\n                \"\"\",\n                [\n                    (\n                        doc[\"id\"],\n                        doc[\"content\"],\n                        doc.get(\"metadata\", {}),\n                        np.array(doc[\"embedding\"]).tolist()\n                    )\n                    for doc in documents\n                ]\n            )\n\n    async def search(\n        self,\n        query_embedding: List[float],\n        limit: int = 10,\n        filter_metadata: Optional[Dict] = None\n    ) -> List[Dict]:\n        \"\"\"Search for similar documents.\"\"\"\n        query = \"\"\"\n            SELECT id, content, metadata,\n                   1 - (embedding <=> $1::vector) as similarity\n            FROM documents\n        \"\"\"\n\n        params = [query_embedding]\n\n        if filter_metadata:\n            conditions = []\n            for key, value in filter_metadata.items():\n                params.append(value)\n                conditions.append(f\"metadata->>'{key}' = ${len(params)}\")\n            query += \" WHERE \" + \" AND \".join(conditions)\n\n        query += f\" ORDER BY embedding <=> $1::vector LIMIT ${len(params) + 1}\"\n        params.append(limit)\n\n        async with self.pool.acquire() as conn:\n            rows = await conn.fetch(query, *params)\n\n        return [\n            {\n                \"id\": row[\"id\"],\n                \"content\": row[\"content\"],\n                \"metadata\": row[\"metadata\"],\n                \"score\": row[\"similarity\"]\n            }\n            for row in rows\n        ]\n\n    async def hybrid_search(\n        self,\n        query_embedding: List[float],\n        query_text: str,\n        limit: int = 10,\n        vector_weight: float = 0.5\n    ) -> List[Dict]:\n        \"\"\"Hybrid search combining vector and full-text.\"\"\"\n        async with self.pool.acquire() as conn:\n            rows = await conn.fetch(\n                \"\"\"\n                WITH vector_results AS (\n                    SELECT id, content, metadata,\n                           1 - (embedding <=> $1::vector) as vector_score\n                    FROM documents\n                    ORDER BY embedding <=> $1::vector\n                    LIMIT $3 * 2\n                ),\n                text_results AS (\n                    SELECT id, content, metadata,\n                           ts_rank(to_tsvector('english', content),\n                                   plainto_tsquery('english', $2)) as text_score\n                    FROM documents\n                    WHERE to_tsvector('english', content) @@ plainto_tsquery('english', $2)\n                    LIMIT $3 * 2\n                )\n                SELECT\n                    COALESCE(v.id, t.id) as id,\n                    COALESCE(v.content, t.content) as content,\n                    COALESCE(v.metadata, t.metadata) as metadata,\n                    COALESCE(v.vector_score, 0) * $4 +\n                    COALESCE(t.text_score, 0) * (1 - $4) as combined_score\n                FROM vector_results v\n                FULL OUTER JOIN text_results t ON v.id = t.id\n                ORDER BY combined_score DESC\n                LIMIT $3\n                \"\"\",\n                query_embedding, query_text, limit, vector_weight\n            )\n\n        return [dict(row) for row in rows]\n```\n\n### Template 4: Weaviate Implementation\n\n```python\nimport weaviate\nfrom weaviate.util import generate_uuid5\nfrom typing import List, Dict, Optional\n\nclass WeaviateVectorStore:\n    def __init__(\n        self,\n        url: str = \"http://localhost:8080\",\n        class_name: str = \"Document\"\n    ):\n        self.client = weaviate.Client(url=url)\n        self.class_name = class_name\n        self._ensure_schema()\n\n    def _ensure_schema(self):\n        \"\"\"Create schema if not exists.\"\"\"\n        schema = {\n            \"class\": self.class_name,\n            \"vectorizer\": \"none\",  # We provide vectors\n            \"properties\": [\n                {\"name\": \"content\", \"dataType\": [\"text\"]},\n                {\"name\": \"source\", \"dataType\": [\"string\"]},\n                {\"name\": \"chunk_id\", \"dataType\": [\"int\"]}\n            ]\n        }\n\n        if not self.client.schema.exists(self.class_name):\n            self.client.schema.create_class(schema)\n\n    def upsert(self, documents: List[Dict]):\n        \"\"\"Batch upsert documents.\"\"\"\n        with self.client.batch as batch:\n            batch.batch_size = 100\n\n            for doc in documents:\n                batch.add_data_object(\n                    data_object={\n                        \"content\": doc[\"content\"],\n                        \"source\": doc.get(\"source\", \"\"),\n                        \"chunk_id\": doc.get(\"chunk_id\", 0)\n                    },\n                    class_name=self.class_name,\n                    uuid=generate_uuid5(doc[\"id\"]),\n                    vector=doc[\"embedding\"]\n                )\n\n    def search(\n        self,\n        query_vector: List[float],\n        limit: int = 10,\n        where_filter: Optional[Dict] = None\n    ) -> List[Dict]:\n        \"\"\"Vector search.\"\"\"\n        query = (\n            self.client.query\n            .get(self.class_name, [\"content\", \"source\", \"chunk_id\"])\n            .with_near_vector({\"vector\": query_vector})\n            .with_limit(limit)\n            .with_additional([\"distance\", \"id\"])\n        )\n\n        if where_filter:\n            query = query.with_where(where_filter)\n\n        results = query.do()\n\n        return [\n            {\n                \"id\": item[\"_additional\"][\"id\"],\n                \"content\": item[\"content\"],\n                \"source\": item[\"source\"],\n                \"score\": 1 - item[\"_additional\"][\"distance\"]\n            }\n            for item in results[\"data\"][\"Get\"][self.class_name]\n        ]\n\n    def hybrid_search(\n        self,\n        query: str,\n        query_vector: List[float],\n        limit: int = 10,\n        alpha: float = 0.5  # 0 = keyword, 1 = vector\n    ) -> List[Dict]:\n        \"\"\"Hybrid search combining BM25 and vector.\"\"\"\n        results = (\n            self.client.query\n            .get(self.class_name, [\"content\", \"source\"])\n            .with_hybrid(query=query, vector=query_vector, alpha=alpha)\n            .with_limit(limit)\n            .with_additional([\"score\"])\n            .do()\n        )\n\n        return [\n            {\n                \"content\": item[\"content\"],\n                \"source\": item[\"source\"],\n                \"score\": item[\"_additional\"][\"score\"]\n            }\n            for item in results[\"data\"][\"Get\"][self.class_name]\n        ]\n```\n\n## Best Practices\n\n### Do's\n- **Use appropriate index** - HNSW for most cases\n- **Tune parameters** - ef_search, nprobe for recall/speed\n- **Implement hybrid search** - Combine with keyword search\n- **Monitor recall** - Measure search quality\n- **Pre-filter when possible** - Reduce search space\n\n### Don'ts\n- **Don't skip evaluation** - Measure before optimizing\n- **Don't over-index** - Start with flat, scale up\n- **Don't ignore latency** - P99 matters for UX\n- **Don't forget costs** - Vector storage adds up\n\n## Resources\n\n- [Pinecone Docs](https://docs.pinecone.io/)\n- [Qdrant Docs](https://qdrant.tech/documentation/)\n- [pgvector](https://github.com/pgvector/pgvector)\n- [Weaviate Docs](https://weaviate.io/developers/weaviate)\n"
}