{
  "id": "data_engineering_data_quality_frameworks",
  "name": "data-quality-frameworks",
  "source": "data-engineering",
  "originalPath": "plugins/data-engineering/skills/data-quality-frameworks/SKILL.md",
  "activationCriteria": "Implement data quality validation with Great Expectations, dbt tests, and data contracts. Use when building data quality pipelines, implementing validation rules, or establishing data contracts.",
  "tier1_metadata": "data-quality-frameworks: Implement data quality validation with Great Expectations, dbt tests, and data contracts. Use when b",
  "tier2_instructions": "# Data Quality Frameworks\n\nProduction patterns for implementing data quality with Great Expectations, dbt tests, and data contracts to ensure reliable data pipelines.\n\n## When to Use This Skill\n\n- Implementing data quality checks in pipelines\n- Setting up Great Expectations validation\n- Building comprehensive dbt test suites\n- Establishing data contracts between teams\n- Monitoring data quality metrics\n- Automating data validation in CI/CD\n\n## Core Concepts\n\n### 1. Data Quality Dimensions\n\n| Dimension | Description | Example Check |\n|-----------|-------------|---------------|\n| **Completeness** | No missing values | `expect_column_values_to_not_be_null` |\n| **Uniqueness** | No duplicates | `expect_column_values_to_be_unique` |\n| **Validity** | Values in expected range | `expect_column_values_to_be_in_set` |\n| **Accuracy** | Data matches reality | Cross-reference validation |\n| **Consistency** | No contradictions | `expect_column_pair_values_A_to_be_greater_than_B` |\n| **Timeliness** | Data is recent | `expect_column_max_to_be_between` |\n\n### 2. Testing Pyramid for Data\n\n```\n          /\\\n         /  \\     Integration Tests (cross-table)\n        /\u2500\u2500\u2500\u2500\\\n       /      \\   Unit Tests (single column)\n      /\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\\n     /          \\ Schema Tests (structure)\n    /\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\\n```\n\n## Quick Start\n\n### Great Expectations Setup\n\n```bash\n# Install\npip install great_expectations\n\n# Initialize project\ngreat_expectations init\n\n# Create datasource\ngreat_expectations datasource new\n```\n\n```python\n# great_expectations/checkpoints/daily_validation.yml\nimport great_expectations as gx\n\n# Create context\ncontext = gx.get_context()\n\n# Create expectation suite\nsuite = context.add_expectation_suite(\"orders_suite\")\n\n# Add expectations\nsuite.add_expectation(\n    gx.expectations.ExpectColumnValuesToNotBeNull(column=\"order_id\")\n)\nsuite.add_expectation(\n    gx.expectations.ExpectColumnValuesToBeUnique(column=\"order_id\")\n)\n\n# Validate\nresults = context.run_checkpoint(checkpoint_name=\"daily_orde",
  "tier3_resources": "rs\")\n```\n\n## Patterns\n\n### Pattern 1: Great Expectations Suite\n\n```python\n# expectations/orders_suite.py\nimport great_expectations as gx\nfrom great_expectations.core import ExpectationSuite\nfrom great_expectations.core.expectation_configuration import ExpectationConfiguration\n\ndef build_orders_suite() -> ExpectationSuite:\n    \"\"\"Build comprehensive orders expectation suite\"\"\"\n\n    suite = ExpectationSuite(expectation_suite_name=\"orders_suite\")\n\n    # Schema expectations\n    suite.add_expectation(ExpectationConfiguration(\n        expectation_type=\"expect_table_columns_to_match_set\",\n        kwargs={\n            \"column_set\": [\"order_id\", \"customer_id\", \"amount\", \"status\", \"created_at\"],\n            \"exact_match\": False  # Allow additional columns\n        }\n    ))\n\n    # Primary key\n    suite.add_expectation(ExpectationConfiguration(\n        expectation_type=\"expect_column_values_to_not_be_null\",\n        kwargs={\"column\": \"order_id\"}\n    ))\n    suite.add_expectation(ExpectationConfiguration(\n        expectation_type=\"expect_column_values_to_be_unique\",\n        kwargs={\"column\": \"order_id\"}\n    ))\n\n    # Foreign key\n    suite.add_expectation(ExpectationConfiguration(\n        expectation_type=\"expect_column_values_to_not_be_null\",\n        kwargs={\"column\": \"customer_id\"}\n    ))\n\n    # Categorical values\n    suite.add_expectation(ExpectationConfiguration(\n        expectation_type=\"expect_column_values_to_be_in_set\",\n        kwargs={\n            \"column\": \"status\",\n            \"value_set\": [\"pending\", \"processing\", \"shipped\", \"delivered\", \"cancelled\"]\n        }\n    ))\n\n    # Numeric ranges\n    suite.add_expectation(ExpectationConfiguration(\n        expectation_type=\"expect_column_values_to_be_between\",\n        kwargs={\n            \"column\": \"amount\",\n            \"min_value\": 0,\n            \"max_value\": 100000,\n            \"strict_min\": True  # amount > 0\n        }\n    ))\n\n    # Date validity\n    suite.add_expectation(ExpectationConfiguration(\n        expectation_type=\"expect_column_values_to_be_dateutil_parseable\",\n        kwargs={\"column\": \"created_at\"}\n    ))\n\n    # Freshness - data should be recent\n    suite.add_expectation(ExpectationConfiguration(\n        expectation_type=\"expect_column_max_to_be_between\",\n        kwargs={\n            \"column\": \"created_at\",\n            \"min_value\": {\"$PARAMETER\": \"now - timedelta(days=1)\"},\n            \"max_value\": {\"$PARAMETER\": \"now\"}\n        }\n    ))\n\n    # Row count sanity\n    suite.add_expectation(ExpectationConfiguration(\n        expectation_type=\"expect_table_row_count_to_be_between\",\n        kwargs={\n            \"min_value\": 1000,  # Expect at least 1000 rows\n            \"max_value\": 10000000\n        }\n    ))\n\n    # Statistical expectations\n    suite.add_expectation(ExpectationConfiguration(\n        expectation_type=\"expect_column_mean_to_be_between\",\n        kwargs={\n            \"column\": \"amount\",\n            \"min_value\": 50,\n            \"max_value\": 500\n        }\n    ))\n\n    return suite\n```\n\n### Patte",
  "tokenEstimate": {
    "tier1": 20.8,
    "tier2": 289.90000000000003,
    "tier3": 1553.5
  },
  "fullDefinition": "---\nname: data-quality-frameworks\ndescription: Implement data quality validation with Great Expectations, dbt tests, and data contracts. Use when building data quality pipelines, implementing validation rules, or establishing data contracts.\n---\n\n# Data Quality Frameworks\n\nProduction patterns for implementing data quality with Great Expectations, dbt tests, and data contracts to ensure reliable data pipelines.\n\n## When to Use This Skill\n\n- Implementing data quality checks in pipelines\n- Setting up Great Expectations validation\n- Building comprehensive dbt test suites\n- Establishing data contracts between teams\n- Monitoring data quality metrics\n- Automating data validation in CI/CD\n\n## Core Concepts\n\n### 1. Data Quality Dimensions\n\n| Dimension | Description | Example Check |\n|-----------|-------------|---------------|\n| **Completeness** | No missing values | `expect_column_values_to_not_be_null` |\n| **Uniqueness** | No duplicates | `expect_column_values_to_be_unique` |\n| **Validity** | Values in expected range | `expect_column_values_to_be_in_set` |\n| **Accuracy** | Data matches reality | Cross-reference validation |\n| **Consistency** | No contradictions | `expect_column_pair_values_A_to_be_greater_than_B` |\n| **Timeliness** | Data is recent | `expect_column_max_to_be_between` |\n\n### 2. Testing Pyramid for Data\n\n```\n          /\\\n         /  \\     Integration Tests (cross-table)\n        /\u2500\u2500\u2500\u2500\\\n       /      \\   Unit Tests (single column)\n      /\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\\n     /          \\ Schema Tests (structure)\n    /\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\\n```\n\n## Quick Start\n\n### Great Expectations Setup\n\n```bash\n# Install\npip install great_expectations\n\n# Initialize project\ngreat_expectations init\n\n# Create datasource\ngreat_expectations datasource new\n```\n\n```python\n# great_expectations/checkpoints/daily_validation.yml\nimport great_expectations as gx\n\n# Create context\ncontext = gx.get_context()\n\n# Create expectation suite\nsuite = context.add_expectation_suite(\"orders_suite\")\n\n# Add expectations\nsuite.add_expectation(\n    gx.expectations.ExpectColumnValuesToNotBeNull(column=\"order_id\")\n)\nsuite.add_expectation(\n    gx.expectations.ExpectColumnValuesToBeUnique(column=\"order_id\")\n)\n\n# Validate\nresults = context.run_checkpoint(checkpoint_name=\"daily_orders\")\n```\n\n## Patterns\n\n### Pattern 1: Great Expectations Suite\n\n```python\n# expectations/orders_suite.py\nimport great_expectations as gx\nfrom great_expectations.core import ExpectationSuite\nfrom great_expectations.core.expectation_configuration import ExpectationConfiguration\n\ndef build_orders_suite() -> ExpectationSuite:\n    \"\"\"Build comprehensive orders expectation suite\"\"\"\n\n    suite = ExpectationSuite(expectation_suite_name=\"orders_suite\")\n\n    # Schema expectations\n    suite.add_expectation(ExpectationConfiguration(\n        expectation_type=\"expect_table_columns_to_match_set\",\n        kwargs={\n            \"column_set\": [\"order_id\", \"customer_id\", \"amount\", \"status\", \"created_at\"],\n            \"exact_match\": False  # Allow additional columns\n        }\n    ))\n\n    # Primary key\n    suite.add_expectation(ExpectationConfiguration(\n        expectation_type=\"expect_column_values_to_not_be_null\",\n        kwargs={\"column\": \"order_id\"}\n    ))\n    suite.add_expectation(ExpectationConfiguration(\n        expectation_type=\"expect_column_values_to_be_unique\",\n        kwargs={\"column\": \"order_id\"}\n    ))\n\n    # Foreign key\n    suite.add_expectation(ExpectationConfiguration(\n        expectation_type=\"expect_column_values_to_not_be_null\",\n        kwargs={\"column\": \"customer_id\"}\n    ))\n\n    # Categorical values\n    suite.add_expectation(ExpectationConfiguration(\n        expectation_type=\"expect_column_values_to_be_in_set\",\n        kwargs={\n            \"column\": \"status\",\n            \"value_set\": [\"pending\", \"processing\", \"shipped\", \"delivered\", \"cancelled\"]\n        }\n    ))\n\n    # Numeric ranges\n    suite.add_expectation(ExpectationConfiguration(\n        expectation_type=\"expect_column_values_to_be_between\",\n        kwargs={\n            \"column\": \"amount\",\n            \"min_value\": 0,\n            \"max_value\": 100000,\n            \"strict_min\": True  # amount > 0\n        }\n    ))\n\n    # Date validity\n    suite.add_expectation(ExpectationConfiguration(\n        expectation_type=\"expect_column_values_to_be_dateutil_parseable\",\n        kwargs={\"column\": \"created_at\"}\n    ))\n\n    # Freshness - data should be recent\n    suite.add_expectation(ExpectationConfiguration(\n        expectation_type=\"expect_column_max_to_be_between\",\n        kwargs={\n            \"column\": \"created_at\",\n            \"min_value\": {\"$PARAMETER\": \"now - timedelta(days=1)\"},\n            \"max_value\": {\"$PARAMETER\": \"now\"}\n        }\n    ))\n\n    # Row count sanity\n    suite.add_expectation(ExpectationConfiguration(\n        expectation_type=\"expect_table_row_count_to_be_between\",\n        kwargs={\n            \"min_value\": 1000,  # Expect at least 1000 rows\n            \"max_value\": 10000000\n        }\n    ))\n\n    # Statistical expectations\n    suite.add_expectation(ExpectationConfiguration(\n        expectation_type=\"expect_column_mean_to_be_between\",\n        kwargs={\n            \"column\": \"amount\",\n            \"min_value\": 50,\n            \"max_value\": 500\n        }\n    ))\n\n    return suite\n```\n\n### Pattern 2: Great Expectations Checkpoint\n\n```yaml\n# great_expectations/checkpoints/orders_checkpoint.yml\nname: orders_checkpoint\nconfig_version: 1.0\nclass_name: Checkpoint\nrun_name_template: \"%Y%m%d-%H%M%S-orders-validation\"\n\nvalidations:\n  - batch_request:\n      datasource_name: warehouse\n      data_connector_name: default_inferred_data_connector_name\n      data_asset_name: orders\n      data_connector_query:\n        index: -1  # Latest batch\n    expectation_suite_name: orders_suite\n\naction_list:\n  - name: store_validation_result\n    action:\n      class_name: StoreValidationResultAction\n\n  - name: store_evaluation_parameters\n    action:\n      class_name: StoreEvaluationParametersAction\n\n  - name: update_data_docs\n    action:\n      class_name: UpdateDataDocsAction\n\n  # Slack notification on failure\n  - name: send_slack_notification\n    action:\n      class_name: SlackNotificationAction\n      slack_webhook: ${SLACK_WEBHOOK}\n      notify_on: failure\n      renderer:\n        module_name: great_expectations.render.renderer.slack_renderer\n        class_name: SlackRenderer\n```\n\n```python\n# Run checkpoint\nimport great_expectations as gx\n\ncontext = gx.get_context()\nresult = context.run_checkpoint(checkpoint_name=\"orders_checkpoint\")\n\nif not result.success:\n    failed_expectations = [\n        r for r in result.run_results.values()\n        if not r.success\n    ]\n    raise ValueError(f\"Data quality check failed: {failed_expectations}\")\n```\n\n### Pattern 3: dbt Data Tests\n\n```yaml\n# models/marts/core/_core__models.yml\nversion: 2\n\nmodels:\n  - name: fct_orders\n    description: Order fact table\n    tests:\n      # Table-level tests\n      - dbt_utils.recency:\n          datepart: day\n          field: created_at\n          interval: 1\n      - dbt_utils.at_least_one\n      - dbt_utils.expression_is_true:\n          expression: \"total_amount >= 0\"\n\n    columns:\n      - name: order_id\n        description: Primary key\n        tests:\n          - unique\n          - not_null\n\n      - name: customer_id\n        description: Foreign key to dim_customers\n        tests:\n          - not_null\n          - relationships:\n              to: ref('dim_customers')\n              field: customer_id\n\n      - name: order_status\n        tests:\n          - accepted_values:\n              values: ['pending', 'processing', 'shipped', 'delivered', 'cancelled']\n\n      - name: total_amount\n        tests:\n          - not_null\n          - dbt_utils.expression_is_true:\n              expression: \">= 0\"\n\n      - name: created_at\n        tests:\n          - not_null\n          - dbt_utils.expression_is_true:\n              expression: \"<= current_timestamp\"\n\n  - name: dim_customers\n    columns:\n      - name: customer_id\n        tests:\n          - unique\n          - not_null\n\n      - name: email\n        tests:\n          - unique\n          - not_null\n          # Custom regex test\n          - dbt_utils.expression_is_true:\n              expression: \"email ~ '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,}$'\"\n```\n\n### Pattern 4: Custom dbt Tests\n\n```sql\n-- tests/generic/test_row_count_in_range.sql\n{% test row_count_in_range(model, min_count, max_count) %}\n\nwith row_count as (\n    select count(*) as cnt from {{ model }}\n)\n\nselect cnt\nfrom row_count\nwhere cnt < {{ min_count }} or cnt > {{ max_count }}\n\n{% endtest %}\n\n-- Usage in schema.yml:\n-- tests:\n--   - row_count_in_range:\n--       min_count: 1000\n--       max_count: 10000000\n```\n\n```sql\n-- tests/generic/test_sequential_values.sql\n{% test sequential_values(model, column_name, interval=1) %}\n\nwith lagged as (\n    select\n        {{ column_name }},\n        lag({{ column_name }}) over (order by {{ column_name }}) as prev_value\n    from {{ model }}\n)\n\nselect *\nfrom lagged\nwhere {{ column_name }} - prev_value != {{ interval }}\n  and prev_value is not null\n\n{% endtest %}\n```\n\n```sql\n-- tests/singular/assert_orders_customers_match.sql\n-- Singular test: specific business rule\n\nwith orders_customers as (\n    select distinct customer_id from {{ ref('fct_orders') }}\n),\n\ndim_customers as (\n    select customer_id from {{ ref('dim_customers') }}\n),\n\norphaned_orders as (\n    select o.customer_id\n    from orders_customers o\n    left join dim_customers c using (customer_id)\n    where c.customer_id is null\n)\n\nselect * from orphaned_orders\n-- Test passes if this returns 0 rows\n```\n\n### Pattern 5: Data Contracts\n\n```yaml\n# contracts/orders_contract.yaml\napiVersion: datacontract.com/v1.0.0\nkind: DataContract\nmetadata:\n  name: orders\n  version: 1.0.0\n  owner: data-platform-team\n  contact: data-team@company.com\n\ninfo:\n  title: Orders Data Contract\n  description: Contract for order event data from the ecommerce platform\n  purpose: Analytics, reporting, and ML features\n\nservers:\n  production:\n    type: snowflake\n    account: company.us-east-1\n    database: ANALYTICS\n    schema: CORE\n\nterms:\n  usage: Internal analytics only\n  limitations: PII must not be exposed in downstream marts\n  billing: Charged per query TB scanned\n\nschema:\n  type: object\n  properties:\n    order_id:\n      type: string\n      format: uuid\n      description: Unique order identifier\n      required: true\n      unique: true\n      pii: false\n\n    customer_id:\n      type: string\n      format: uuid\n      description: Customer identifier\n      required: true\n      pii: true\n      piiClassification: indirect\n\n    total_amount:\n      type: number\n      minimum: 0\n      maximum: 100000\n      description: Order total in USD\n\n    created_at:\n      type: string\n      format: date-time\n      description: Order creation timestamp\n      required: true\n\n    status:\n      type: string\n      enum: [pending, processing, shipped, delivered, cancelled]\n      description: Current order status\n\nquality:\n  type: SodaCL\n  specification:\n    checks for orders:\n      - row_count > 0\n      - missing_count(order_id) = 0\n      - duplicate_count(order_id) = 0\n      - invalid_count(status) = 0:\n          valid values: [pending, processing, shipped, delivered, cancelled]\n      - freshness(created_at) < 24h\n\nsla:\n  availability: 99.9%\n  freshness: 1 hour\n  latency: 5 minutes\n```\n\n### Pattern 6: Automated Quality Pipeline\n\n```python\n# quality_pipeline.py\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any\nimport great_expectations as gx\nfrom datetime import datetime\n\n@dataclass\nclass QualityResult:\n    table: str\n    passed: bool\n    total_expectations: int\n    failed_expectations: int\n    details: List[Dict[str, Any]]\n    timestamp: datetime\n\nclass DataQualityPipeline:\n    \"\"\"Orchestrate data quality checks across tables\"\"\"\n\n    def __init__(self, context: gx.DataContext):\n        self.context = context\n        self.results: List[QualityResult] = []\n\n    def validate_table(self, table: str, suite: str) -> QualityResult:\n        \"\"\"Validate a single table against expectation suite\"\"\"\n\n        checkpoint_config = {\n            \"name\": f\"{table}_validation\",\n            \"config_version\": 1.0,\n            \"class_name\": \"Checkpoint\",\n            \"validations\": [{\n                \"batch_request\": {\n                    \"datasource_name\": \"warehouse\",\n                    \"data_asset_name\": table,\n                },\n                \"expectation_suite_name\": suite,\n            }],\n        }\n\n        result = self.context.run_checkpoint(**checkpoint_config)\n\n        # Parse results\n        validation_result = list(result.run_results.values())[0]\n        results = validation_result.results\n\n        failed = [r for r in results if not r.success]\n\n        return QualityResult(\n            table=table,\n            passed=result.success,\n            total_expectations=len(results),\n            failed_expectations=len(failed),\n            details=[{\n                \"expectation\": r.expectation_config.expectation_type,\n                \"success\": r.success,\n                \"observed_value\": r.result.get(\"observed_value\"),\n            } for r in results],\n            timestamp=datetime.now()\n        )\n\n    def run_all(self, tables: Dict[str, str]) -> Dict[str, QualityResult]:\n        \"\"\"Run validation for all tables\"\"\"\n        results = {}\n\n        for table, suite in tables.items():\n            print(f\"Validating {table}...\")\n            results[table] = self.validate_table(table, suite)\n\n        return results\n\n    def generate_report(self, results: Dict[str, QualityResult]) -> str:\n        \"\"\"Generate quality report\"\"\"\n        report = [\"# Data Quality Report\", f\"Generated: {datetime.now()}\", \"\"]\n\n        total_passed = sum(1 for r in results.values() if r.passed)\n        total_tables = len(results)\n\n        report.append(f\"## Summary: {total_passed}/{total_tables} tables passed\")\n        report.append(\"\")\n\n        for table, result in results.items():\n            status = \"\u2705\" if result.passed else \"\u274c\"\n            report.append(f\"### {status} {table}\")\n            report.append(f\"- Expectations: {result.total_expectations}\")\n            report.append(f\"- Failed: {result.failed_expectations}\")\n\n            if not result.passed:\n                report.append(\"- Failed checks:\")\n                for detail in result.details:\n                    if not detail[\"success\"]:\n                        report.append(f\"  - {detail['expectation']}: {detail['observed_value']}\")\n            report.append(\"\")\n\n        return \"\\n\".join(report)\n\n# Usage\ncontext = gx.get_context()\npipeline = DataQualityPipeline(context)\n\ntables_to_validate = {\n    \"orders\": \"orders_suite\",\n    \"customers\": \"customers_suite\",\n    \"products\": \"products_suite\",\n}\n\nresults = pipeline.run_all(tables_to_validate)\nreport = pipeline.generate_report(results)\n\n# Fail pipeline if any table failed\nif not all(r.passed for r in results.values()):\n    print(report)\n    raise ValueError(\"Data quality checks failed!\")\n```\n\n## Best Practices\n\n### Do's\n- **Test early** - Validate source data before transformations\n- **Test incrementally** - Add tests as you find issues\n- **Document expectations** - Clear descriptions for each test\n- **Alert on failures** - Integrate with monitoring\n- **Version contracts** - Track schema changes\n\n### Don'ts\n- **Don't test everything** - Focus on critical columns\n- **Don't ignore warnings** - They often precede failures\n- **Don't skip freshness** - Stale data is bad data\n- **Don't hardcode thresholds** - Use dynamic baselines\n- **Don't test in isolation** - Test relationships too\n\n## Resources\n\n- [Great Expectations Documentation](https://docs.greatexpectations.io/)\n- [dbt Testing Documentation](https://docs.getdbt.com/docs/build/tests)\n- [Data Contract Specification](https://datacontract.com/)\n- [Soda Core](https://docs.soda.io/soda-core/overview.html)\n"
}