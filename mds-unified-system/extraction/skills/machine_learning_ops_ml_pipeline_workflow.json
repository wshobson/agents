{
  "id": "machine_learning_ops_ml_pipeline_workflow",
  "name": "ml-pipeline-workflow",
  "source": "machine-learning-ops",
  "originalPath": "plugins/machine-learning-ops/skills/ml-pipeline-workflow/SKILL.md",
  "activationCriteria": "Build end-to-end MLOps pipelines from data preparation through model training, validation, and production deployment. Use when creating ML pipelines, implementing MLOps practices, or automating model ",
  "tier1_metadata": "ml-pipeline-workflow: Build end-to-end MLOps pipelines from data preparation through model training, validation, and produ",
  "tier2_instructions": "# ML Pipeline Workflow\n\nComplete end-to-end MLOps pipeline orchestration from data preparation through model deployment.\n\n## Overview\n\nThis skill provides comprehensive guidance for building production ML pipelines that handle the full lifecycle: data ingestion \u2192 preparation \u2192 training \u2192 validation \u2192 deployment \u2192 monitoring.\n\n## When to Use This Skill\n\n- Building new ML pipelines from scratch\n- Designing workflow orchestration for ML systems\n- Implementing data \u2192 model \u2192 deployment automation\n- Setting up reproducible training workflows\n- Creating DAG-based ML orchestration\n- Integrating ML components into production systems\n\n## What This Skill Provides\n\n### Core Capabilities\n\n1. **Pipeline Architecture**\n   - End-to-end workflow design\n   - DAG orchestration patterns (Airflow, Dagster, Kubeflow)\n   - Component dependencies and data flow\n   - Error handling and retry strategies\n\n2. **Data Preparation**\n   - Data validation and quality checks\n   - Feature engineering pipelines\n   - Data versioning and lineage\n   - Train/validation/test splitting strategies\n\n3. **Model Training**\n   - Training job orchestration\n   - Hyperparameter management\n   - Experiment tracking integration\n   - Distributed training patterns\n\n4. **Model Validation**\n   - Validation frameworks and metrics\n   - A/B testing infrastructure\n   - Performance regression detection\n   - Model comparison workflows\n\n5. **Deployment Automation**\n   - Model serving patterns\n   - Canary deployments\n   - Blue-green deployment strategies\n   - Rollback mechanisms\n\n### Reference Documentation\n\nSee the `references/` directory for detailed guides:\n- **data-preparation.md** - Data cleaning, validation, and feature engineering\n- **model-training.md** - Training workflows and best practices\n- **model-validation.md** - Validation strategies and metrics\n- **model-deployment.md** - Deployment patterns and serving architectures\n\n### Assets and Templates\n\nThe `assets/` directory contains:\n- **pipeline-dag.yaml.template** - D",
  "tier3_resources": "AG template for workflow orchestration\n- **training-config.yaml** - Training configuration template\n- **validation-checklist.md** - Pre-deployment validation checklist\n\n## Usage Patterns\n\n### Basic Pipeline Setup\n\n```python\n# 1. Define pipeline stages\nstages = [\n    \"data_ingestion\",\n    \"data_validation\",\n    \"feature_engineering\",\n    \"model_training\",\n    \"model_validation\",\n    \"model_deployment\"\n]\n\n# 2. Configure dependencies\n# See assets/pipeline-dag.yaml.template for full example\n```\n\n### Production Workflow\n\n1. **Data Preparation Phase**\n   - Ingest raw data from sources\n   - Run data quality checks\n   - Apply feature transformations\n   - Version processed datasets\n\n2. **Training Phase**\n   - Load versioned training data\n   - Execute training jobs\n   - Track experiments and metrics\n   - Save trained models\n\n3. **Validation Phase**\n   - Run validation test suite\n   - Compare against baseline\n   - Generate performance reports\n   - Approve for deployment\n\n4. **Deployment Phase**\n   - Package model artifacts\n   - Deploy to serving infrastructure\n   - Configure monitoring\n   - Validate production traffic\n\n## Best Practices\n\n### Pipeline Design\n\n- **Modularity**: Each stage should be independently testable\n- **Idempotency**: Re-running stages should be safe\n- **Observability**: Log metrics at every stage\n- **Versioning**: Track data, code, and model versions\n- **Failure Handling**: Implement retry logic and alerting\n\n### Data Management\n\n- Use data validation libraries (Great Expectations, TFX)\n- Version datasets with DVC or similar tools\n- Document feature engineering transformations\n- Maintain data lineage tracking\n\n### Model Operations\n\n- Separate training and serving infrastructure\n- Use model registries (MLflow, Weights & Biases)\n- Implement gradual rollouts for new models\n- Monitor model performance drift\n- Maintain rollback capabilities\n\n### Deployment Strategies\n\n- Start with shadow deployments\n- Use canary releases for validation\n- Implement A/B testing infrastructure\n- Set up automated rollback triggers\n- Monitor latency and throughput\n\n## Integration Points\n\n### Orchestration Tools\n\n- **Apache Airflow**: DAG-based workflow orchestration\n- **Dagster**: Asset-based pipeline orchestration\n- **Kubeflow Pipelines**: Kubernetes-native ML workflows\n- **Prefect**: Modern dataflow automation\n\n### Experiment Tracking\n\n- MLflow for experiment tracking and model registry\n- Weights & Biases for visualization and collaboration\n- TensorBoard for training metrics\n\n### Deployment Platforms\n\n- AWS SageMaker for managed ML infrastructure\n- Google Vertex AI for GCP deployments\n- Azure ML for Azure cloud\n- Kubernetes + KServe for cloud-agnostic serving\n\n## Progressive Disclosure\n\nStart with the basics and gradually add complexity:\n\n1. **Level 1**: Simple linear pipeline (data \u2192 train \u2192 deploy)\n2. **Level 2**: Add validation and monitoring stages\n3. **Level 3**: Implement hyperparameter tuning\n4. **Level 4**: Add A/B testing and gradual rollouts\n5. **Leve",
  "tokenEstimate": {
    "tier1": 18.2,
    "tier2": 331.5,
    "tier3": 783.9
  },
  "fullDefinition": "---\nname: ml-pipeline-workflow\ndescription: Build end-to-end MLOps pipelines from data preparation through model training, validation, and production deployment. Use when creating ML pipelines, implementing MLOps practices, or automating model training and deployment workflows.\n---\n\n# ML Pipeline Workflow\n\nComplete end-to-end MLOps pipeline orchestration from data preparation through model deployment.\n\n## Overview\n\nThis skill provides comprehensive guidance for building production ML pipelines that handle the full lifecycle: data ingestion \u2192 preparation \u2192 training \u2192 validation \u2192 deployment \u2192 monitoring.\n\n## When to Use This Skill\n\n- Building new ML pipelines from scratch\n- Designing workflow orchestration for ML systems\n- Implementing data \u2192 model \u2192 deployment automation\n- Setting up reproducible training workflows\n- Creating DAG-based ML orchestration\n- Integrating ML components into production systems\n\n## What This Skill Provides\n\n### Core Capabilities\n\n1. **Pipeline Architecture**\n   - End-to-end workflow design\n   - DAG orchestration patterns (Airflow, Dagster, Kubeflow)\n   - Component dependencies and data flow\n   - Error handling and retry strategies\n\n2. **Data Preparation**\n   - Data validation and quality checks\n   - Feature engineering pipelines\n   - Data versioning and lineage\n   - Train/validation/test splitting strategies\n\n3. **Model Training**\n   - Training job orchestration\n   - Hyperparameter management\n   - Experiment tracking integration\n   - Distributed training patterns\n\n4. **Model Validation**\n   - Validation frameworks and metrics\n   - A/B testing infrastructure\n   - Performance regression detection\n   - Model comparison workflows\n\n5. **Deployment Automation**\n   - Model serving patterns\n   - Canary deployments\n   - Blue-green deployment strategies\n   - Rollback mechanisms\n\n### Reference Documentation\n\nSee the `references/` directory for detailed guides:\n- **data-preparation.md** - Data cleaning, validation, and feature engineering\n- **model-training.md** - Training workflows and best practices\n- **model-validation.md** - Validation strategies and metrics\n- **model-deployment.md** - Deployment patterns and serving architectures\n\n### Assets and Templates\n\nThe `assets/` directory contains:\n- **pipeline-dag.yaml.template** - DAG template for workflow orchestration\n- **training-config.yaml** - Training configuration template\n- **validation-checklist.md** - Pre-deployment validation checklist\n\n## Usage Patterns\n\n### Basic Pipeline Setup\n\n```python\n# 1. Define pipeline stages\nstages = [\n    \"data_ingestion\",\n    \"data_validation\",\n    \"feature_engineering\",\n    \"model_training\",\n    \"model_validation\",\n    \"model_deployment\"\n]\n\n# 2. Configure dependencies\n# See assets/pipeline-dag.yaml.template for full example\n```\n\n### Production Workflow\n\n1. **Data Preparation Phase**\n   - Ingest raw data from sources\n   - Run data quality checks\n   - Apply feature transformations\n   - Version processed datasets\n\n2. **Training Phase**\n   - Load versioned training data\n   - Execute training jobs\n   - Track experiments and metrics\n   - Save trained models\n\n3. **Validation Phase**\n   - Run validation test suite\n   - Compare against baseline\n   - Generate performance reports\n   - Approve for deployment\n\n4. **Deployment Phase**\n   - Package model artifacts\n   - Deploy to serving infrastructure\n   - Configure monitoring\n   - Validate production traffic\n\n## Best Practices\n\n### Pipeline Design\n\n- **Modularity**: Each stage should be independently testable\n- **Idempotency**: Re-running stages should be safe\n- **Observability**: Log metrics at every stage\n- **Versioning**: Track data, code, and model versions\n- **Failure Handling**: Implement retry logic and alerting\n\n### Data Management\n\n- Use data validation libraries (Great Expectations, TFX)\n- Version datasets with DVC or similar tools\n- Document feature engineering transformations\n- Maintain data lineage tracking\n\n### Model Operations\n\n- Separate training and serving infrastructure\n- Use model registries (MLflow, Weights & Biases)\n- Implement gradual rollouts for new models\n- Monitor model performance drift\n- Maintain rollback capabilities\n\n### Deployment Strategies\n\n- Start with shadow deployments\n- Use canary releases for validation\n- Implement A/B testing infrastructure\n- Set up automated rollback triggers\n- Monitor latency and throughput\n\n## Integration Points\n\n### Orchestration Tools\n\n- **Apache Airflow**: DAG-based workflow orchestration\n- **Dagster**: Asset-based pipeline orchestration\n- **Kubeflow Pipelines**: Kubernetes-native ML workflows\n- **Prefect**: Modern dataflow automation\n\n### Experiment Tracking\n\n- MLflow for experiment tracking and model registry\n- Weights & Biases for visualization and collaboration\n- TensorBoard for training metrics\n\n### Deployment Platforms\n\n- AWS SageMaker for managed ML infrastructure\n- Google Vertex AI for GCP deployments\n- Azure ML for Azure cloud\n- Kubernetes + KServe for cloud-agnostic serving\n\n## Progressive Disclosure\n\nStart with the basics and gradually add complexity:\n\n1. **Level 1**: Simple linear pipeline (data \u2192 train \u2192 deploy)\n2. **Level 2**: Add validation and monitoring stages\n3. **Level 3**: Implement hyperparameter tuning\n4. **Level 4**: Add A/B testing and gradual rollouts\n5. **Level 5**: Multi-model pipelines with ensemble strategies\n\n## Common Patterns\n\n### Batch Training Pipeline\n\n```yaml\n# See assets/pipeline-dag.yaml.template\nstages:\n  - name: data_preparation\n    dependencies: []\n  - name: model_training\n    dependencies: [data_preparation]\n  - name: model_evaluation\n    dependencies: [model_training]\n  - name: model_deployment\n    dependencies: [model_evaluation]\n```\n\n### Real-time Feature Pipeline\n\n```python\n# Stream processing for real-time features\n# Combined with batch training\n# See references/data-preparation.md\n```\n\n### Continuous Training\n\n```python\n# Automated retraining on schedule\n# Triggered by data drift detection\n# See references/model-training.md\n```\n\n## Troubleshooting\n\n### Common Issues\n\n- **Pipeline failures**: Check dependencies and data availability\n- **Training instability**: Review hyperparameters and data quality\n- **Deployment issues**: Validate model artifacts and serving config\n- **Performance degradation**: Monitor data drift and model metrics\n\n### Debugging Steps\n\n1. Check pipeline logs for each stage\n2. Validate input/output data at boundaries\n3. Test components in isolation\n4. Review experiment tracking metrics\n5. Inspect model artifacts and metadata\n\n## Next Steps\n\nAfter setting up your pipeline:\n\n1. Explore **hyperparameter-tuning** skill for optimization\n2. Learn **experiment-tracking-setup** for MLflow/W&B\n3. Review **model-deployment-patterns** for serving strategies\n4. Implement monitoring with observability tools\n\n## Related Skills\n\n- **experiment-tracking-setup**: MLflow and Weights & Biases integration\n- **hyperparameter-tuning**: Automated hyperparameter optimization\n- **model-deployment-patterns**: Advanced deployment strategies\n"
}