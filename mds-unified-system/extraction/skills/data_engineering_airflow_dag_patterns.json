{
  "id": "data_engineering_airflow_dag_patterns",
  "name": "airflow-dag-patterns",
  "source": "data-engineering",
  "originalPath": "plugins/data-engineering/skills/airflow-dag-patterns/SKILL.md",
  "activationCriteria": "Build production Apache Airflow DAGs with best practices for operators, sensors, testing, and deployment. Use when creating data pipelines, orchestrating workflows, or scheduling batch jobs.",
  "tier1_metadata": "airflow-dag-patterns: Build production Apache Airflow DAGs with best practices for operators, sensors, testing, and deploy",
  "tier2_instructions": "# Apache Airflow DAG Patterns\n\nProduction-ready patterns for Apache Airflow including DAG design, operators, sensors, testing, and deployment strategies.\n\n## When to Use This Skill\n\n- Creating data pipeline orchestration with Airflow\n- Designing DAG structures and dependencies\n- Implementing custom operators and sensors\n- Testing Airflow DAGs locally\n- Setting up Airflow in production\n- Debugging failed DAG runs\n\n## Core Concepts\n\n### 1. DAG Design Principles\n\n| Principle | Description |\n|-----------|-------------|\n| **Idempotent** | Running twice produces same result |\n| **Atomic** | Tasks succeed or fail completely |\n| **Incremental** | Process only new/changed data |\n| **Observable** | Logs, metrics, alerts at every step |\n\n### 2. Task Dependencies\n\n```python\n# Linear\ntask1 >> task2 >> task3\n\n# Fan-out\ntask1 >> [task2, task3, task4]\n\n# Fan-in\n[task1, task2, task3] >> task4\n\n# Complex\ntask1 >> task2 >> task4\ntask1 >> task3 >> task4\n```\n\n## Quick Start\n\n```python\n# dags/example_dag.py\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.empty import EmptyOperator\n\ndefault_args = {\n    'owner': 'data-team',\n    'depends_on_past': False,\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n    'retry_exponential_backoff': True,\n    'max_retry_delay': timedelta(hours=1),\n}\n\nwith DAG(\n    dag_id='example_etl',\n    default_args=default_args,\n    description='Example ETL pipeline',\n    schedule='0 6 * * *',  # Daily at 6 AM\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    tags=['etl', 'example'],\n    max_active_runs=1,\n) as dag:\n\n    start = EmptyOperator(task_id='start')\n\n    def extract_data(**context):\n        execution_date = context['ds']\n        # Extract logic here\n        return {'records': 1000}\n\n    extract = PythonOperator(\n        task_id='extract',\n        python_callable=extract_data,\n    )\n\n    e",
  "tier3_resources": "nd = EmptyOperator(task_id='end')\n\n    start >> extract >> end\n```\n\n## Patterns\n\n### Pattern 1: TaskFlow API (Airflow 2.0+)\n\n```python\n# dags/taskflow_example.py\nfrom datetime import datetime\nfrom airflow.decorators import dag, task\nfrom airflow.models import Variable\n\n@dag(\n    dag_id='taskflow_etl',\n    schedule='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    tags=['etl', 'taskflow'],\n)\ndef taskflow_etl():\n    \"\"\"ETL pipeline using TaskFlow API\"\"\"\n\n    @task()\n    def extract(source: str) -> dict:\n        \"\"\"Extract data from source\"\"\"\n        import pandas as pd\n\n        df = pd.read_csv(f's3://bucket/{source}/{{ ds }}.csv')\n        return {'data': df.to_dict(), 'rows': len(df)}\n\n    @task()\n    def transform(extracted: dict) -> dict:\n        \"\"\"Transform extracted data\"\"\"\n        import pandas as pd\n\n        df = pd.DataFrame(extracted['data'])\n        df['processed_at'] = datetime.now()\n        df = df.dropna()\n        return {'data': df.to_dict(), 'rows': len(df)}\n\n    @task()\n    def load(transformed: dict, target: str):\n        \"\"\"Load data to target\"\"\"\n        import pandas as pd\n\n        df = pd.DataFrame(transformed['data'])\n        df.to_parquet(f's3://bucket/{target}/{{ ds }}.parquet')\n        return transformed['rows']\n\n    @task()\n    def notify(rows_loaded: int):\n        \"\"\"Send notification\"\"\"\n        print(f'Loaded {rows_loaded} rows')\n\n    # Define dependencies with XCom passing\n    extracted = extract(source='raw_data')\n    transformed = transform(extracted)\n    loaded = load(transformed, target='processed_data')\n    notify(loaded)\n\n# Instantiate the DAG\ntaskflow_etl()\n```\n\n### Pattern 2: Dynamic DAG Generation\n\n```python\n# dags/dynamic_dag_factory.py\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.models import Variable\nimport json\n\n# Configuration for multiple similar pipelines\nPIPELINE_CONFIGS = [\n    {'name': 'customers', 'schedule': '@daily', 'source': 's3://raw/customers'},\n    {'name': 'orders', 'schedule': '@hourly', 'source': 's3://raw/orders'},\n    {'name': 'products', 'schedule': '@weekly', 'source': 's3://raw/products'},\n]\n\ndef create_dag(config: dict) -> DAG:\n    \"\"\"Factory function to create DAGs from config\"\"\"\n\n    dag_id = f\"etl_{config['name']}\"\n\n    default_args = {\n        'owner': 'data-team',\n        'retries': 3,\n        'retry_delay': timedelta(minutes=5),\n    }\n\n    dag = DAG(\n        dag_id=dag_id,\n        default_args=default_args,\n        schedule=config['schedule'],\n        start_date=datetime(2024, 1, 1),\n        catchup=False,\n        tags=['etl', 'dynamic', config['name']],\n    )\n\n    with dag:\n        def extract_fn(source, **context):\n            print(f\"Extracting from {source} for {context['ds']}\")\n\n        def transform_fn(**context):\n            print(f\"Transforming data for {context['ds']}\")\n\n        def load_fn(table_name, **context):\n            print(f\"Loading to {table_name} for {con",
  "tokenEstimate": {
    "tier1": 19.5,
    "tier2": 313.3,
    "tier3": 1439.1000000000001
  },
  "fullDefinition": "---\nname: airflow-dag-patterns\ndescription: Build production Apache Airflow DAGs with best practices for operators, sensors, testing, and deployment. Use when creating data pipelines, orchestrating workflows, or scheduling batch jobs.\n---\n\n# Apache Airflow DAG Patterns\n\nProduction-ready patterns for Apache Airflow including DAG design, operators, sensors, testing, and deployment strategies.\n\n## When to Use This Skill\n\n- Creating data pipeline orchestration with Airflow\n- Designing DAG structures and dependencies\n- Implementing custom operators and sensors\n- Testing Airflow DAGs locally\n- Setting up Airflow in production\n- Debugging failed DAG runs\n\n## Core Concepts\n\n### 1. DAG Design Principles\n\n| Principle | Description |\n|-----------|-------------|\n| **Idempotent** | Running twice produces same result |\n| **Atomic** | Tasks succeed or fail completely |\n| **Incremental** | Process only new/changed data |\n| **Observable** | Logs, metrics, alerts at every step |\n\n### 2. Task Dependencies\n\n```python\n# Linear\ntask1 >> task2 >> task3\n\n# Fan-out\ntask1 >> [task2, task3, task4]\n\n# Fan-in\n[task1, task2, task3] >> task4\n\n# Complex\ntask1 >> task2 >> task4\ntask1 >> task3 >> task4\n```\n\n## Quick Start\n\n```python\n# dags/example_dag.py\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.empty import EmptyOperator\n\ndefault_args = {\n    'owner': 'data-team',\n    'depends_on_past': False,\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n    'retry_exponential_backoff': True,\n    'max_retry_delay': timedelta(hours=1),\n}\n\nwith DAG(\n    dag_id='example_etl',\n    default_args=default_args,\n    description='Example ETL pipeline',\n    schedule='0 6 * * *',  # Daily at 6 AM\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    tags=['etl', 'example'],\n    max_active_runs=1,\n) as dag:\n\n    start = EmptyOperator(task_id='start')\n\n    def extract_data(**context):\n        execution_date = context['ds']\n        # Extract logic here\n        return {'records': 1000}\n\n    extract = PythonOperator(\n        task_id='extract',\n        python_callable=extract_data,\n    )\n\n    end = EmptyOperator(task_id='end')\n\n    start >> extract >> end\n```\n\n## Patterns\n\n### Pattern 1: TaskFlow API (Airflow 2.0+)\n\n```python\n# dags/taskflow_example.py\nfrom datetime import datetime\nfrom airflow.decorators import dag, task\nfrom airflow.models import Variable\n\n@dag(\n    dag_id='taskflow_etl',\n    schedule='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    tags=['etl', 'taskflow'],\n)\ndef taskflow_etl():\n    \"\"\"ETL pipeline using TaskFlow API\"\"\"\n\n    @task()\n    def extract(source: str) -> dict:\n        \"\"\"Extract data from source\"\"\"\n        import pandas as pd\n\n        df = pd.read_csv(f's3://bucket/{source}/{{ ds }}.csv')\n        return {'data': df.to_dict(), 'rows': len(df)}\n\n    @task()\n    def transform(extracted: dict) -> dict:\n        \"\"\"Transform extracted data\"\"\"\n        import pandas as pd\n\n        df = pd.DataFrame(extracted['data'])\n        df['processed_at'] = datetime.now()\n        df = df.dropna()\n        return {'data': df.to_dict(), 'rows': len(df)}\n\n    @task()\n    def load(transformed: dict, target: str):\n        \"\"\"Load data to target\"\"\"\n        import pandas as pd\n\n        df = pd.DataFrame(transformed['data'])\n        df.to_parquet(f's3://bucket/{target}/{{ ds }}.parquet')\n        return transformed['rows']\n\n    @task()\n    def notify(rows_loaded: int):\n        \"\"\"Send notification\"\"\"\n        print(f'Loaded {rows_loaded} rows')\n\n    # Define dependencies with XCom passing\n    extracted = extract(source='raw_data')\n    transformed = transform(extracted)\n    loaded = load(transformed, target='processed_data')\n    notify(loaded)\n\n# Instantiate the DAG\ntaskflow_etl()\n```\n\n### Pattern 2: Dynamic DAG Generation\n\n```python\n# dags/dynamic_dag_factory.py\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.models import Variable\nimport json\n\n# Configuration for multiple similar pipelines\nPIPELINE_CONFIGS = [\n    {'name': 'customers', 'schedule': '@daily', 'source': 's3://raw/customers'},\n    {'name': 'orders', 'schedule': '@hourly', 'source': 's3://raw/orders'},\n    {'name': 'products', 'schedule': '@weekly', 'source': 's3://raw/products'},\n]\n\ndef create_dag(config: dict) -> DAG:\n    \"\"\"Factory function to create DAGs from config\"\"\"\n\n    dag_id = f\"etl_{config['name']}\"\n\n    default_args = {\n        'owner': 'data-team',\n        'retries': 3,\n        'retry_delay': timedelta(minutes=5),\n    }\n\n    dag = DAG(\n        dag_id=dag_id,\n        default_args=default_args,\n        schedule=config['schedule'],\n        start_date=datetime(2024, 1, 1),\n        catchup=False,\n        tags=['etl', 'dynamic', config['name']],\n    )\n\n    with dag:\n        def extract_fn(source, **context):\n            print(f\"Extracting from {source} for {context['ds']}\")\n\n        def transform_fn(**context):\n            print(f\"Transforming data for {context['ds']}\")\n\n        def load_fn(table_name, **context):\n            print(f\"Loading to {table_name} for {context['ds']}\")\n\n        extract = PythonOperator(\n            task_id='extract',\n            python_callable=extract_fn,\n            op_kwargs={'source': config['source']},\n        )\n\n        transform = PythonOperator(\n            task_id='transform',\n            python_callable=transform_fn,\n        )\n\n        load = PythonOperator(\n            task_id='load',\n            python_callable=load_fn,\n            op_kwargs={'table_name': config['name']},\n        )\n\n        extract >> transform >> load\n\n    return dag\n\n# Generate DAGs\nfor config in PIPELINE_CONFIGS:\n    globals()[f\"dag_{config['name']}\"] = create_dag(config)\n```\n\n### Pattern 3: Branching and Conditional Logic\n\n```python\n# dags/branching_example.py\nfrom airflow.decorators import dag, task\nfrom airflow.operators.python import BranchPythonOperator\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n@dag(\n    dag_id='branching_pipeline',\n    schedule='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n)\ndef branching_pipeline():\n\n    @task()\n    def check_data_quality() -> dict:\n        \"\"\"Check data quality and return metrics\"\"\"\n        quality_score = 0.95  # Simulated\n        return {'score': quality_score, 'rows': 10000}\n\n    def choose_branch(**context) -> str:\n        \"\"\"Determine which branch to execute\"\"\"\n        ti = context['ti']\n        metrics = ti.xcom_pull(task_ids='check_data_quality')\n\n        if metrics['score'] >= 0.9:\n            return 'high_quality_path'\n        elif metrics['score'] >= 0.7:\n            return 'medium_quality_path'\n        else:\n            return 'low_quality_path'\n\n    quality_check = check_data_quality()\n\n    branch = BranchPythonOperator(\n        task_id='branch',\n        python_callable=choose_branch,\n    )\n\n    high_quality = EmptyOperator(task_id='high_quality_path')\n    medium_quality = EmptyOperator(task_id='medium_quality_path')\n    low_quality = EmptyOperator(task_id='low_quality_path')\n\n    # Join point - runs after any branch completes\n    join = EmptyOperator(\n        task_id='join',\n        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,\n    )\n\n    quality_check >> branch >> [high_quality, medium_quality, low_quality] >> join\n\nbranching_pipeline()\n```\n\n### Pattern 4: Sensors and External Dependencies\n\n```python\n# dags/sensor_patterns.py\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.sensors.filesystem import FileSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3KeySensor\nfrom airflow.sensors.external_task import ExternalTaskSensor\nfrom airflow.operators.python import PythonOperator\n\nwith DAG(\n    dag_id='sensor_example',\n    schedule='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n) as dag:\n\n    # Wait for file on S3\n    wait_for_file = S3KeySensor(\n        task_id='wait_for_s3_file',\n        bucket_name='data-lake',\n        bucket_key='raw/{{ ds }}/data.parquet',\n        aws_conn_id='aws_default',\n        timeout=60 * 60 * 2,  # 2 hours\n        poke_interval=60 * 5,  # Check every 5 minutes\n        mode='reschedule',  # Free up worker slot while waiting\n    )\n\n    # Wait for another DAG to complete\n    wait_for_upstream = ExternalTaskSensor(\n        task_id='wait_for_upstream_dag',\n        external_dag_id='upstream_etl',\n        external_task_id='final_task',\n        execution_date_fn=lambda dt: dt,  # Same execution date\n        timeout=60 * 60 * 3,\n        mode='reschedule',\n    )\n\n    # Custom sensor using @task.sensor decorator\n    @task.sensor(poke_interval=60, timeout=3600, mode='reschedule')\n    def wait_for_api() -> PokeReturnValue:\n        \"\"\"Custom sensor for API availability\"\"\"\n        import requests\n\n        response = requests.get('https://api.example.com/health')\n        is_done = response.status_code == 200\n\n        return PokeReturnValue(is_done=is_done, xcom_value=response.json())\n\n    api_ready = wait_for_api()\n\n    def process_data(**context):\n        api_result = context['ti'].xcom_pull(task_ids='wait_for_api')\n        print(f\"API returned: {api_result}\")\n\n    process = PythonOperator(\n        task_id='process',\n        python_callable=process_data,\n    )\n\n    [wait_for_file, wait_for_upstream, api_ready] >> process\n```\n\n### Pattern 5: Error Handling and Alerts\n\n```python\n# dags/error_handling.py\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.models import Variable\n\ndef task_failure_callback(context):\n    \"\"\"Callback on task failure\"\"\"\n    task_instance = context['task_instance']\n    exception = context.get('exception')\n\n    # Send to Slack/PagerDuty/etc\n    message = f\"\"\"\n    Task Failed!\n    DAG: {task_instance.dag_id}\n    Task: {task_instance.task_id}\n    Execution Date: {context['ds']}\n    Error: {exception}\n    Log URL: {task_instance.log_url}\n    \"\"\"\n    # send_slack_alert(message)\n    print(message)\n\ndef dag_failure_callback(context):\n    \"\"\"Callback on DAG failure\"\"\"\n    # Aggregate failures, send summary\n    pass\n\nwith DAG(\n    dag_id='error_handling_example',\n    schedule='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    on_failure_callback=dag_failure_callback,\n    default_args={\n        'on_failure_callback': task_failure_callback,\n        'retries': 3,\n        'retry_delay': timedelta(minutes=5),\n    },\n) as dag:\n\n    def might_fail(**context):\n        import random\n        if random.random() < 0.3:\n            raise ValueError(\"Random failure!\")\n        return \"Success\"\n\n    risky_task = PythonOperator(\n        task_id='risky_task',\n        python_callable=might_fail,\n    )\n\n    def cleanup(**context):\n        \"\"\"Cleanup runs regardless of upstream failures\"\"\"\n        print(\"Cleaning up...\")\n\n    cleanup_task = PythonOperator(\n        task_id='cleanup',\n        python_callable=cleanup,\n        trigger_rule=TriggerRule.ALL_DONE,  # Run even if upstream fails\n    )\n\n    def notify_success(**context):\n        \"\"\"Only runs if all upstream succeeded\"\"\"\n        print(\"All tasks succeeded!\")\n\n    success_notification = PythonOperator(\n        task_id='notify_success',\n        python_callable=notify_success,\n        trigger_rule=TriggerRule.ALL_SUCCESS,\n    )\n\n    risky_task >> [cleanup_task, success_notification]\n```\n\n### Pattern 6: Testing DAGs\n\n```python\n# tests/test_dags.py\nimport pytest\nfrom datetime import datetime\nfrom airflow.models import DagBag\n\n@pytest.fixture\ndef dagbag():\n    return DagBag(dag_folder='dags/', include_examples=False)\n\ndef test_dag_loaded(dagbag):\n    \"\"\"Test that all DAGs load without errors\"\"\"\n    assert len(dagbag.import_errors) == 0, f\"DAG import errors: {dagbag.import_errors}\"\n\ndef test_dag_structure(dagbag):\n    \"\"\"Test specific DAG structure\"\"\"\n    dag = dagbag.get_dag('example_etl')\n\n    assert dag is not None\n    assert len(dag.tasks) == 3\n    assert dag.schedule_interval == '0 6 * * *'\n\ndef test_task_dependencies(dagbag):\n    \"\"\"Test task dependencies are correct\"\"\"\n    dag = dagbag.get_dag('example_etl')\n\n    extract_task = dag.get_task('extract')\n    assert 'start' in [t.task_id for t in extract_task.upstream_list]\n    assert 'end' in [t.task_id for t in extract_task.downstream_list]\n\ndef test_dag_integrity(dagbag):\n    \"\"\"Test DAG has no cycles and is valid\"\"\"\n    for dag_id, dag in dagbag.dags.items():\n        assert dag.test_cycle() is None, f\"Cycle detected in {dag_id}\"\n\n# Test individual task logic\ndef test_extract_function():\n    \"\"\"Unit test for extract function\"\"\"\n    from dags.example_dag import extract_data\n\n    result = extract_data(ds='2024-01-01')\n    assert 'records' in result\n    assert isinstance(result['records'], int)\n```\n\n## Project Structure\n\n```\nairflow/\n\u251c\u2500\u2500 dags/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 common/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 operators.py    # Custom operators\n\u2502   \u2502   \u251c\u2500\u2500 sensors.py      # Custom sensors\n\u2502   \u2502   \u2514\u2500\u2500 callbacks.py    # Alert callbacks\n\u2502   \u251c\u2500\u2500 etl/\n\u2502   \u2502   \u251c\u2500\u2500 customers.py\n\u2502   \u2502   \u2514\u2500\u2500 orders.py\n\u2502   \u2514\u2500\u2500 ml/\n\u2502       \u2514\u2500\u2500 training.py\n\u251c\u2500\u2500 plugins/\n\u2502   \u2514\u2500\u2500 custom_plugin.py\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 test_dags.py\n\u2502   \u2514\u2500\u2500 test_operators.py\n\u251c\u2500\u2500 docker-compose.yml\n\u2514\u2500\u2500 requirements.txt\n```\n\n## Best Practices\n\n### Do's\n- **Use TaskFlow API** - Cleaner code, automatic XCom\n- **Set timeouts** - Prevent zombie tasks\n- **Use `mode='reschedule'`** - For sensors, free up workers\n- **Test DAGs** - Unit tests and integration tests\n- **Idempotent tasks** - Safe to retry\n\n### Don'ts\n- **Don't use `depends_on_past=True`** - Creates bottlenecks\n- **Don't hardcode dates** - Use `{{ ds }}` macros\n- **Don't use global state** - Tasks should be stateless\n- **Don't skip catchup blindly** - Understand implications\n- **Don't put heavy logic in DAG file** - Import from modules\n\n## Resources\n\n- [Airflow Documentation](https://airflow.apache.org/docs/)\n- [Astronomer Guides](https://docs.astronomer.io/learn)\n- [TaskFlow API](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html)\n"
}